{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prof_v2 Att HTR_tf_unif_att_v5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/prof_v2_Att_HTR_tf_unif_att_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "fd01d8ce-8d66-46eb-ae3d-fbf61ec12a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "5e20bc91-a4f4-4afc-a667-c45538c785f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "image_width = 1500\n",
        "image_height = 200\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QuAkNIOOQkar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnectedX(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, last_fn=None):\n",
        "        super(FullyConnectedX, self).__init__()\n",
        "        layers = []\n",
        "        self.flatten = flatten\n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "        else: \n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        if last_fn is not None:\n",
        "            layers.append(last_fn)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.flatten:\n",
        "            x = x.view(x.shape[0], -1)\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 4, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([4, 16, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([16, 32, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([32, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([64, 64, 1], padding=0, stride=(1,11), bn=batchnorm, pool_layer=None))\n",
        "        \n",
        "        #self.fc = FullyConnectedX([64*15*49, 64*49*3, 64*49], activation_fn=nn.ReLU())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = h.squeeze(-1)\n",
        "        #h = h.flatten(start_dim=1)\n",
        "        #h = self.fc(h)\n",
        "        #h = F.max_pool2d(h, [1, h.size(1)], padding=[0, 0])\n",
        "        #h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([2, 3, 0, 1])\n",
        "        \n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "outputId": "afd3a744-d0d3-41f9-e152-1b6b5c4534c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 92])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BEG3WoC71hQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Co0VksJ81rz0",
        "colab_type": "code",
        "outputId": "baf3a0eb-7de8-45de-a174-c3359c114aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "64*15*49\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=92, encoded_height=64, batchnorm=True, dropout=False, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = floor(features_size*0.3)\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.0)\n",
        "\n",
        "        self.fc = FullyConnectedX([features_size, floor(features_size*0.7), floor(features_size*0.5), lstm_inp_size], activation_fn=nn.ReLU(), last_fn=nn.Tanh())\n",
        "        \n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "#        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_height*self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        #print(self.attention)\n",
        "        #self.concatenated = torch.FloatTensor(24, )\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        #set_trace()\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "          \n",
        "          hidden_m = hidden.permute(1, 0, 2)\n",
        "\n",
        "          hidden_m = hidden_m.flatten(start_dim=1)\n",
        "          \n",
        "          attention_inp = torch.cat([x, hidden_m], dim=1).detach()\n",
        "          self.attention_weights = self.attention(attention_inp)\n",
        "          #print(x.shape)\n",
        "          #print(hidden_m.shape)\n",
        "          #print(attention_inp.shape)\n",
        "          #print(self.attention_weights.shape)\n",
        "          \n",
        "          self.attention_weights = F.softmax(self.attention_weights, dim=1)\n",
        "          #print(self.attention_weights.shape)\n",
        "          \n",
        "          self.attention_weights = self.attention_weights.repeat([1, self.encoded_height])\n",
        "          #print(self.attention_weights.shape)\n",
        "                  \n",
        "          #print(x.shape)\n",
        "          x = x * self.attention_weights\n",
        "          #print(x.shape)\n",
        "          #raise Exception()\n",
        "          \n",
        "          \n",
        "          #print(\"********************\")\n",
        "          #print(x)\n",
        "          #print(attention_w)\n",
        "          #print(X)\n",
        "          #print(\"---------------\")\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        \n",
        "        \n",
        "        x = torch.cat([x, emb], dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = x.unsqueeze(0)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        \n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return h1\n",
        "      \n",
        "decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AbsiEQDIv_v3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRRecognition(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HTRRecognition, self).__init__()\n",
        "    self.encoder = HTREncoder()\n",
        "    self.decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\")\n",
        "    self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "    self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "    self.START = train_set.start_code\n",
        "    self.STOP = train_set.stop_code\n",
        "    self.recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "    self.old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    self.loss = 0\n",
        "    self.stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    self.stop_symbol.fill_(self.STOP)\n",
        "    self.criterion = nn.NLLLoss()\n",
        "    \n",
        "  def zero_grad(self):\n",
        "    self.encoder.zero_grad()\n",
        "    self.decoder.zero_grad()\n",
        "    \n",
        "  def forward(self, data, target, use_teacher_forcing):\n",
        "    orig_data = data\n",
        "\n",
        "    hidden = self.decoder.makeHidden()   \n",
        "\n",
        "    self.loss = 0\n",
        "    enc = self.encoder(data)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    \n",
        "    self.old_symbol[:, 0] = self.START\n",
        "    \n",
        "    self.hidden_states_ = []\n",
        "\n",
        "    for i in range(0, target.shape[1]):\n",
        "\n",
        "      dec, hidden = self.decoder(s, self.old_symbol, hidden)\n",
        "      self.hidden_states_.append(hidden.unsqueeze(0))\n",
        "      self.recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "      if use_teacher_forcing:\n",
        "        self.old_symbol[:, 0] = target[:, i]\n",
        "      else:\n",
        "        self.old_symbol[:, 0] = self.recognition_result[:, i]\n",
        "      self.loss += self.criterion(dec, target[:, i])\n",
        "    self.length = target.shape[1]\n",
        "    return self.recognition_result[:, 0: target.shape[1]]\n",
        "  \n",
        "  def normed_loss(self):\n",
        "    return self.loss/target.shape[1]\n",
        "   \n",
        "  def backprop(self):\n",
        "    self.loss.backward()\n",
        "    \n",
        "  def hidden_states(self):\n",
        "    r = torch.cat(self.hidden_states_, dim = 0)\n",
        "    r = r.permute(0, 2, 1, 3)\n",
        "    r = r.flatten(start_dim=2)\n",
        "    return r\n",
        "    \n",
        "  def step(self):\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), grad_clip)\n",
        "    self.encoder_optimizer.step()\n",
        "    self.decoder_optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_dQSCL04lmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From https://github.com/aryopg/Professor_Forcing_Pytorch/blob/master/models/losses.py\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, input_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        from math import floor\n",
        "        self.hidden_cells = 256\n",
        "        self.hidden_layers = 2\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_length = input_length\n",
        "        self.rnn_layers = 2\n",
        "        \n",
        "        gru_input_size = 256*2\n",
        "        \n",
        "        #self.enc = FullyConnectedX([input_size, floor(input_size*0.7), gru_input_size], activation_fn=nn.ReLU())\n",
        "\n",
        "        self.gru = nn.GRU(gru_input_size, hidden_size, self.rnn_layers)\n",
        "        \n",
        "        gru_out = input_length*hidden_size\n",
        "        self.fc = FullyConnectedX([gru_out, floor(gru_out*0.7), floor(gru_out*0.3), 1], activation_fn=nn.ReLU())\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "        \n",
        "    def zero_grad(self):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = torch.zeros(self.input_length, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "        hidden = self.initHidden()\n",
        "        for ei in range(x.shape[0]):\n",
        "            #embedded = self.embedding(x[:, ei])\n",
        "            #embedded = embedded.view(1, batch_size, -1)\n",
        "            #output = embedded\n",
        "            output = x[ei]\n",
        "            #output = self.enc(output)\n",
        "            output = output.unsqueeze(0)\n",
        "            #print(output.shape)\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "            outputs[ei] = output[0, 0]\n",
        "\n",
        "        outputs = outputs.permute(1,0,2)\n",
        "        #print(outputs.shape)\n",
        "        #feat = outputs.contiguous().view(x.shape[0], -1)\n",
        "        feat = outputs.flatten(start_dim=1)\n",
        "        #print(feat.shape)\n",
        "        out = self.fc(feat)\n",
        "        \n",
        "        self.features = feat\n",
        "\n",
        "        return out\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.rnn_layers, batch_size, self.hidden_size, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7kaQayXglGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "batch_ones = torch.ones((batch_size, 1)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1KQNsFrRYAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = HTRRecognition().to(device)\n",
        "discriminator = Discriminator(256*2, 512, 10).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZG9dz-viW6d",
        "colab_type": "code",
        "outputId": "3bdaae6c-b034-4ebd-850c-59da25f6a9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "def process_batch(data, target, batch_idx):\n",
        "  generator.zero_grad()\n",
        "  period = 30\n",
        "  for i in range(0, 3):\n",
        "    discriminator.zero_grad()\n",
        "    with torch.no_grad():\n",
        "      free_run_result = generator(data, target, False)\n",
        "      \n",
        "      if (batch_idx % period == 0) and (i == 0):\n",
        "        from random import random\n",
        "        for k in range(0, min(2, target.shape[0])):\n",
        "              decoded =free_run_result[k,0:target.shape[1]]\n",
        "              #plt.imshow(data[k].cpu(), cmap=\"gray\")\n",
        "              #plt.show()\n",
        "              print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      \n",
        "      free_run_hidden = generator.hidden_states().detach()\n",
        "      generator(data, target, True)\n",
        "      teacher_forcing_hidden = generator.hidden_states().detach()\n",
        "    d_free_run = discriminator(free_run_hidden)\n",
        "    d_teacher_forcing = discriminator(teacher_forcing_hidden)\n",
        "    true_loss = F.binary_cross_entropy_with_logits(d_free_run, batch_zeros)\n",
        "    fake_loss = F.binary_cross_entropy_with_logits(d_teacher_forcing, batch_ones)\n",
        "    D_loss = 0.5*(fake_loss + true_loss)\n",
        "    d_val = D_loss.item()\n",
        "    D_loss.backward()\n",
        "    discriminator.optimizer.step()\n",
        "    \n",
        "  generator.zero_grad()\n",
        "  discriminator.zero_grad()\n",
        "  generator(data, target, True)\n",
        "  teacher_forcing_hidden_ = generator.hidden_states()\n",
        "  fake_pred = discriminator(teacher_forcing_hidden_)\n",
        "  tf_val = generator.normed_loss().item()\n",
        "  G_loss = generator.normed_loss() + F.binary_cross_entropy_with_logits(fake_pred, batch_zeros)\n",
        "  g_val = G_loss.item()\n",
        "  G_loss.backward()\n",
        "  generator.step()\n",
        "  if batch_idx % period == 0:\n",
        "    print(\"Batch: %d Descr %.4f TF %.4f Full %.4f\" % (batch_idx, d_val, tf_val, g_val)) \n",
        "  \n",
        "def train(epoch):\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch(use_binarization=False)\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    if target.shape[1] > 5:\n",
        "      return\n",
        "    target = target.to(device)\n",
        "    data = data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    process_batch(data, target, batch_idx)\n",
        "    batch_idx += 1\n",
        "    \n",
        "for i in range(0, 100):\n",
        "  print(\"Epoch %d\" % i)\n",
        "  print(\"***************************************\")\n",
        "  train(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "***************************************\n",
            "  a -> t\n",
            "  I -> o\n",
            "Batch: 0 Descr 0.6966 TF 2.3540 Full 3.1344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UVKet5eTNNq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "free_run = generator(data, target, False)\n",
        "teacher_forcing = generator(data, target, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0daZUTUgwUa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d_free_run = discriminator(free_run)\n",
        "d_teacher_forcing = discriminator(teacher_forcing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9NaP9boxgNXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "true_loss = F.binary_cross_entropy_with_logits(d_free_run, batch_zeros)\n",
        "fake_loss = F.binary_cross_entropy_with_logits(d_teacher_forcing, batch_ones)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_vS08Ro4hMGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "D_loss = 0.5*(fake_loss + true_loss)\n",
        "\n",
        "D_loss.backward()\n",
        "discriminator.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZpswIFybsCi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "teacher_forcing.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jKDWUKVoaJ5b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "desc_teacher_forcing = discriminator(teacher_forcing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3wpoggMZdysP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "desc_teacher_forcing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9wI82dfTsUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Professor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Professor, self).__init__()\n",
        "    self.generator = HTRRecognition().to(device)\n",
        "    self.discriminator = Discriminator(len(train_set.codes), 256*2, 32)\n",
        "    \n",
        "    \n",
        "    \n",
        "  def trainDiscriminator(self):\n",
        "#     for p in discriminator.parameters():\n",
        "#         p.requires_grad = True\n",
        "\n",
        "    self.generator.eval()\n",
        "    self.discriminator.train()\n",
        "    self.generator.zero_grad()\n",
        "    self.discriminator.zero_grad()\n",
        "\n",
        "    free_run_nllloss, G_free_run_output, G_free_run_output_sents = generator(x=input_tensor,\n",
        "                                                                             y=target_tensor,\n",
        "                                                                             encoder_hidden=encoder_hidden,\n",
        "                                                                             input_length=input_length,\n",
        "                                                                             target_length=target_length,\n",
        "                                                                             criterion=criterion,\n",
        "                                                                             teacher_forced=False,\n",
        "                                                                             train=True)\n",
        "\n",
        "    teacher_forced_nllloss, G_teacher_forced_output, G_teacher_forced_output_sents = generator(x=input_tensor,\n",
        "                                                                                                 y=target_tensor,\n",
        "                                                                                                 encoder_hidden=encoder_hidden,\n",
        "                                                                                                 input_length=input_length,\n",
        "                                                                                                 target_length=target_length,\n",
        "                                                                                                 criterion=criterion,\n",
        "                                                                                                 teacher_forced=True,\n",
        "                                                                                                 train=True)\n",
        "\n",
        "    D_free_run_feat, D_free_run_out = discriminator(G_free_run_output.type(torch.LongTensor).to(device))\n",
        "    D_teacher_forced_feat, D_teacher_forced_out = discriminator(G_teacher_forced_output.type(torch.LongTensor).to(device))\n",
        "\n",
        "    D_loss = -(torch.mean(D_teacher_forced_out) - torch.mean(D_free_run_out))\n",
        "\n",
        "    D_loss.backward(retain_graph=True)\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "    for p in discriminator.parameters():\n",
        "        p.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "    return discriminator, discriminator_optimizer, D_loss.data.cpu().numpy()\n",
        "  \n",
        "  def trainGenerator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length):\n",
        "    for p in discriminator.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.eval()\n",
        "    generator.zero_grad()\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    free_run_nllloss, G_free_run_output, G_free_run_output_sents = generator(x=input_tensor,\n",
        "                                                                             y=target_tensor,\n",
        "                                                                             encoder_hidden=encoder_hidden,\n",
        "                                                                             input_length=input_length,\n",
        "                                                                             target_length=target_length,\n",
        "                                                                             criterion=criterion,\n",
        "                                                                             teacher_forced=False,\n",
        "                                                                             train=True)\n",
        "\n",
        "    teacher_forced_nllloss, G_teacher_forced_output, G_teacher_forced_output_sents = generator(x=input_tensor,\n",
        "                                                                                                 y=target_tensor,\n",
        "                                                                                                 encoder_hidden=encoder_hidden,\n",
        "                                                                                                 input_length=input_length,\n",
        "                                                                                                 target_length=target_length,\n",
        "                                                                                                 criterion=criterion,\n",
        "                                                                                                 teacher_forced=True,\n",
        "                                                                                                 train=True)\n",
        "\n",
        "    D_free_run_feat, D_free_run_out = discriminator(G_free_run_output.type(torch.LongTensor).to(device))\n",
        "    D_teacher_forced_feat, D_teacher_forced_out = discriminator(G_teacher_forced_output.type(torch.LongTensor).to(device))\n",
        "    #print(D_free_run_feat == D_teacher_forced_feat)\n",
        "    #cos = nn.CosineSimilarity(dim=1)\n",
        "    #print(cos(D_free_run_feat, D_teacher_forced_feat))\n",
        "    #print(cos(D_free_run_feat, D_teacher_forced_feat).mean())\n",
        "\n",
        "\n",
        "    free_run_loss = jsdloss(D_teacher_forced_feat, D_free_run_feat) / BATCH_SIZE\n",
        "    teacher_forced_loss = jsdloss(D_free_run_feat, D_teacher_forced_feat) / BATCH_SIZE\n",
        "\n",
        "    print('free_run_loss: ', free_run_loss)\n",
        "    print('teacher_forced_loss: ', teacher_forced_loss)\n",
        "\n",
        "    G_loss = (teacher_forced_nllloss / target_length) + free_run_loss# + teacher_forced_loss\n",
        "\n",
        "    G_loss.backward(retain_graph=True)\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    return generator, generator_optimizer, G_loss.data.cpu().numpy()[0][0], (free_run_nllloss.item() / target_length)\n",
        "\n",
        "      \n",
        "    \n",
        "    \n",
        "  def train(self):\n",
        "    encoder_hidden = generator.initEncoderHidden(batch_size=BATCH_SIZE)\n",
        "\n",
        "    generator.zero_grad()\n",
        "    \n",
        "    D_PRESTEP = 5\n",
        "\n",
        "    D_loss = 0\n",
        "    for _ in range(D_PRESTEP):\n",
        "        discriminator, discriminator_optimizer, D_loss_step = trainDiscriminator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length)\n",
        "        D_loss += D_loss_step\n",
        "    D_loss = D_loss/D_PRESTEP\n",
        "\n",
        "    generator, generator_optimizer, G_loss, performance_nllloss = trainGenerator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length)\n",
        "\n",
        "    return generator, generator_optimizer, discriminator, discriminator_optimizer, D_loss, G_loss, performance_nllloss\n",
        "\n",
        "  \n",
        "\n",
        "  \n",
        "professor = Professor()\n",
        "professor.train()  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e42hUva8T53U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brQd8oPXNyo2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainDiscriminator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length):\n",
        "    for p in discriminator.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    generator.eval()\n",
        "    discriminator.train()\n",
        "    generator.zero_grad()\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    free_run_nllloss, G_free_run_output, G_free_run_output_sents = generator(x=input_tensor,\n",
        "                                                                             y=target_tensor,\n",
        "                                                                             encoder_hidden=encoder_hidden,\n",
        "                                                                             input_length=input_length,\n",
        "                                                                             target_length=target_length,\n",
        "                                                                             criterion=criterion,\n",
        "                                                                             teacher_forced=False,\n",
        "                                                                             train=True)\n",
        "\n",
        "    teacher_forced_nllloss, G_teacher_forced_output, G_teacher_forced_output_sents = generator(x=input_tensor,\n",
        "                                                                                                 y=target_tensor,\n",
        "                                                                                                 encoder_hidden=encoder_hidden,\n",
        "                                                                                                 input_length=input_length,\n",
        "                                                                                                 target_length=target_length,\n",
        "                                                                                                 criterion=criterion,\n",
        "                                                                                                 teacher_forced=True,\n",
        "                                                                                                 train=True)\n",
        "\n",
        "    D_free_run_feat, D_free_run_out = discriminator(G_free_run_output.type(torch.LongTensor).to(device))\n",
        "    D_teacher_forced_feat, D_teacher_forced_out = discriminator(G_teacher_forced_output.type(torch.LongTensor).to(device))\n",
        "\n",
        "    D_loss = -(torch.mean(D_teacher_forced_out) - torch.mean(D_free_run_out))\n",
        "\n",
        "    D_loss.backward(retain_graph=True)\n",
        "    discriminator_optimizer.step()\n",
        "\n",
        "    for p in discriminator.parameters():\n",
        "        p.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "    return discriminator, discriminator_optimizer, D_loss.data.cpu().numpy()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAysjEBoORPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainGenerator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length):\n",
        "    for p in discriminator.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    generator.train()\n",
        "    discriminator.eval()\n",
        "    generator.zero_grad()\n",
        "    discriminator.zero_grad()\n",
        "\n",
        "    free_run_nllloss, G_free_run_output, G_free_run_output_sents = generator(x=input_tensor,\n",
        "                                                                             y=target_tensor,\n",
        "                                                                             encoder_hidden=encoder_hidden,\n",
        "                                                                             input_length=input_length,\n",
        "                                                                             target_length=target_length,\n",
        "                                                                             criterion=criterion,\n",
        "                                                                             teacher_forced=False,\n",
        "                                                                             train=True)\n",
        "\n",
        "    teacher_forced_nllloss, G_teacher_forced_output, G_teacher_forced_output_sents = generator(x=input_tensor,\n",
        "                                                                                                 y=target_tensor,\n",
        "                                                                                                 encoder_hidden=encoder_hidden,\n",
        "                                                                                                 input_length=input_length,\n",
        "                                                                                                 target_length=target_length,\n",
        "                                                                                                 criterion=criterion,\n",
        "                                                                                                 teacher_forced=True,\n",
        "                                                                                                 train=True)\n",
        "\n",
        "    D_free_run_feat, D_free_run_out = discriminator(G_free_run_output.type(torch.LongTensor).to(device))\n",
        "    D_teacher_forced_feat, D_teacher_forced_out = discriminator(G_teacher_forced_output.type(torch.LongTensor).to(device))\n",
        "    #print(D_free_run_feat == D_teacher_forced_feat)\n",
        "    #cos = nn.CosineSimilarity(dim=1)\n",
        "    #print(cos(D_free_run_feat, D_teacher_forced_feat))\n",
        "    #print(cos(D_free_run_feat, D_teacher_forced_feat).mean())\n",
        "    \n",
        "\n",
        "    free_run_loss = jsdloss(D_teacher_forced_feat, D_free_run_feat) / BATCH_SIZE\n",
        "    teacher_forced_loss = jsdloss(D_free_run_feat, D_teacher_forced_feat) / BATCH_SIZE\n",
        "    \n",
        "    print('free_run_loss: ', free_run_loss)\n",
        "    print('teacher_forced_loss: ', teacher_forced_loss)\n",
        "\n",
        "    G_loss = (teacher_forced_nllloss / target_length) + free_run_loss# + teacher_forced_loss\n",
        "\n",
        "    G_loss.backward(retain_graph=True)\n",
        "    generator_optimizer.step()\n",
        "\n",
        "    return generator, generator_optimizer, G_loss.data.cpu().numpy()[0][0], (free_run_nllloss.item() / target_length)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOJbLvi_OXVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = generator.initEncoderHidden(batch_size=BATCH_SIZE)\n",
        "\n",
        "    generator.zero_grad()\n",
        "\n",
        "    input_tensor = torch.cat([input_tensor_i.unsqueeze(0) for input_tensor_i in input_tensor], 0).to(device)\n",
        "    target_tensor = torch.cat([target_tensor_i.unsqueeze(0) for target_tensor_i in target_tensor], 0).to(device)\n",
        "\n",
        "    input_length = input_tensor.size(1)\n",
        "    target_length = target_tensor.size(1)\n",
        "\n",
        "    input_tensor = input_tensor.permute(1,0,2)\n",
        "    target_tensor = target_tensor.permute(1,0,2)\n",
        "\n",
        "    D_loss = 0\n",
        "    for _ in range(D_PRESTEP):\n",
        "        discriminator, discriminator_optimizer, D_loss_step = trainDiscriminator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length)\n",
        "        D_loss += D_loss_step\n",
        "    D_loss = D_loss/D_PRESTEP\n",
        "\n",
        "    generator, generator_optimizer, G_loss, performance_nllloss = trainGenerator(input_tensor, target_tensor, encoder_hidden, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, input_length, target_length)\n",
        "\n",
        "    return generator, generator_optimizer, discriminator, discriminator_optimizer, D_loss, G_loss, performance_nllloss\n",
        "\n",
        "def trainIters(generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss, training_pairs, n_iters, print_every=1000, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    plot_G_losses = []\n",
        "    plot_D_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    print_G_loss_total = 0\n",
        "    plot_G_loss_total = 0\n",
        "    print_D_loss_total = 0\n",
        "    plot_D_loss_total = 0\n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[(iter-1)*BATCH_SIZE:iter*BATCH_SIZE] # n tensor (list)\n",
        "        input_tensor = [pair[0] for pair in training_pair]\n",
        "        target_tensor = [pair[1] for pair in training_pair]\n",
        "\n",
        "        generator, generator_optimizer, discriminator, discriminator_optimizer, D_loss, G_loss, loss = train(input_tensor, target_tensor, generator, discriminator, generator_optimizer, discriminator_optimizer, criterion, jsdloss)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        \n",
        "        print_G_loss_total += G_loss\n",
        "        plot_G_loss_total += G_loss\n",
        "        \n",
        "        print_D_loss_total += D_loss\n",
        "        plot_D_loss_total += D_loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print_G_loss_avg = print_G_loss_total / print_every\n",
        "            print_G_loss_total = 0\n",
        "            print_D_loss_avg = print_D_loss_total / print_every\n",
        "            print_D_loss_total = 0\n",
        "            print('%s (%d %d%%) NLLLoss: %.4f - Generator Loss: %.4f - Discriminator Loss: %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg, print_G_loss_avg, print_D_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "            \n",
        "            plot_G_loss_avg = plot_G_loss_total / plot_every\n",
        "            plot_G_losses.append(plot_G_loss_avg)\n",
        "            plot_G_loss_total = 0\n",
        "            \n",
        "            plot_D_loss_avg = plot_D_loss_total / plot_every\n",
        "            plot_D_losses.append(plot_D_loss_avg)\n",
        "            plot_D_loss_total = 0\n",
        "\n",
        "    return generator, discriminator, plot_losses, plot_G_losses, plot_D_losses\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GHhQNaOtyJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "23*64\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r5YWBxEcEbdC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "recognition = HTRRecognition().to(device)\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 100\n",
        "  \n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  \n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    orig_data, target = batch\n",
        "    if target.shape[1] > 4:\n",
        "      break\n",
        "      \n",
        "      \n",
        "    data = orig_data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    target = target.to(device)\n",
        "    \n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "    recognition.train(data, target, use_teacher_forcing)  \n",
        "    \n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    c_loss += recognition.loss.item()/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition.recognition_result[k,0:target.shape[1]]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yg8-hpPq2e2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}