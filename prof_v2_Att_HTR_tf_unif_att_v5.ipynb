{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prof_v2 Att HTR_tf_unif_att_v5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/prof_v2_Att_HTR_tf_unif_att_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "fd01d8ce-8d66-46eb-ae3d-fbf61ec12a67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "5e20bc91-a4f4-4afc-a667-c45538c785f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "image_width = 1500\n",
        "image_height = 200\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QuAkNIOOQkar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnectedX(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, last_fn=None):\n",
        "        super(FullyConnectedX, self).__init__()\n",
        "        layers = []\n",
        "        self.flatten = flatten\n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "        else: \n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        if last_fn is not None:\n",
        "            layers.append(last_fn)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.flatten:\n",
        "            x = x.view(x.shape[0], -1)\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 4, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([4, 16, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([16, 32, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([32, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([64, 64, 1], padding=0, stride=(1,11), bn=batchnorm, pool_layer=None))\n",
        "        \n",
        "        #self.fc = FullyConnectedX([64*15*49, 64*49*3, 64*49], activation_fn=nn.ReLU())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = h.squeeze(-1)\n",
        "        #h = h.flatten(start_dim=1)\n",
        "        #h = self.fc(h)\n",
        "        #h = F.max_pool2d(h, [1, h.size(1)], padding=[0, 0])\n",
        "        #h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([2, 3, 0, 1])\n",
        "        \n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "outputId": "afd3a744-d0d3-41f9-e152-1b6b5c4534c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 92])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BEG3WoC71hQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Co0VksJ81rz0",
        "colab_type": "code",
        "outputId": "baf3a0eb-7de8-45de-a174-c3359c114aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "64*15*49\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=92, encoded_height=64, batchnorm=True, dropout=False, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = floor(features_size*0.3)\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.0)\n",
        "\n",
        "        self.fc = FullyConnectedX([features_size, floor(features_size*0.7), floor(features_size*0.5), lstm_inp_size], activation_fn=nn.ReLU(), last_fn=nn.Tanh())\n",
        "        \n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "#        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_height*self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        #print(self.attention)\n",
        "        #self.concatenated = torch.FloatTensor(24, )\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        #set_trace()\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "          \n",
        "          hidden_m = hidden.permute(1, 0, 2)\n",
        "\n",
        "          hidden_m = hidden_m.flatten(start_dim=1)\n",
        "          \n",
        "          attention_inp = torch.cat([x, hidden_m], dim=1).detach()\n",
        "          self.attention_weights = self.attention(attention_inp)\n",
        "          #print(x.shape)\n",
        "          #print(hidden_m.shape)\n",
        "          #print(attention_inp.shape)\n",
        "          #print(self.attention_weights.shape)\n",
        "          \n",
        "          self.attention_weights = F.softmax(self.attention_weights, dim=1)\n",
        "          #print(self.attention_weights.shape)\n",
        "          \n",
        "          self.attention_weights = self.attention_weights.repeat([1, self.encoded_height])\n",
        "          #print(self.attention_weights.shape)\n",
        "                  \n",
        "          #print(x.shape)\n",
        "          x = x * self.attention_weights\n",
        "          #print(x.shape)\n",
        "          #raise Exception()\n",
        "          \n",
        "          \n",
        "          #print(\"********************\")\n",
        "          #print(x)\n",
        "          #print(attention_w)\n",
        "          #print(X)\n",
        "          #print(\"---------------\")\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        \n",
        "        \n",
        "        x = torch.cat([x, emb], dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = x.unsqueeze(0)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        \n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return h1\n",
        "      \n",
        "decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AbsiEQDIv_v3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRRecognition(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(HTRRecognition, self).__init__()\n",
        "    self.encoder = HTREncoder()\n",
        "    self.decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\")\n",
        "    self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "    self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "    self.START = train_set.start_code\n",
        "    self.STOP = train_set.stop_code\n",
        "    self.recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "    self.old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    self.loss = 0\n",
        "    self.stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    self.stop_symbol.fill_(self.STOP)\n",
        "    self.criterion = nn.NLLLoss()\n",
        "    \n",
        "  def zero_grad(self):\n",
        "    self.encoder.zero_grad()\n",
        "    self.decoder.zero_grad()\n",
        "    \n",
        "  def forward(self, data, target, use_teacher_forcing):\n",
        "    orig_data = data\n",
        "\n",
        "    hidden = self.decoder.makeHidden()   \n",
        "\n",
        "    self.loss = 0\n",
        "    enc = self.encoder(data)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    \n",
        "    self.old_symbol[:, 0] = self.START\n",
        "    \n",
        "    self.hidden_states_ = []\n",
        "\n",
        "    for i in range(0, target.shape[1]):\n",
        "\n",
        "      dec, hidden = self.decoder(s, self.old_symbol, hidden)\n",
        "      self.hidden_states_.append(hidden.unsqueeze(0))\n",
        "      self.recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "      if use_teacher_forcing:\n",
        "        self.old_symbol[:, 0] = target[:, i]\n",
        "      else:\n",
        "        self.old_symbol[:, 0] = self.recognition_result[:, i]\n",
        "      self.loss += self.criterion(dec, target[:, i])\n",
        "    self.length = target.shape[1]\n",
        "    return self.recognition_result[:, 0: target.shape[1]]\n",
        "  \n",
        "  def normed_loss(self):\n",
        "    return self.loss/target.shape[1]\n",
        "   \n",
        "  def backprop(self):\n",
        "    self.loss.backward()\n",
        "    \n",
        "  def hidden_states(self):\n",
        "    r = torch.cat(self.hidden_states_, dim = 0)\n",
        "    r = r.permute(0, 2, 1, 3)\n",
        "    r = r.flatten(start_dim=2)\n",
        "    return r\n",
        "    \n",
        "  def step(self):\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(self.decoder.parameters(), grad_clip)\n",
        "    self.encoder_optimizer.step()\n",
        "    self.decoder_optimizer.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_dQSCL04lmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From https://github.com/aryopg/Professor_Forcing_Pytorch/blob/master/models/losses.py\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, input_length):\n",
        "        super(Discriminator, self).__init__()\n",
        "        from math import floor\n",
        "        self.hidden_cells = 256\n",
        "        self.hidden_layers = 2\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_length = input_length\n",
        "        self.rnn_layers = 2\n",
        "        \n",
        "        gru_input_size = 256*2\n",
        "        \n",
        "        #self.enc = FullyConnectedX([input_size, floor(input_size*0.7), gru_input_size], activation_fn=nn.ReLU())\n",
        "\n",
        "        self.gru = nn.GRU(gru_input_size, hidden_size, self.rnn_layers)\n",
        "        \n",
        "        gru_out = input_length*hidden_size\n",
        "        self.fc = FullyConnectedX([gru_out, floor(gru_out*0.7), floor(gru_out*0.3), 1], activation_fn=nn.ReLU())\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "        \n",
        "    def zero_grad(self):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = torch.zeros(self.input_length, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "        hidden = self.initHidden()\n",
        "        for ei in range(x.shape[0]):\n",
        "            #embedded = self.embedding(x[:, ei])\n",
        "            #embedded = embedded.view(1, batch_size, -1)\n",
        "            #output = embedded\n",
        "            output = x[ei]\n",
        "            #output = self.enc(output)\n",
        "            output = output.unsqueeze(0)\n",
        "            #print(output.shape)\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "            outputs[ei] = output[0, 0]\n",
        "\n",
        "        outputs = outputs.permute(1,0,2)\n",
        "        #print(outputs.shape)\n",
        "        #feat = outputs.contiguous().view(x.shape[0], -1)\n",
        "        feat = outputs.flatten(start_dim=1)\n",
        "        #print(feat.shape)\n",
        "        out = self.fc(feat)\n",
        "        \n",
        "        self.features = feat\n",
        "\n",
        "        return out\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.rnn_layers, batch_size, self.hidden_size, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7kaQayXglGL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_zeros = torch.zeros((batch_size, 1)).to(device)\n",
        "batch_ones = torch.ones((batch_size, 1)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1KQNsFrRYAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = HTRRecognition().to(device)\n",
        "discriminator = Discriminator(256*2, 512, 10).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ZG9dz-viW6d",
        "colab_type": "code",
        "outputId": "3bdaae6c-b034-4ebd-850c-59da25f6a9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6360
        }
      },
      "cell_type": "code",
      "source": [
        "def process_batch(data, target, batch_idx):\n",
        "  generator.zero_grad()\n",
        "  period = 30\n",
        "  for i in range(0, 3):\n",
        "    discriminator.zero_grad()\n",
        "    with torch.no_grad():\n",
        "      free_run_result = generator(data, target, False)\n",
        "      \n",
        "      if (batch_idx % period == 0) and (i == 0):\n",
        "        from random import random\n",
        "        for k in range(0, min(2, target.shape[0])):\n",
        "              decoded =free_run_result[k,0:target.shape[1]]\n",
        "              #plt.imshow(data[k].cpu(), cmap=\"gray\")\n",
        "              #plt.show()\n",
        "              print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      \n",
        "      free_run_hidden = generator.hidden_states().detach()\n",
        "      generator(data, target, True)\n",
        "      teacher_forcing_hidden = generator.hidden_states().detach()\n",
        "    d_free_run = discriminator(free_run_hidden)\n",
        "    d_teacher_forcing = discriminator(teacher_forcing_hidden)\n",
        "    true_loss = F.binary_cross_entropy_with_logits(d_free_run, batch_zeros)\n",
        "    fake_loss = F.binary_cross_entropy_with_logits(d_teacher_forcing, batch_ones)\n",
        "    D_loss = 0.5*(fake_loss + true_loss)\n",
        "    d_val = D_loss.item()\n",
        "    D_loss.backward()\n",
        "    discriminator.optimizer.step()\n",
        "    \n",
        "  generator.zero_grad()\n",
        "  discriminator.zero_grad()\n",
        "  generator(data, target, True)\n",
        "  teacher_forcing_hidden_ = generator.hidden_states()\n",
        "  fake_pred = discriminator(teacher_forcing_hidden_)\n",
        "  tf_val = generator.normed_loss().item()\n",
        "  G_loss = generator.normed_loss() + F.binary_cross_entropy_with_logits(fake_pred, batch_zeros)\n",
        "  g_val = G_loss.item()\n",
        "  G_loss.backward()\n",
        "  generator.step()\n",
        "  if batch_idx % period == 0:\n",
        "    print(\"Batch: %d Descr %.4f TF %.4f Full %.4f\" % (batch_idx, d_val, tf_val, g_val)) \n",
        "  \n",
        "def train(epoch):\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch(use_binarization=False)\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    if target.shape[1] > 5:\n",
        "      return\n",
        "    target = target.to(device)\n",
        "    data = data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    process_batch(data, target, batch_idx)\n",
        "    batch_idx += 1\n",
        "    \n",
        "for i in range(0, 100):\n",
        "  print(\"Epoch %d\" % i)\n",
        "  print(\"***************************************\")\n",
        "  train(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "***************************************\n",
            "  a -> t\n",
            "  I -> o\n",
            "Batch: 0 Descr 0.6966 TF 2.3540 Full 3.1344\n",
            "  at -> af\n",
            "  of -> an\n",
            "Batch: 30 Descr 0.6886 TF 4.8871 Full 5.5150\n",
            "  of -> ff\n",
            "  be -> an\n",
            "Batch: 60 Descr 0.6616 TF 3.7783 Full 4.5121\n",
            "  In -> zn\n",
            "  of -> of\n",
            "Batch: 90 Descr 0.6900 TF 3.6652 Full 4.4322\n",
            "  's -> ot\n",
            "  of -> of\n",
            "Batch: 120 Descr 0.6911 TF 3.5387 Full 4.2361\n",
            "  the -> (tt\n",
            "  got -> ton\n",
            "Batch: 150 Descr 0.7114 TF 7.0844 Full 7.8584\n",
            "  had -> ttt\n",
            "  her -> the\n",
            "Batch: 180 Descr 0.6460 TF 5.9092 Full 6.6867\n",
            "  the -> ttt\n",
            "  was -> the\n",
            "Batch: 210 Descr 0.7216 TF 5.4038 Full 5.9409\n",
            "  the -> ttn\n",
            "  you -> the\n",
            "Batch: 240 Descr 0.7052 TF 4.9518 Full 5.5516\n",
            "  you -> htt\n",
            "  out -> the\n",
            "Batch: 270 Descr 0.6886 TF 5.3393 Full 6.0550\n",
            "  that -> tnnn\n",
            "  sent -> then\n",
            "Batch: 300 Descr 0.6946 TF 10.7114 Full 11.4000\n",
            "  went -> hhtt\n",
            "  What -> that\n",
            "Batch: 330 Descr 0.6527 TF 9.7014 Full 10.5215\n",
            "  that -> thah\n",
            "  full -> that\n",
            "Batch: 360 Descr 1.0030 TF 10.6123 Full 11.0759\n",
            "  order -> thern\n",
            "  Plato -> thern\n",
            "Batch: 390 Descr 0.5675 TF 15.2696 Full 16.2822\n",
            "  which -> there\n",
            "  given -> there\n",
            "Batch: 420 Descr 0.4002 TF 13.7188 Full 14.8973\n",
            "Epoch 1\n",
            "***************************************\n",
            "  a -> t\n",
            "  a -> w\n",
            "Batch: 0 Descr 0.6962 TF 3.8347 Full 4.5894\n",
            "  at -> aa\n",
            "  In -> aa\n",
            "Batch: 30 Descr 0.6932 TF 5.6905 Full 6.3511\n",
            "  to -> ao\n",
            "  in -> ao\n",
            "Batch: 60 Descr 0.0368 TF 4.0192 Full 9.3373\n",
            "  he -> of\n",
            "  in -> of\n",
            "Batch: 90 Descr 0.1228 TF 4.2644 Full 8.4541\n",
            "  it -> of\n",
            "  an -> to\n",
            "Batch: 120 Descr 0.8122 TF 3.7947 Full 4.5442\n",
            "  his -> tnd\n",
            "  how -> tnd\n",
            "Batch: 150 Descr 0.1085 TF 7.7885 Full 15.6809\n",
            "  the -> thd\n",
            "  had -> the\n",
            "Batch: 180 Descr 0.6703 TF 6.3979 Full 7.0713\n",
            "  and -> the\n",
            "  The -> the\n",
            "Batch: 210 Descr 0.1454 TF 6.4979 Full 15.3567\n",
            "  the -> the\n",
            "  the -> the\n",
            "Batch: 240 Descr 0.7493 TF 5.9700 Full 6.3783\n",
            "  are -> the\n",
            "  the -> the\n",
            "Batch: 270 Descr 0.1311 TF 6.0160 Full 9.3422\n",
            "  used -> hean\n",
            "  Then -> here\n",
            "Batch: 300 Descr 0.2523 TF 10.9551 Full 14.6990\n",
            "  1956 -> ther\n",
            "  very -> thet\n",
            "Batch: 330 Descr 0.0653 TF 10.4246 Full 14.8927\n",
            "  find -> were\n",
            "  were -> them\n",
            "Batch: 360 Descr 0.2760 TF 10.5122 Full 17.6867\n",
            "  think -> harer\n",
            "  1960s -> hathe\n",
            "Batch: 390 Descr 1.7867 TF 14.4235 Full 15.0755\n",
            "  scale -> hiree\n",
            "  Prime -> thare\n",
            "Batch: 420 Descr 0.1897 TF 13.7750 Full 16.0063\n",
            "Epoch 2\n",
            "***************************************\n",
            "  3 -> h\n",
            "  a -> t\n",
            "Batch: 0 Descr 0.7376 TF 3.6036 Full 4.5934\n",
            "  of -> aa\n",
            "  if -> an\n",
            "Batch: 30 Descr 0.6703 TF 5.4746 Full 6.3021\n",
            "  so -> an\n",
            "  he -> an\n",
            "Batch: 60 Descr 0.2604 TF 4.3311 Full 6.7004\n",
            "  of -> to\n",
            "  us -> to\n",
            "Batch: 90 Descr 0.7016 TF 3.8960 Full 4.5594\n",
            "  by -> of\n",
            "  or -> of\n",
            "Batch: 120 Descr 0.6752 TF 3.8255 Full 4.5465\n",
            "  and -> toe\n",
            "  are -> tof\n",
            "Batch: 150 Descr 0.4626 TF 6.0703 Full 7.7911\n",
            "  the -> the\n",
            "  the -> the\n",
            "Batch: 180 Descr 0.7290 TF 5.7692 Full 6.3185\n",
            "  for -> the\n",
            "  the -> the\n",
            "Batch: 210 Descr 0.8754 TF 5.6902 Full 6.2611\n",
            "  Sir -> the\n",
            "  two -> the\n",
            "Batch: 240 Descr 0.2617 TF 5.8173 Full 8.9754\n",
            "  was -> the\n",
            "  the -> the\n",
            "Batch: 270 Descr 0.1449 TF 5.5663 Full 8.2802\n",
            "  Rock -> oa96\n",
            "  many -> ther\n",
            "Batch: 300 Descr 0.4305 TF 10.7620 Full 14.3427\n",
            "  well -> mfGJ\n",
            "  even -> then\n",
            "Batch: 330 Descr 0.5447 TF 9.5794 Full 10.3082\n",
            "  that -> 8f2m\n",
            "  fine -> that\n",
            "Batch: 360 Descr 0.6159 TF 9.4765 Full 11.0499\n",
            "  Pearl -> ffffd\n",
            "  blind -> thath\n",
            "Batch: 390 Descr 0.7742 TF 14.0327 Full 14.4017\n",
            "  alone -> udddu\n",
            "  earth -> there\n",
            "Batch: 420 Descr 0.6854 TF 13.1820 Full 13.8355\n",
            "Epoch 3\n",
            "***************************************\n",
            "  a -> r\n",
            "  A -> w\n",
            "Batch: 0 Descr 0.7615 TF 3.7808 Full 4.6404\n",
            "  it -> uu\n",
            "  of -> an\n",
            "Batch: 30 Descr 0.6911 TF 5.2741 Full 6.0990\n",
            "  on -> ud\n",
            "  to -> an\n",
            "Batch: 60 Descr 0.7013 TF 3.8674 Full 4.5048\n",
            "  to -> if\n",
            "  of -> of\n",
            "Batch: 90 Descr 0.6595 TF 3.9246 Full 4.6274\n",
            "  be -> tn\n",
            "  of -> of\n",
            "Batch: 120 Descr 0.6662 TF 3.5473 Full 4.2996\n",
            "  the -> tes\n",
            "  the -> the\n",
            "Batch: 150 Descr 1.2783 TF 6.6473 Full 6.8520\n",
            "  the -> the\n",
            "  was -> the\n",
            "Batch: 180 Descr 0.7254 TF 5.9317 Full 6.3413\n",
            "  her -> the\n",
            "  and -> the\n",
            "Batch: 210 Descr 0.3231 TF 4.8862 Full 8.4286\n",
            "  the -> hhh\n",
            "  the -> the\n",
            "Batch: 240 Descr 0.8061 TF 4.5483 Full 5.0047\n",
            "  his -> ahe\n",
            "  try -> the\n",
            "Batch: 270 Descr 0.1992 TF 4.8949 Full 7.8653\n",
            "  ever -> thee\n",
            "  take -> ther\n",
            "Batch: 300 Descr 0.0615 TF 10.5729 Full 21.2189\n",
            "  five -> atho\n",
            "  plan -> wath\n",
            "Batch: 330 Descr 0.3617 TF 9.5890 Full 12.5604\n",
            "  they -> wtha\n",
            "  much -> thar\n",
            "Batch: 360 Descr 0.3284 TF 10.0948 Full 11.0371\n",
            "  sides -> +wwhh\n",
            "  about -> werer\n",
            "Batch: 390 Descr 0.7257 TF 13.6351 Full 14.1723\n",
            "  visit -> +999:\n",
            "  Mauro -> whise\n",
            "Batch: 420 Descr 1.4592 TF 13.9568 Full 14.0650\n",
            "Epoch 4\n",
            "***************************************\n",
            "  I -> w\n",
            "  a -> t\n",
            "Batch: 0 Descr 0.8152 TF 3.4905 Full 4.7408\n",
            "  as -> ah\n",
            "  at -> an\n",
            "Batch: 30 Descr 0.6864 TF 4.9463 Full 5.5612\n",
            "  on -> ao\n",
            "  by -> at\n",
            "Batch: 60 Descr 0.7333 TF 4.1041 Full 4.7574\n",
            "  in -> os\n",
            "  it -> of\n",
            "Batch: 90 Descr 0.6397 TF 3.6825 Full 4.2637\n",
            "  at -> os\n",
            "  up -> to\n",
            "Batch: 120 Descr 0.6405 TF 3.9900 Full 4.6288\n",
            "  now -> tnd\n",
            "  the -> the\n",
            "Batch: 150 Descr 0.5662 TF 5.6645 Full 6.7004\n",
            "  the -> the\n",
            "  off -> the\n",
            "Batch: 180 Descr 0.6498 TF 6.0127 Full 6.7648\n",
            "  was -> tho\n",
            "  and -> the\n",
            "Batch: 210 Descr 0.4159 TF 5.6559 Full 6.2497\n",
            "  and -> ahe\n",
            "  and -> and\n",
            "Batch: 240 Descr 0.7297 TF 5.4325 Full 6.1222\n",
            "  and -> the\n",
            "  1.4 -> the\n",
            "Batch: 270 Descr 0.6553 TF 4.9380 Full 5.7197\n",
            "  went -> thoi\n",
            "  with -> ande\n",
            "Batch: 300 Descr 1.3572 TF 10.3989 Full 12.6520\n",
            "  that -> thho\n",
            "  well -> thea\n",
            "Batch: 330 Descr 0.5086 TF 10.2694 Full 11.9887\n",
            "  deep -> thhi\n",
            "  spot -> when\n",
            "Batch: 360 Descr 0.6356 TF 10.2623 Full 11.0462\n",
            "  shoal -> whhoh\n",
            "  those -> worer\n",
            "Batch: 390 Descr 0.5965 TF 13.4712 Full 14.9768\n",
            "  which -> hhhih\n",
            "  other -> whind\n",
            "Batch: 420 Descr 0.7333 TF 12.8693 Full 13.4326\n",
            "Epoch 5\n",
            "***************************************\n",
            "  I -> h\n",
            "  4 -> t\n",
            "Batch: 0 Descr 0.8525 TF 3.6346 Full 4.9172\n",
            "  to -> aa\n",
            "  In -> an\n",
            "Batch: 30 Descr 0.6617 TF 4.9062 Full 5.6387\n",
            "  is -> ao\n",
            "  to -> an\n",
            "Batch: 60 Descr 0.7100 TF 3.9086 Full 4.5909\n",
            "  at -> of\n",
            "  of -> of\n",
            "Batch: 90 Descr 0.0629 TF 3.9350 Full 6.6334\n",
            "  is -> if\n",
            "  It -> to\n",
            "Batch: 120 Descr 0.6930 TF 4.6164 Full 5.4887\n",
            "  the -> tor\n",
            "  bar -> the\n",
            "Batch: 150 Descr 0.6591 TF 6.1923 Full 6.9684\n",
            "  the -> the\n",
            "  the -> the\n",
            "Batch: 180 Descr 0.6759 TF 5.7968 Full 6.3781\n",
            "  the -> the\n",
            "  for -> the\n",
            "Batch: 210 Descr 0.8529 TF 5.3602 Full 5.8516\n",
            "  and -> the\n",
            "  his -> the\n",
            "Batch: 240 Descr 0.5980 TF 5.3287 Full 6.6978\n",
            "  kit -> tha\n",
            "  can -> the\n",
            "Batch: 270 Descr 0.4741 TF 5.3034 Full 7.0026\n",
            "  well -> thoo\n",
            "  side -> thea\n",
            "Batch: 300 Descr 0.3955 TF 10.8195 Full 12.8021\n",
            "  know -> thoa\n",
            "  been -> thea\n",
            "Batch: 330 Descr 0.3002 TF 9.9112 Full 10.5885\n",
            "  when -> thoa\n",
            "  Adam -> thea\n",
            "Batch: 360 Descr 0.8613 TF 9.8263 Full 11.8742\n",
            "  women -> hoaae\n",
            "  There -> waene\n",
            "Batch: 390 Descr 0.2426 TF 13.3776 Full 17.0428\n",
            "  cheap -> areee\n",
            "  dozen -> there\n",
            "Batch: 420 Descr 0.6356 TF 13.6710 Full 14.3630\n",
            "Epoch 6\n",
            "***************************************\n",
            "  a -> w\n",
            "  3 -> w\n",
            "Batch: 0 Descr 0.8119 TF 3.8375 Full 5.0855\n",
            "  to -> aa\n",
            "  It -> an\n",
            "Batch: 30 Descr 0.3001 TF 5.2236 Full 8.5854\n",
            "  it -> an\n",
            "  on -> to\n",
            "Batch: 60 Descr 0.6618 TF 4.4583 Full 5.5487\n",
            "  of -> of\n",
            "  of -> of\n",
            "Batch: 90 Descr 0.6914 TF 3.6996 Full 4.3631\n",
            "  TO -> of\n",
            "  be -> of\n",
            "Batch: 120 Descr 0.6529 TF 3.7402 Full 4.3896\n",
            "  had -> the\n",
            "  all -> the\n",
            "Batch: 150 Descr 1.6711 TF 5.9732 Full 9.8746\n",
            "  the -> the\n",
            "  his -> the\n",
            "Batch: 180 Descr 0.6998 TF 5.1986 Full 5.7999\n",
            "  The -> the\n",
            "  not -> the\n",
            "Batch: 210 Descr 0.5561 TF 4.9608 Full 5.9470\n",
            "  who -> the\n",
            "  the -> the\n",
            "Batch: 240 Descr 0.5744 TF 5.1974 Full 6.0325\n",
            "  try -> the\n",
            "  was -> the\n",
            "Batch: 270 Descr 0.7686 TF 4.9167 Full 5.2880\n",
            "  cast -> ther\n",
            "  show -> ther\n",
            "Batch: 300 Descr 0.2445 TF 10.3263 Full 13.1410\n",
            "  Duke -> tndd\n",
            "  then -> that\n",
            "Batch: 330 Descr 0.3739 TF 9.7294 Full 10.2362\n",
            "  last -> hedd\n",
            "  15mc -> that\n",
            "Batch: 360 Descr 0.6379 TF 10.1037 Full 11.1217\n",
            "  comes -> oeeee\n",
            "  dance -> thath\n",
            "Batch: 390 Descr 0.4316 TF 13.8992 Full 14.9052\n",
            "  Pearl -> slldd\n",
            "  shirt -> whise\n",
            "Batch: 420 Descr 0.6857 TF 12.6119 Full 13.2469\n",
            "Epoch 7\n",
            "***************************************\n",
            "  a -> w\n",
            "  I -> t\n",
            "Batch: 0 Descr 0.7847 TF 3.6875 Full 4.6837\n",
            "  of -> ai\n",
            "  be -> an\n",
            "Batch: 30 Descr 0.6488 TF 4.9645 Full 5.7509\n",
            "  in -> bf\n",
            "  We -> an\n",
            "Batch: 60 Descr 0.6730 TF 4.2523 Full 5.4299\n",
            "  in -> ts\n",
            "  to -> of\n",
            "Batch: 90 Descr 0.8474 TF 3.7291 Full 4.5014\n",
            "  in -> ts\n",
            "  or -> of\n",
            "Batch: 120 Descr 0.6904 TF 3.6035 Full 4.2527\n",
            "  was -> tnd\n",
            "  all -> the\n",
            "Batch: 150 Descr 0.6413 TF 6.3855 Full 8.3328\n",
            "  and -> tse\n",
            "  and -> the\n",
            "Batch: 180 Descr 0.5298 TF 5.7159 Full 7.3506\n",
            "  How -> hoo\n",
            "  and -> the\n",
            "Batch: 210 Descr 0.6568 TF 5.1663 Full 6.0883\n",
            "  lay -> aoe\n",
            "  'll -> the\n",
            "Batch: 240 Descr 0.3461 TF 5.5803 Full 6.7430\n",
            "  way -> the\n",
            "  his -> the\n",
            "Batch: 270 Descr 0.1830 TF 5.7340 Full 8.3638\n",
            "  them -> ther\n",
            "  help -> ther\n",
            "Batch: 300 Descr 0.7390 TF 9.5884 Full 9.9849\n",
            "  play -> toor\n",
            "  less -> than\n",
            "Batch: 330 Descr 0.3694 TF 9.1322 Full 11.1585\n",
            "  past -> whor\n",
            "  ever -> wall\n",
            "Batch: 360 Descr 0.1806 TF 8.8986 Full 13.3563\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}