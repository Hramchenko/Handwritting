{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V10 of EQ_no_prof simpl new_proffs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/V10_of_EQ_no_prof_simpl_new_proffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "492986ab-c37b-4cd6-d2bc-801172832c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "image_width = 1500\n",
        "image_height = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqHNfBMaLYmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from xml.dom import minidom\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from random import random\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "class ArtificialHandwrittingObject:\n",
        "    \n",
        "    def __init__(self, name, img):\n",
        "        self.name = name\n",
        "        self.image = img\n",
        "\n",
        "class ArtificialHandwritting:\n",
        "    \n",
        "    def __init__(self, datasetDir, scale, image_width, image_height, encode_word):\n",
        "        self.scale = scale\n",
        "        self.height = image_height\n",
        "        self.width = image_width\n",
        "        self.datasetDirectory = datasetDir\n",
        "        self.data = {}\n",
        "        self.initOffsets()\n",
        "        self.encode_word = encode_word\n",
        "        \n",
        "        for f_name in os.listdir(datasetDir):\n",
        "            if not f_name.endswith(\".xml\"):\n",
        "                continue\n",
        "            full_path = datasetDir + f_name\n",
        "            print(full_path)\n",
        "            self.process_xml(full_path)\n",
        "            \n",
        "    def initOffsets(self):\n",
        "        self.offsets = {\n",
        "            'a': 0,\n",
        "            'b': -0.1,\n",
        "            'c': 0,\n",
        "            'd': 0,\n",
        "            'e': 0,\n",
        "            'f': -0.2,\n",
        "            'g': +0.3,\n",
        "            'h': 0,\n",
        "            'i': -0.15,\n",
        "            'j': +0.2,\n",
        "            'k': 0,\n",
        "            'l': -0.15,\n",
        "            'm': 0,\n",
        "            'n': 0,\n",
        "            'o': 0,\n",
        "            'p': 0,\n",
        "            'q': +0.2,\n",
        "            'r': 0,\n",
        "            's': 0,\n",
        "            't': -0.2,\n",
        "            'u': 0,\n",
        "            'v': 0,\n",
        "            'w': 0,\n",
        "            'x': 0,\n",
        "            'y': +0.2,\n",
        "            'z': 0,\n",
        "        }\n",
        "        \n",
        "        \n",
        "        \n",
        "    def getValue(self, node, name):\n",
        "        v  = node.getElementsByTagName(name) \n",
        "        if len(v) != 1:\n",
        "            raise Exception()\n",
        "        v = v[0].firstChild.data\n",
        "        return v\n",
        "    \n",
        "    def make_random_batch(self, word_len, rand_x):\n",
        "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "        texts = []\n",
        "        datas = []\n",
        "        #data = torch.FloatTensor(batch_size, self.image_height, self.image_width)\n",
        "        for batch_idx in range(0, batch_size):\n",
        "          w = \"\"\n",
        "          for i in range(0, word_len):\n",
        "            s_idx = floor(random()*len(alphabet))\n",
        "            s = alphabet[s_idx]\n",
        "            w += s\n",
        "          x = floor(random()*rand_x)\n",
        "          d, _ = self.make_word(w, x)\n",
        "          d = d = torch.as_tensor(d)#.unsqueeze(0)\n",
        "          \n",
        "          datas.append(d)\n",
        "          w = self.encode_word(w)\n",
        "          texts.append(w)\n",
        "        t = torch.stack(texts)\n",
        "        data = torch.stack(datas, dim=0)\n",
        "        return data, t\n",
        "          \n",
        "       \n",
        "    def make_word(self, word, x_start):\n",
        "        center = self.height*0.5\n",
        "        result = sp.ones([self.height, self.width], dtype=\"float32\")\n",
        "        candidates = []\n",
        "        for k in self.data.keys():\n",
        "            d = self.data[k]\n",
        "            flag = True\n",
        "            for s in word:\n",
        "                if s not in d:\n",
        "                    flag = False\n",
        "                    break\n",
        "            if flag:\n",
        "                candidates.append(k)\n",
        "        idx = floor(random()*len(candidates))\n",
        "        if (len(candidates) == 0):\n",
        "          print(\"Bad word: \" + word)\n",
        "          raise Exception()\n",
        "        \n",
        "        img_file = candidates[idx]\n",
        "        page = self.data[img_file]\n",
        "        x = x_start\n",
        "        for s in word:\n",
        "            s_idx = floor(len(page[s])*random())\n",
        "            obj_im = page[s][s_idx].image\n",
        "            c = 0.5*obj_im.shape[0]\n",
        "            dy = 0 + floor(center - c + obj_im.shape[0]*(self.offsets[s]+random()*0.05))#+random()*0.05))\n",
        "            #print(dy)\n",
        "            #print(obj_im.dtype)\n",
        "            result[dy: dy + obj_im.shape[0], x: x+obj_im.shape[1]] = obj_im\n",
        "            x += obj_im.shape[1]\n",
        "        return result, img_file  \n",
        "      \n",
        "            \n",
        "    def process_xml(self, f_path):\n",
        "        \n",
        "        xml_file = minidom.parse(f_path)\n",
        "        #xml_file.normalize()\n",
        "        annotation = xml_file.getElementsByTagName('annotation')\n",
        "        if len(annotation) != 1:\n",
        "            raise Exception()\n",
        "        annotation = annotation[0]\n",
        "        img_name = self.getValue(annotation, 'filename')\n",
        "        image_file = self.datasetDirectory + img_name\n",
        "        img = cv2.imread(image_file, 0)\n",
        "        print(image_file)\n",
        "        objects = annotation.getElementsByTagName('object')\n",
        "        #print(len(objects))\n",
        "        content = {}\n",
        "        for obj in objects:\n",
        "            name  = self.getValue(obj, 'name')\n",
        "            if name not in content:\n",
        "                content[name] = []\n",
        "            x_min = int(self.getValue(obj, 'xmin'))\n",
        "            y_min = int(self.getValue(obj, 'ymin'))\n",
        "            x_max = int(self.getValue(obj, 'xmax'))\n",
        "            y_max = int(self.getValue(obj, 'ymax'))\n",
        "            obj_im = img[y_min: y_max, x_min: x_max]\n",
        "            s = self.scale\n",
        "            sh = obj_im.shape\n",
        "            obj_im = cv2.resize(obj_im, (floor(sh[1]*s), floor(sh[0]*s)), cv2.INTER_LANCZOS4)\n",
        "            if obj_im.shape[0] > self.height:\n",
        "                continue\n",
        "            obj_im = sp.float32(obj_im)    \n",
        "            obj_im = (obj_im - obj_im.min())/(obj_im.max() - obj_im.min())\n",
        "            handwr_obj = ArtificialHandwrittingObject(name, obj_im)\n",
        "            content[name].append(handwr_obj)\n",
        "        if img_name in self.data:\n",
        "            raise Exception()\n",
        "        self.data[img_name] = content\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "2a57529e-977c-497c-d5c2-4fc95157696e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "\n",
        "pad_length=-1\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPROjYtaLvLS",
        "colab_type": "code",
        "outputId": "0907137c-a332-4644-b024-3f77de6d8d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "artificial_data = ArtificialHandwritting(\"./MyLetters/\", 1.0, image_width, image_height, train_set.encode_word)\n",
        "data, target = artificial_data.make_random_batch(3, 400)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./MyLetters/a01-107u.xml\n",
            "./MyLetters/a01-107u.png\n",
            "./MyLetters/my.xml\n",
            "./MyLetters/my.jpg\n",
            "./MyLetters/a01-053u.xml\n",
            "./MyLetters/a01-053u.png\n",
            "./MyLetters/a01-128.xml\n",
            "./MyLetters/a01-128.png\n",
            "./MyLetters/a01-107.xml\n",
            "./MyLetters/a01-107.png\n",
            "./MyLetters/a01-026u.xml\n",
            "./MyLetters/a01-026u.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yx2SxZgGT0Z-",
        "colab_type": "code",
        "outputId": "c446cf2e-57b4-42a0-c3e9-243ad4f08634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "artificial_data.data[\"my.jpg\"].keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 'u', 't', 'v', 'w', 'x', 'y', 'z', 'j'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wceUSjfAWM1v",
        "colab_type": "code",
        "outputId": "52005de5-060d-423e-fa62-33245619ae59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(data[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAABeCAYAAADyiGdeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmYiISAb+B1FUzd7KdahDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 200, 1500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOlJkOzNgngX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Layers import *\n",
        "from HTREncoder import *\n",
        "from HTRDecoder import *\n",
        "from HTRDiscriminator import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "outputId": "1a483f73-13c1-44bc-b778-80d4ea4d2615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 9, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "cXja4G8p7KKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_onehot(x, n):\n",
        "    one_hot = torch.zeros((x.shape[0], n)).to(device)\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoderResult:\n",
        "  \n",
        "  def __init__(self):\n",
        "    None\n",
        "\n",
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, batch_size, ntoken, encoded_width=92, encoded_height=64, batchnorm=False, dropout=True, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = features_size\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "            hidden_m = hidden.permute(1, 0, 2)\n",
        "            hidden_m = hidden_m.flatten(start_dim=1)\n",
        "            #print(x.shape)\n",
        "            #print(hidden_m.shape)\n",
        "            x_m = x.flatten(start_dim=1)\n",
        "            attention_inp = torch.cat([x_m, hidden_m], dim=1).detach()\n",
        "            self.attention_weights = self.attention(attention_inp)\n",
        "            self.attention_weights = F.softmax(self.attention_weights, dim=1).unsqueeze(1)\n",
        "            #print(\"iiiif\")\n",
        "            #print(self.attention_weights.shape)\n",
        "            \n",
        "            self.attention_weights = self.attention_weights.repeat([1, self.encoded_height, 1])\n",
        "            #print(\"fffff\")\n",
        "            #print(x.shape)\n",
        "            #print(self.attention_weights.shape)\n",
        "            x = x * self.attention_weights\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        x = torch.cat([x.flatten(start_dim=1), emb], dim=1)\n",
        "        x = x.unsqueeze(0)\n",
        "        result = HTRDecoderResult()\n",
        "        result.rnn_input = x\n",
        "        result.input_hidden = hidden\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        result.x = x\n",
        "        result.hidden = hidden\n",
        "        return result\n",
        "      \n",
        "    def makeHidden(self):\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            h2 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return (h1, h2)\n",
        "        else:\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return h1\n",
        "\n",
        "\n",
        "decoder = HTRDecoder(batch_size, len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHbnIOOP03r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(max_size):\n",
        "  print(\"Testing...\")\n",
        "  \n",
        "  freq = 20\n",
        "  \n",
        "  test_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  \n",
        "  test_loss = 0\n",
        "  \n",
        "  with torch.no_grad():  \n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "\n",
        "      if True:\n",
        "        l = 1 + floor((max_size - 1)*random())\n",
        "        data, target = artificial_data.make_random_batch(l, 200)\n",
        "        orig_data = data*255;\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)  \n",
        "      else:  \n",
        "        orig_data, target = batch\n",
        "        data = orig_data/255.0\n",
        "        #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)\n",
        "      hidden = decoder.makeHidden().to(device)    \n",
        "\n",
        "      loss = 0\n",
        "      enc = encoder(data)\n",
        "      #print(enc.shape)\n",
        "      s = enc\n",
        "      #print(s.shape)\n",
        "      #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "\n",
        "      old_symbol[:, 0] = START\n",
        "\n",
        "      for i in range(0, target.shape[1]):\n",
        "\n",
        "        decoder_result = decoder(s, old_symbol, hidden)\n",
        "        dec = decoder_result.x\n",
        "        hidden = decoder_result.hidden\n",
        "\n",
        "        recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "\n",
        "        loss += criterion(dec, target[:, i])\n",
        "      c_loss += loss.item()/(target.shape[1] + 0)\n",
        "      test_loss += loss.item()/(target.shape[1] + 0)\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        if True:#not use_teacher_forcing:\n",
        "          for k in range(0, min(3, target.shape[0])):\n",
        "              decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "              plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "              plt.show()\n",
        "              print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "        c_loss /= freq \n",
        "        print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "        c_loss = 0\n",
        "      batch_idx += 1  \n",
        "  print(\"Test loss: %f\" % (test_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dvEYjtLYlcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator = HTRDiscriminator(batch_size, 256*2, 512, 10, len(train_set.codes)).to(device)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zel5bfutWKKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_discriminator(s, target, use_teacher_forcing, train_mode, discriminator_target):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  train_mask = torch.ByteTensor(batch_size).to(device)\n",
        "  train_mask[:] = 1\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)    \n",
        "  discriminator_loss = 0\n",
        "  discriminator_hidden = discriminator.makeHidden().to(device)\n",
        "\n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    decoder_outputs = dec.topk(1, dim=1)[1].flatten()\n",
        "\n",
        "    \n",
        "    if train_mode:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden.detach(), decoder_result.input_hidden.detach(),decoder_result.rnn_input.detach(), decoder_outputs.detach(), discriminator_target)\n",
        "    else:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden, decoder_result.input_hidden, decoder_result.rnn_input, decoder_outputs, discriminator_target)\n",
        "      \n",
        "    if i != 0:\n",
        "      discriminator_loss += dl\n",
        "\n",
        "    recognition_result[:, i] = decoder_outputs.detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]    \n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        " \n",
        "  discriminator_loss /= target.shape[1]\n",
        "  return (recognition_result, loss, discriminator_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yrtvy2ikWNwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_decoder(s, target, use_teacher_forcing):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)  \n",
        "  \n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]\n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        "  return (recognition_result, loss)\n",
        "\n",
        "\n",
        "batch_zeros = torch.zeros(batch_size, 1).to(device)\n",
        "batch_ones = torch.ones(batch_size, 1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DxYdkXeTnNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 30\n",
        "  \n",
        "  train_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "\n",
        "  \n",
        "  train_loss = 0\n",
        "  discr_applied = 0\n",
        "  \n",
        "  while True:\n",
        "    if batch_idx > 400:\n",
        "      break\n",
        "    if True:\n",
        "      l = 1 + floor((max_size - 1)*random())\n",
        "      data, target = artificial_data.make_random_batch(l, 200)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "    else:\n",
        "      batch = train_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "      orig_data, target = batch\n",
        "      data = orig_data/255.0\n",
        "      #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    \n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc\n",
        "    #s = enc.permute(1, 0, 2)\n",
        "    #s = s.squeeze(0)\n",
        "    #print(s.shape)\n",
        "    #print(s.shape)\n",
        "    #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    #s = s.flatten(start_dim=1).squeeze(0)\n",
        "    s = s.unsqueeze(0)\n",
        "    #print(s.shape)\n",
        "    \n",
        "    discr_loss = 1\n",
        "    if False: #True#target.shape[1] > 1:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      _, _, tf_loss = apply_discriminator(s, target, True, True, batch_ones)\n",
        "      _, _, fr_loss = apply_discriminator(s, target, False, True, batch_zeros)\n",
        "      dl = tf_loss + fr_loss\n",
        "      discr_loss = dl.item()\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        print(\"Discr loss: %f\" %(dl.item()))\n",
        "      #print(dl)\n",
        "      dl.backward()\n",
        "      discriminator_optimizer.step()\n",
        "   \n",
        "\n",
        "      #     use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "      # #    recognition_result, loss, discriminator_loss = apply_discriminator(s, target, use_teacher_forcing, True, batch_zeros)\n",
        "      #     recognition_result, loss = apply_decoder(s, target, use_teacher_forcing)\n",
        "    batch_loss = None  \n",
        "    if False:#discr_loss < 0.3:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      recognition_result, loss, discriminator_loss = apply_discriminator(s, target, True, False, batch_zeros)\n",
        "      batch_loss = loss.item()\n",
        "      loss = loss + discriminator_loss\n",
        "      discr_applied += 1\n",
        "      #print(\"Apply descr...\")\n",
        "    else:\n",
        "      recognition_result, loss = apply_decoder(s, target, True)\n",
        "      batch_loss = loss.item()\n",
        "\n",
        "      \n",
        "      \n",
        "    c_loss += batch_loss/(target.shape[1] + 0)\n",
        "    train_loss += batch_loss/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      \n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "  print(\"Discr applied %d times.\"%discr_applied)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xrGl2QIsP13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = \"/gdrive/My Drive/v9.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3iWRuawx-Dx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state = torch.load(file_name)\n",
        "# encoder.load_state_dict(state[\"encoder\"])\n",
        "# encoder_optimizer.load_state_dict(state[\"encoder_optimizer\"])\n",
        "# decoder.load_state_dict(state[\"decoder\"])\n",
        "# decoder_optimizer.load_state_dict(state[\"decoder_optimizer\"])\n",
        "# discriminator.load_state_dict(state[\"discriminator\"])\n",
        "# discriminator_optimizer.load_state_dict(state[\"discriminator_optimizer\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJBi4bwwywG6",
        "colab_type": "code",
        "outputId": "1b0e4a64-f86d-4448-c817-a137b344f2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1065
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 100):\n",
        "  max_size = 2\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "  Batch: 30 Loss: 7.295249287287394\n",
            "  Batch: 60 Loss: 4.558819262186686\n",
            "  Batch: 90 Loss: 3.9972149610519407\n",
            "  Batch: 120 Loss: 3.701356569925944\n",
            "  Batch: 150 Loss: 3.5436795949935913\n",
            "  Batch: 180 Loss: 3.4509010155995687\n",
            "  Batch: 210 Loss: 3.4013819217681887\n",
            "  Batch: 240 Loss: 3.3678136428197223\n",
            "  Batch: 270 Loss: 3.3503577788670857\n",
            "  Batch: 300 Loss: 3.345582048098246\n",
            "  Batch: 330 Loss: 3.3353366454442344\n",
            "  Batch: 360 Loss: 3.323920996983846\n",
            "  Batch: 390 Loss: 3.326951853434245\n",
            "Train loss: 3.823204\n",
            "Discr applied 0 times.\n",
            "Testing...\n",
            "Test loss: 3.320430\n",
            "Training epoch 1...\n",
            "  Batch: 30 Loss: 3.4293203353881836\n",
            "  Batch: 60 Loss: 3.3277196804682414\n",
            "  Batch: 90 Loss: 3.3077754894892375\n",
            "  Batch: 120 Loss: 3.315673995018005\n",
            "  Batch: 150 Loss: 3.3151301542917886\n",
            "  Batch: 180 Loss: 3.3044222672780355\n",
            "  Batch: 210 Loss: 3.3075642347335816\n",
            "  Batch: 240 Loss: 3.305756711959839\n",
            "  Batch: 270 Loss: 3.3020386616388957\n",
            "  Batch: 300 Loss: 3.3108496109644574\n",
            "  Batch: 330 Loss: 3.3008629163106282\n",
            "  Batch: 360 Loss: 3.3024919748306276\n",
            "  Batch: 390 Loss: 3.299975037574768\n",
            "Train loss: 3.308705\n",
            "Discr applied 0 times.\n",
            "Testing...\n",
            "Test loss: 3.290153\n",
            "Training epoch 2...\n",
            "  Batch: 30 Loss: 3.410560051600138\n",
            "  Batch: 60 Loss: 3.297041900952657\n",
            "  Batch: 90 Loss: 3.303414066632589\n",
            "  Batch: 120 Loss: 3.2979832251866656\n",
            "  Batch: 150 Loss: 3.2952052036921184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b121bc4025e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-3b4ccee49d6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, max_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0mrecognition_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HsfEczGzx_ph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state_dict = {\n",
        "#         \"encoder\": encoder.state_dict(),\n",
        "#         \"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
        "#         \"decoder\": decoder.state_dict(),\n",
        "#         \"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
        "#         \"discriminator\": discriminator.state_dict(),\n",
        "#         \"discriminator_optimizer\": discriminator_optimizer.state_dict(),\n",
        "#       }\n",
        "\n",
        "# torch.save(state_dict, file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8IIhQBGsOoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrN5kEctKjQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data, target = artificial_data.make_random_batch(3, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBYhXSxDKjtf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCBCuthGKmW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 30):\n",
        "  plt.imshow(data[i])\n",
        "  plt.show()\n",
        "  print(test_set.decode_word(target[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obmiumeGK5E9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}