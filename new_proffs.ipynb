{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "new_proffs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/new_proffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "4c0e7d9d-c4a7-4d84-9476-a553cfba089c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "5c199eda-28dd-4d23-9236-8cbb201265ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "image_width = 1500\n",
        "image_height = 200\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QuAkNIOOQkar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnectedX(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, last_fn=None):\n",
        "        super(FullyConnectedX, self).__init__()\n",
        "        layers = []\n",
        "        self.flatten = flatten\n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "        else: \n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        if last_fn is not None:\n",
        "            layers.append(last_fn)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.flatten:\n",
        "            x = x.view(x.shape[0], -1)\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=False, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 4, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([4, 16, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([16, 32, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([32, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None),\n",
        "        ConvLayer([64, 64, 1], padding=0, stride=(1,11), bn=batchnorm, pool_layer=None))\n",
        "        \n",
        "        #self.fc = FullyConnectedX([64*15*49, 64*49*3, 64*49], activation_fn=nn.ReLU())\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = h.squeeze(-1)\n",
        "        #h = h.flatten(start_dim=1)\n",
        "        #h = self.fc(h)\n",
        "        #h = F.max_pool2d(h, [1, h.size(1)], padding=[0, 0])\n",
        "        #h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([2, 3, 0, 1])\n",
        "        \n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "outputId": "989d68ab-a9e1-4858-bf79-d3918af44892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 92])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "BEG3WoC71hQb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Co0VksJ81rz0",
        "colab_type": "code",
        "outputId": "0f33d250-102b-44a0-ef37-72406df7b100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "64*15*49\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47040"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "cXja4G8p7KKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_onehot(x, n):\n",
        "    one_hot = torch.zeros((x.shape[0], n)).to(device)\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoderResult:\n",
        "  \n",
        "  def __init__(self):\n",
        "    None\n",
        "\n",
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=92, encoded_height=64, batchnorm=False, dropout=True, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = floor(features_size*0.3)\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        self.fc = FullyConnectedX([features_size, floor(features_size*0.7), floor(features_size*0.5), lstm_inp_size], activation_fn=nn.ReLU(), last_fn=nn.Tanh())\n",
        "        \n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "          \n",
        "          hidden_m = hidden.permute(1, 0, 2)\n",
        "\n",
        "          hidden_m = hidden_m.flatten(start_dim=1)\n",
        "          \n",
        "          attention_inp = torch.cat([x, hidden_m], dim=1).detach()\n",
        "          self.attention_weights = self.attention(attention_inp)\n",
        "          \n",
        "          self.attention_weights = F.softmax(self.attention_weights, dim=1)\n",
        "          \n",
        "          self.attention_weights = self.attention_weights.repeat([1, self.encoded_height])\n",
        "                  \n",
        "          x = x * self.attention_weights\n",
        "\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        \n",
        "        \n",
        "        x = torch.cat([x, emb], dim=1)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        x = x.unsqueeze(0)\n",
        "        \n",
        "        result = HTRDecoderResult()\n",
        "        result.rnn_input = x\n",
        "        result.input_hidden = hidden\n",
        "        \n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        \n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        result.x = x\n",
        "        result.hidden = hidden\n",
        "        \n",
        "        return result\n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(self.lstm_layers, batch_size, self.lstm_size).to(device)\n",
        "        return h1\n",
        "      \n",
        "decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHbnIOOP03r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(max_size=4):\n",
        "  print(\"Testing...\")\n",
        "  \n",
        "  freq = 20\n",
        "  \n",
        "  test_set.to_start(max_size)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  \n",
        "  test_loss = 0\n",
        "  \n",
        "  with torch.no_grad():  \n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "\n",
        "      orig_data, target = batch\n",
        "      data = orig_data/255.0\n",
        "      data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "      target = target.to(device)\n",
        "      hidden = decoder.makeHidden()    \n",
        "\n",
        "      loss = 0\n",
        "      enc = encoder(data)\n",
        "      #print(enc.shape)\n",
        "      s = enc.permute(1, 0, 2)\n",
        "      #print(s.shape)\n",
        "      s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "\n",
        "      old_symbol[:, 0] = START\n",
        "\n",
        "      for i in range(0, target.shape[1]):\n",
        "\n",
        "        decoder_result = decoder(s, old_symbol, hidden)\n",
        "        dec = decoder_result.x\n",
        "        hidden = decoder_result.hidden\n",
        "\n",
        "        recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "\n",
        "        loss += criterion(dec, target[:, i])\n",
        "      c_loss += loss.item()/(target.shape[1] + 0)\n",
        "      test_loss += loss.item()/(target.shape[1] + 0)\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        if True:#not use_teacher_forcing:\n",
        "          for k in range(0, min(3, target.shape[0])):\n",
        "              decoded = recognition_result[k,0:target.shape[1]]\n",
        "              plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "              plt.show()\n",
        "              print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "        c_loss /= freq \n",
        "        print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "        c_loss = 0\n",
        "      batch_idx += 1  \n",
        "  print(\"Test loss: %f\" % (test_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dvEYjtLYlcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# From https://github.com/aryopg/Professor_Forcing_Pytorch/blob/master/models/losses.py\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, input_length, symbs_cnt):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(symbs_cnt, 128)\n",
        "        \n",
        "        from math import floor\n",
        "        self.hidden_cells = 256\n",
        "        self.hidden_layers = 2\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_length = input_length\n",
        "        self.rnn_layers = 2\n",
        "        \n",
        "        input_size = 512 + 1804 + 128\n",
        "        gru_input_size = 256*2\n",
        "        \n",
        "        self.enc = FullyConnectedX([input_size, floor(input_size*0.7), gru_input_size], activation_fn=nn.ReLU())\n",
        "\n",
        "        self.gru = nn.GRU(gru_input_size, hidden_size, self.rnn_layers)\n",
        "        \n",
        "        gru_out = hidden_size\n",
        "        self.fc = FullyConnectedX([gru_out, floor(gru_out*0.7), floor(gru_out*0.3), 1], activation_fn=nn.ReLU())\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "        \n",
        "    def zero_grad(self):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "    def apply(self, hidden, hidden_states, dec_inputs, dec_outputs, targets):\n",
        "        emb_outputs = self.embedding(dec_outputs)#.permute(1, 0, 2)\n",
        "        hidden_states = hidden_states.permute(1, 0, 2).flatten(start_dim=1)\n",
        "        dec_inputs = dec_inputs.squeeze(0)\n",
        "#         print(emb_outputs.shape)\n",
        "#         print(hidden_states.shape)\n",
        "#         print(dec_inputs.shape)\n",
        "        full_input = torch.cat([hidden_states, dec_inputs, emb_outputs], dim=1)\n",
        "        output = self.enc(full_input)\n",
        "        output = output.unsqueeze(0)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output= output.squeeze(0)\n",
        "        out = self.fc(output)\n",
        "#         print(out.shape)\n",
        "#         print(targets.shape)\n",
        "        loss = F.binary_cross_entropy_with_logits(out, targets)\n",
        "        return loss, hidden\n",
        "\n",
        "    def makeHidden(self):\n",
        "        return torch.zeros(self.rnn_layers, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "      \n",
        "discriminator = Discriminator(256*2, 512, 10, len(train_set.codes)).to(device)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zel5bfutWKKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_discriminator(s, target, use_teacher_forcing, train_mode, discriminator_target):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "\n",
        "\n",
        "  hidden = decoder.makeHidden()    \n",
        "  discriminator_loss = 0\n",
        "  discriminator_hidden = discriminator.makeHidden()\n",
        "\n",
        "  for i in range(0, target.shape[1]):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    decoder_outputs = dec.topk(1, dim=1)[1].flatten()\n",
        "\n",
        "    \n",
        "    if train_mode:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden.detach(), decoder_result.input_hidden.detach(),decoder_result.rnn_input.detach(), decoder_outputs.detach(), discriminator_target)\n",
        "    else:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden, decoder_result.input_hidden, decoder_result.rnn_input, decoder_outputs, discriminator_target)\n",
        "      \n",
        "    if i != 0:\n",
        "      discriminator_loss += dl\n",
        "\n",
        "    recognition_result[:, i] = decoder_outputs.detach()\n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target[:, i]\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target[:, i])\n",
        "  if target.shape[1] > 1:\n",
        "    discriminator_loss /= target.shape[1] - 1\n",
        "  return (recognition_result, loss, discriminator_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yrtvy2ikWNwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_decoder(s, target, use_teacher_forcing):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  hidden = decoder.makeHidden()    \n",
        "\n",
        "  for i in range(0, target.shape[1]):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target[:, i]\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target[:, i])\n",
        "  return (recognition_result, loss)\n",
        "\n",
        "\n",
        "batch_zeros = torch.zeros(batch_size, 1).to(device)\n",
        "batch_ones = torch.ones(batch_size, 1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DxYdkXeTnNB",
        "colab_type": "code",
        "outputId": "afaa6e7c-b4b6-4fcb-f229-f51aa5ec5e46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 100\n",
        "  \n",
        "  train_set.to_start(max_size)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "\n",
        "  \n",
        "  train_loss = 0\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    \n",
        "    \n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    orig_data, target = batch\n",
        "\n",
        "    data = orig_data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    \n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    #print(s.shape)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        " \n",
        "\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "\n",
        "#    recognition_result, loss, discriminator_loss = apply_discriminator(s, target, use_teacher_forcing, True, batch_zeros)\n",
        "\n",
        "    recognition_result, loss = apply_decoder(s, target, use_teacher_forcing)\n",
        "\n",
        "      \n",
        "      \n",
        "    c_loss += loss.item()/(target.shape[1] + 0)\n",
        "    train_loss += loss.item()/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1]]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "\n",
        "for i in range(0, 100):\n",
        "  max_size = 5\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "TF: True\n",
            "  Batch: 100 Loss: 2.0168311417102815\n",
            "TF: True\n",
            "  Batch: 200 Loss: 2.0264943830172215\n",
            "TF: True\n",
            "  Batch: 300 Loss: 1.9182803213596347\n",
            "Train loss: 1.972748\n",
            "Testing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncVE3MysRF3UzsxJxUTczK5H/AzdZbboX+XCDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  too -> tho\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXNTNzErERd3MrET+D3IJQrHTgFL+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  and -> wnd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  the -> the\n",
            "  Batch: 20 Loss: 2.0399491945902506\n",
            "Test loss: 1.899273\n",
            "Training epoch 1...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f0cae4f2e03e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f0cae4f2e03e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, max_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_data\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ICTm22T6Tnoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raise Exception()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YGkeE9bqRYwt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 100\n",
        "  \n",
        "  train_set.to_start(max_size)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "\n",
        "  \n",
        "  train_loss = 0\n",
        "  \n",
        "  while True:\n",
        "\n",
        "    \n",
        "    \n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    orig_data, target = batch\n",
        "\n",
        "    data = orig_data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    \n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    #print(s.shape)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        " \n",
        "\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "\n",
        "    loss = 0\n",
        "    START = train_set.start_code\n",
        "    STOP = train_set.stop_code\n",
        "    recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "    old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "    stop_symbol.fill_(STOP)\n",
        "    old_symbol[:, 0] = START\n",
        "    \n",
        "    \n",
        " \n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    for i in range(0, target.shape[1]):\n",
        "\n",
        "      decoder_result = decoder(s, old_symbol, hidden)\n",
        "      dec = decoder_result.x\n",
        "      hidden = decoder_result.hidden\n",
        "      \n",
        "      recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "      if use_teacher_forcing:\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "      else:\n",
        "        old_symbol[:, 0] = recognition_result[:, i]\n",
        "      #import pdb; pdb.set_trace()\n",
        "\n",
        "      loss += criterion(dec, target[:, i])\n",
        "      \n",
        "      \n",
        "      \n",
        "    c_loss += loss.item()/(target.shape[1] + 0)\n",
        "    train_loss += loss.item()/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1]]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "\n",
        "for i in range(0, 100):\n",
        "  max_size = 5\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pft77RKvOmqZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 100\n",
        "  \n",
        "  train_set.to_start(max_size)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  \n",
        "  train_loss = 0\n",
        "  \n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    orig_data, target = batch\n",
        "\n",
        "    data = orig_data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    #print(s.shape)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    \n",
        "    old_symbol[:, 0] = START\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "\n",
        "    for i in range(0, target.shape[1]):\n",
        "\n",
        "      decoder_result = decoder(s, old_symbol, hidden)\n",
        "      dec = decoder_result.x\n",
        "      hidden = decoder_result.hidden\n",
        "      \n",
        "      recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "      if use_teacher_forcing:\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "      else:\n",
        "        old_symbol[:, 0] = recognition_result[:, i]\n",
        "      #import pdb; pdb.set_trace()\n",
        "\n",
        "      loss += criterion(dec, target[:, i])\n",
        "    c_loss += loss.item()/(target.shape[1] + 0)\n",
        "    train_loss += loss.item()/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1]]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "\n",
        "for i in range(0, 100):\n",
        "  max_size = 5\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t-t0Hnh9AZc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 100\n",
        "  \n",
        "  train_set.to_start(max_size)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  \n",
        "  train_loss = 0\n",
        "  \n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    orig_data, target = batch\n",
        "\n",
        "    data = orig_data/255.0\n",
        "    data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc.permute(1, 0, 2)\n",
        "    #print(s.shape)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    \n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "    recognition_result, loss = decode(target, s, use_teacher_forcing)\n",
        "    c_loss += loss.item()/(target.shape[1] + 0)\n",
        "\n",
        "    train_loss += loss.item()/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1]]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(decoded))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    \n",
        "    # Не убирать\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "\n",
        "for i in range(0, 100):\n",
        "  max_size = 5\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u94ezPoY9uwZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode(target, s, use_teacher_forcing):\n",
        "  START = train_set.start_code\n",
        "  #STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  #stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  #stop_symbol.fill_(STOP)\n",
        "  \n",
        "  hidden = decoder.makeHidden()    \n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  for i in range(0, target.shape[1]):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "\n",
        "    recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "    if use_teacher_forcing:\n",
        "      old_symbol = target[:, i]\n",
        "    else:\n",
        "      old_symbol = recognition_result[:, i].detach()\n",
        "\n",
        "    loss += criterion(dec, target[:, i])\n",
        "      \n",
        "    return (recognition_result, loss)  \n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}