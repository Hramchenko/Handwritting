{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTR_tf_unif.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/HTR_tf_unif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "568dd93b-8278-419f-a695-f9beaf37e071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "ee346356-9bfc-4e05-bbf8-473c2cba0b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SiZq2SoAI3yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_dataset(dataset):\n",
        "  l = len(dataset.codes)\n",
        "  s = \"<START>\"\n",
        "  dataset.codes[s] = l\n",
        "  dataset.inv_codes[l] = s\n",
        "  return dataset\n",
        "\n",
        "train_set = modify_dataset(train_set)\n",
        "test_set = modify_dataset(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch()\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, 128, 400).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=23, encoded_height=64, batchnorm=True, dropout=False, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        lstm_layers = 1\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, ntoken)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.concatenated = torch.FloatTensor(24, )\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x)\n",
        "        emb = self.embedding(prev)\n",
        "        emb = emb.permute([1, 0, 2])\n",
        "        x = torch.cat([x, emb], dim=2)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return h1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coyZNSEbv6CS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9ea1902e-66ab-4c36-d2a6-e868f161994a"
      },
      "cell_type": "code",
      "source": [
        "decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "outputId": "570aa2e7-7f3c-46bf-beb3-f8b6134e6967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15158
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.codes['<START>']\n",
        "  current_symbol = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #s = enc.contiguous().view(1, batch_size, -1)\n",
        "   \n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "    \n",
        "    current_symbol[:, 0] = START\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "    for i in range(0, target.shape[1]):\n",
        "      symb = current_symbol[:, i].view(batch_size, 1).contiguous()\n",
        "      dec, hidden = decoder(s, symb, hidden)\n",
        "      if use_teacher_forcing:\n",
        "        current_symbol[:, i + 1] = target[:, i]\n",
        "      else:\n",
        "        sampled = torch.multinomial(dec.exp(), 1)\n",
        "        current_symbol[:, i+1] = sampled.squeeze()\n",
        "      o = dec.view(30, 1, 81).flatten(start_dim=0,end_dim=1)\n",
        "      t = target[:, i].flatten()\n",
        "      loss += criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 30\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if not use_teacher_forcing:\n",
        "        for k in range(0, 5):\n",
        "           print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(current_symbol[k,:]))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "      \n",
        "\n",
        "      \n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "TF: True\n",
            "  Batch: 30 Loss: 65.0776471455892\n",
            "TF: False\n",
            "  of                             -> <START>a1                           \n",
            "  .                              -> <START>M<START> h                         \n",
            "  .                              -> <START>&/             J      U      \n",
            "  had                            -> <START>eiOh    d       e            \n",
            "  come                           -> <START>ki i  n  k             4     \n",
            "  Batch: 60 Loss: 22.54894561767578\n",
            "TF: True\n",
            "  Batch: 90 Loss: 17.58822317123413\n",
            "TF: True\n",
            "  Batch: 120 Loss: 16.798765659332275\n",
            "TF: False\n",
            "  .                              -> <START>G                            \n",
            "  .                              -> <START> p                           \n",
            "  shown                          -> <START>hsry              v          \n",
            "  Miss                           -> <START>exZ nr                       \n",
            "  blind                          -> <START>t o f                        \n",
            "  Batch: 150 Loss: 16.172208881378175\n",
            "TF: False\n",
            "  change                         -> <START>mgdewefds   l n              \n",
            "  '                              -> <START>                             \n",
            "  ebriety                        -> <START>:8Ls t hte                   \n",
            "  to                             -> <START> e                           \n",
            "  direction                      -> <START>fiOrgsli                     \n",
            "  Batch: 180 Loss: 15.61527525583903\n",
            "TF: True\n",
            "  Batch: 210 Loss: 15.767782084147136\n",
            "TF: False\n",
            "  want                           -> <START>Veolrs                       \n",
            "  containing                     -> <START>Tu eirnlete         r        \n",
            "  !                              -> <START>wh                           \n",
            "  lordship                       -> <START>htqmssP r wn                 \n",
            "  '                              -> <START>bh                           \n",
            "  Batch: 240 Loss: 14.938670221964518\n",
            "TF: False\n",
            "  feelings                       -> <START>deure                        \n",
            "  the                            -> <START>uaa                          \n",
            "  the                            -> <START>bu.o                         \n",
            "  train                          -> <START>.f s                         \n",
            "  me                             -> <START>hts                          \n",
            "  Batch: 270 Loss: 15.145623938242595\n",
            "TF: False\n",
            "  EICHMANN                       -> <START>0edncodr                     \n",
            "  once                           -> <START>hfnelte                      \n",
            "  under                          -> <START>wl                           \n",
            "  common                         -> <START>tdeupses    n                \n",
            "  .                              -> <START>I                            \n",
            "  Batch: 300 Loss: 14.984616915384928\n",
            "TF: False\n",
            "  faces                          -> <START>:oa s                        \n",
            "  mortgage                       -> <START>sdtyt                        \n",
            "  .                              -> <START>oL                           \n",
            "  stomach-ulcers                 -> <START>mwyif o    e                 \n",
            "  electrically                   -> <START>wttuudc                      \n",
            "  Batch: 330 Loss: 15.203015041351318\n",
            "TF: False\n",
            "  1960                           -> <START>utde dy                      \n",
            "  consoled                       -> <START>Claeck-ta                    \n",
            "  a                              -> <START> s                           \n",
            "  Conway                         -> <START>ifyed h                      \n",
            "  secret                         -> <START>Jv                           \n",
            "  Batch: 360 Loss: 14.483510367075603\n",
            "TF: True\n",
            "  Batch: 390 Loss: 15.170058631896973\n",
            "TF: False\n",
            "  .                              -> <START>h                            \n",
            "  -                              -> <START>ba                           \n",
            "  room                           -> <START>Jcs l                        \n",
            "  the                            -> <START>fee                          \n",
            "  statement                      -> <START>naerqlpoed c  x s            \n",
            "  Batch: 420 Loss: 14.222477753957113\n",
            "TF: True\n",
            "  Batch: 450 Loss: 14.456545035044352\n",
            "TF: False\n",
            "  it                             -> <START>Ee                           \n",
            "  that                           -> <START>1lle                         \n",
            "  intellectual                   -> <START>bacg tstd t                  \n",
            "  to                             -> <START>ate                          \n",
            "  towards                        -> <START>to d                         \n",
            "  Batch: 480 Loss: 13.97187884648641\n",
            "TF: False\n",
            "  be                             -> <START>els                          \n",
            "  law                            -> <START>tcd                          \n",
            "  high                           -> <START>heasees                      \n",
            "  greater                        -> <START>senRyigc                     \n",
            "  for                            -> <START>fee                          \n",
            "  Batch: 510 Loss: 14.278638585408528\n",
            "TF: False\n",
            "  ,                              -> <START>,                            \n",
            "  which                          -> <START>ittts                        \n",
            "  to                             -> <START>o.                           \n",
            "  removed                        -> <START>fcauecb eg                   \n",
            "  According                      -> <START>cihotien                     \n",
            "  Batch: 540 Loss: 13.668235969543456\n",
            "TF: False\n",
            "  the                            -> <START>esw                          \n",
            "  all                            -> <START>mo                           \n",
            "  room                           -> <START>scareyi                      \n",
            "  a                              -> <START>.                            \n",
            "  \"                              -> <START>oh                           \n",
            "  Batch: 570 Loss: 14.055937639872234\n",
            "TF: False\n",
            "  their                          -> <START>olRet                        \n",
            "  gross                          -> <START>rao                          \n",
            "  them                           -> <START>ano                          \n",
            "  staff                          -> <START>eprs                         \n",
            "  you                            -> <START>onnr                         \n",
            "  Batch: 600 Loss: 13.86486161549886\n",
            "TF: False\n",
            "  its                            -> <START>cm                           \n",
            "  month                          -> <START>mos                          \n",
            "  .                              -> <START>.                            \n",
            "  always                         -> <START>/Lurdn                       \n",
            "  I                              -> <START>                             \n",
            "  Batch: 630 Loss: 13.79164161682129\n",
            "TF: False\n",
            "  new                            -> <START>aT                           \n",
            "  summer                         -> <START>hTuel s                      \n",
            "  said                           -> <START>hed                          \n",
            "  be                             -> <START>wh                           \n",
            "  Bryan                          -> <START>tvefnv                       \n",
            "  Batch: 660 Loss: 14.127370834350586\n",
            "TF: True\n",
            "  Batch: 690 Loss: 13.69145123163859\n",
            "TF: True\n",
            "  Batch: 720 Loss: 14.215289974212647\n",
            "TF: False\n",
            "  slightly                       -> <START>oigflf                       \n",
            "  conference                     -> <START>aelasiegi                    \n",
            "  a                              -> <START>to                           \n",
            "  guilds                         -> <START>li ah                        \n",
            "  !                              -> <START>n                            \n",
            "  Batch: 750 Loss: 13.82456267674764\n",
            "TF: False\n",
            "  died                           -> <START>3ned                         \n",
            "  Thus                           -> <START>ihns                         \n",
            "  ,                              -> <START>m                            \n",
            "  below                          -> <START>Smalntd                      \n",
            "  helping                        -> <START>dhole                        \n",
            "  Batch: 780 Loss: 13.86374766031901\n",
            "TF: False\n",
            "  minor                          -> <START>isodol                       \n",
            "  cosmic                         -> <START>aende                        \n",
            "  a                              -> <START>T                            \n",
            "  soon                           -> <START>ostrf                  y     \n",
            "  co-operation                   -> <START>cotiestrg                    \n",
            "  Batch: 810 Loss: 14.02924534479777\n",
            "TF: True\n",
            "  Batch: 840 Loss: 13.772989082336426\n",
            "TF: False\n",
            "  very                           -> <START>0eee                         \n",
            "  everyone                       -> <START>pgios O                      \n",
            "  ,                              -> <START>V                            \n",
            "  the                            -> <START>oot                          \n",
            "  war                            -> <START>wit                          \n",
            "  Batch: 870 Loss: 14.32415517171224\n",
            "TF: True\n",
            "  Batch: 900 Loss: 13.566293525695801\n",
            "TF: False\n",
            "  the                            -> <START>oiyhl                        \n",
            "  .                              -> <START>I                            \n",
            "  battle                         -> <START>mvcmvef d                    \n",
            "  The                            -> <START>oh                           \n",
            "  the                            -> <START>ot                           \n",
            "  Batch: 930 Loss: 13.272617435455322\n",
            "TF: False\n",
            "  \"                              -> <START>.                            \n",
            "  to                             -> <START>fce                          \n",
            "  over                           -> <START>tr                           \n",
            "  throw                          -> <START>ghrtae                       \n",
            "  what                           -> <START>thoh                         \n",
            "  Batch: 960 Loss: 13.820561122894286\n",
            "TF: True\n",
            "  Batch: 990 Loss: 13.538908227284749\n",
            "TF: True\n",
            "  Batch: 1020 Loss: 14.005431938171387\n",
            "TF: False\n",
            "  obligation                     -> <START>nalfclylsrles                \n",
            "  That                           -> <START>oro                          \n",
            "  West                           -> <START>cea                          \n",
            "  the-                           -> <START>maer                         \n",
            "  so                             -> <START>tE                           \n",
            "  Batch: 1050 Loss: 13.881357383728027\n",
            "TF: True\n",
            "  Batch: 1080 Loss: 13.592873096466064\n",
            "TF: False\n",
            "  has                            -> <START>otnl                         \n",
            "  beside                         -> <START>2hhie                        \n",
            "  of                             -> <START>,a                           \n",
            "  with                           -> <START>oaebo                        \n",
            "  taking                         -> <START>id<START>oa                        \n",
            "  Batch: 1110 Loss: 13.518631394704183\n",
            "TF: False\n",
            "  or                             -> <START>-a                           \n",
            "  which                          -> <START>wiaoeed                      \n",
            "  .                              -> <START>.                            \n",
            "  \"                              -> <START>a                            \n",
            "  dairy                          -> <START>tadsg                        \n",
            "  Batch: 1140 Loss: 13.360510317484538\n",
            "TF: True\n",
            "  Batch: 1170 Loss: 13.21378787358602\n",
            "TF: True\n",
            "  Batch: 1200 Loss: 13.83590087890625\n",
            "TF: False\n",
            "  Old                            -> <START>mxm                          \n",
            "  with                           -> <START>tiis                         \n",
            "  Welensky                       -> <START>sbcSrco                      \n",
            "  time                           -> <START>tus                          \n",
            "  drained                        -> <START>tfhae cm                     \n",
            "  Batch: 1230 Loss: 13.80006004969279\n",
            "TF: False\n",
            "  had                            -> <START>thit                         \n",
            "  a                              -> <START>o                            \n",
            "  very                           -> <START>Weig                         \n",
            "  Giuseppe                       -> <START>jcacon                       \n",
            "  he                             -> <START>Sk                           \n",
            "  Batch: 1260 Loss: 13.492681217193603\n",
            "TF: False\n",
            "  of                             -> <START>or                           \n",
            "  again                          -> <START>haoo                         \n",
            "  '                              -> <START>,5                           \n",
            "  of                             -> <START>ts                           \n",
            "  something                      -> <START>pnaagyliech    r             \n",
            "  Batch: 1290 Loss: 14.277172978719076\n",
            "TF: True\n",
            "  Batch: 1320 Loss: 13.282871373494466\n",
            "TF: False\n",
            "  had                            -> <START>nnll                         \n",
            "  it                             -> <START>wdl                          \n",
            "  .                              -> <START>.h                           \n",
            "  it                             -> <START>o                            \n",
            "  of                             -> <START>o                            \n",
            "  Batch: 1350 Loss: 12.925119177500408\n",
            "TF: False\n",
            "  ,                              -> <START>h                            \n",
            "  Lord                           -> <START>vs                           \n",
            "  .                              -> <START>.                            \n",
            "  a                              -> <START>t                            \n",
            "  the                            -> <START>hs                           \n",
            "  Batch: 1380 Loss: 13.933677514394125\n",
            "TF: True\n",
            "  Batch: 1410 Loss: 13.045218912760417\n",
            "TF: False\n",
            "  (                              -> <START>,                            \n",
            "  day-time                       -> <START>abagtlf                      \n",
            "  is                             -> <START>.                            \n",
            "  negotiations                   -> <START>ceniiic and                  \n",
            "  point                          -> <START>ointl                        \n",
            "  Batch: 1440 Loss: 13.10121529897054\n",
            "TF: False\n",
            "  diamonds                       -> <START>araIerd                      \n",
            "  say                            -> <START>qo                           \n",
            "  saw                            -> <START>tee                          \n",
            "  '                              -> <START>U                            \n",
            "  been                           -> <START>wcAl                         \n",
            "  Batch: 1470 Loss: 13.680528767903645\n",
            "TF: True\n",
            "  Batch: 1500 Loss: 12.965011024475098\n",
            "TF: False\n",
            "  .                              -> <START>.                            \n",
            "  cinema                         -> <START>1idcrtger                    \n",
            "  Anglesey                       -> <START>.ioenreger                   \n",
            "  A                              -> <START>Ir                           \n",
            "  in                             -> <START>n                            \n",
            "  Batch: 1530 Loss: 13.295944913228352\n",
            "TF: True\n",
            "  Batch: 1560 Loss: 12.739574750264486\n",
            "TF: True\n",
            "  Batch: 1590 Loss: 12.911151854197184\n",
            "TF: False\n",
            "  Joshua                         -> <START>bYnhmmde                     \n",
            "  to                             -> <START>t                            \n",
            "  going                          -> <START>Neas-                        \n",
            "  from                           -> <START>rgeotd                       \n",
            "  United                         -> <START>teeiradel                    \n",
            "  Batch: 1620 Loss: 12.838962745666503\n",
            "TF: False\n",
            "  that                           -> <START>gS6e                         \n",
            "  sparked                        -> <START>Ihtsctmua                    \n",
            "  shops                          -> <START>taariu                       \n",
            "  happens                        -> <START>chernlrt                     \n",
            "  and                            -> <START>ta                           \n",
            "  Batch: 1650 Loss: 12.944397862752279\n",
            "TF: True\n",
            "  Batch: 1680 Loss: 12.916289329528809\n",
            "TF: False\n",
            "  and                            -> <START>wnst                         \n",
            "  activities                     -> <START>acuarent                     \n",
            "  their                          -> <START>mnnenge                      \n",
            "  which                          -> <START> iitee                       \n",
            "  is                             -> <START>its                          \n",
            "  Batch: 1710 Loss: 13.206173833211263\n",
            "TF: True\n",
            "  Batch: 1740 Loss: 13.034429486592611\n",
            "TF: False\n",
            "  Hebrew                         -> <START>nidose                       \n",
            "  difference                     -> <START>eiascra                      \n",
            "  He                             -> <START>wt                           \n",
            "  actually                       -> <START>lomde s                      \n",
            "  too                            -> <START>aoli                         \n",
            "  Batch: 1770 Loss: 13.320873578389486\n",
            "TF: False\n",
            "  For                            -> <START>wuc                          \n",
            "  monoxide                       -> <START>Nnposdaonngidd               \n",
            "  on                             -> <START>hee                          \n",
            "  such                           -> <START>atos                         \n",
            "  the                            -> <START>Dore                         \n",
            "  Batch: 1800 Loss: 12.904269218444824\n",
            "TF: False\n",
            "  deals                          -> <START>sadai                        \n",
            "  then                           -> <START>abeenn                       \n",
            "  got                            -> <START>pne                          \n",
            "  had                            -> <START>qedg                         \n",
            "  with                           -> <START>Tcgv                         \n",
            "  Batch: 1830 Loss: 12.833583450317382\n",
            "TF: False\n",
            "  ,                              -> <START>Yf                           \n",
            "  former                         -> <START>tuwokce                      \n",
            "  far                            -> <START>Jfard                        \n",
            "  defence                        -> <START>atubed                       \n",
            "  of                             -> <START>I                            \n",
            "  Batch: 1860 Loss: 12.69722188313802\n",
            "TF: False\n",
            "  south                          -> <START>slet                         \n",
            "  against                        -> <START>Muevbrre                     \n",
            "  \"                              -> <START>\"                            \n",
            "  was                            -> <START>arl                          \n",
            "  agitation                      -> <START>vmaauol s                    \n",
            "  Batch: 1890 Loss: 13.183027299245198\n",
            "TF: True\n",
            "  Batch: 1920 Loss: 12.454338995615641\n",
            "TF: True\n",
            "  Batch: 1950 Loss: 14.076680183410645\n",
            "TF: True\n",
            "  Batch: 1980 Loss: 12.806114228566488\n",
            "TF: True\n",
            "  Batch: 2010 Loss: 12.951115989685059\n",
            "TF: False\n",
            "  reason                         -> <START>_askgs                       \n",
            "  Mr.                            -> <START>hen                          \n",
            "  he                             -> <START>thg                          \n",
            "  ,                              -> <START>,                            \n",
            "  set                            -> <START>tcr                          \n",
            "  Batch: 2040 Loss: 12.836269696553549\n",
            "TF: False\n",
            "  house                          -> <START>ames                         \n",
            "  boys                           -> <START>weo                          \n",
            "  its                            -> <START>tho                          \n",
            "  is                             -> <START>ta                           \n",
            "  might                          -> <START>iapho                        \n",
            "  Batch: 2070 Loss: 13.45826473236084\n",
            "TF: True\n",
            "  Batch: 2100 Loss: 12.905753294626871\n",
            "TF: True\n",
            "  Batch: 2130 Loss: 12.46191234588623\n",
            "TF: True\n",
            "  Batch: 2160 Loss: 12.87059055964152\n",
            "TF: False\n",
            "  pattern                        -> <START>Beilucni                     \n",
            "  only                           -> <START>oniy                         \n",
            "  the                            -> <START>oet                          \n",
            "  character                      -> <START>dmuliepotcn                  \n",
            "  into                           -> <START>ai                           \n",
            "  Batch: 2190 Loss: 12.696340243021647\n",
            "TF: False\n",
            "  she                            -> <START>Aso                          \n",
            "  and                            -> <START>Mhote                        \n",
            "  of                             -> <START>th                           \n",
            "  ,                              -> <START>,                            \n",
            "  things                         -> <START>berrusnseig                  \n",
            "  Batch: 2220 Loss: 13.270748138427734\n",
            "TF: False\n",
            "  which                          -> <START>Hhweis'                      \n",
            "  leading                        -> <START>thponrtut                    \n",
            "  Oi                             -> <START>he                           \n",
            "  came                           -> <START>twsne                        \n",
            "  water                          -> <START>pfig                         \n",
            "  Batch: 2250 Loss: 12.830772431691488\n",
            "TF: False\n",
            "  .                              -> <START>.                            \n",
            "  a                              -> <START>on                           \n",
            "  ,                              -> <START>,                            \n",
            "  fact                           -> <START>shl                          \n",
            "  a                              -> <START>.                            \n",
            "  Batch: 2280 Loss: 12.668179162343343\n",
            "TF: True\n",
            "  Batch: 2310 Loss: 12.364356009165446\n",
            "TF: True\n",
            "  Batch: 2340 Loss: 12.65818812052409\n",
            "TF: False\n",
            "  in                             -> <START>\"n4                          \n",
            "  the                            -> <START>and                          \n",
            "  facilities                     -> <START>gouigo                       \n",
            "  ,                              -> <START>.                            \n",
            "  apparently                     -> <START>sscteem                      \n",
            "  Batch: 2370 Loss: 12.672698879241944\n",
            "TF: True\n",
            "  Batch: 2400 Loss: 12.210582494735718\n",
            "TF: True\n",
            "  Batch: 2430 Loss: 12.82984218597412\n",
            "TF: True\n",
            "  Batch: 2460 Loss: 13.443888664245605\n",
            "TF: True\n",
            "  Batch: 2490 Loss: 12.90924654006958\n",
            "TF: True\n",
            "  Batch: 2520 Loss: 13.059308719635009\n",
            "TF: False\n",
            "  behind                         -> <START>octhnei3                     \n",
            "  start                          -> <START>fddu                         \n",
            "  in                             -> <START>wi                           \n",
            "  ?                              -> <START>,                            \n",
            "  in                             -> <START>ad                           \n",
            "  Batch: 2550 Loss: 12.152278836568197\n",
            "Training epoch 1...\n",
            "TF: True\n",
            "  Batch: 30 Loss: 13.06053196589152\n",
            "TF: True\n",
            "  Batch: 60 Loss: 12.861200682322185\n",
            "TF: True\n",
            "  Batch: 90 Loss: 12.035343551635743\n",
            "TF: False\n",
            "  and                            -> <START>lhee                         \n",
            "  like                           -> <START>prtr                         \n",
            "  If                             -> <START>y                            \n",
            "  to                             -> <START>of                           \n",
            "  only                           -> <START>lere                         \n",
            "  Batch: 120 Loss: 12.81392599741618\n",
            "TF: True\n",
            "  Batch: 150 Loss: 12.682864348093668\n",
            "TF: True\n",
            "  Batch: 180 Loss: 12.499497159322102\n",
            "TF: False\n",
            "  engaged                        -> <START>rotuegd                      \n",
            "  that                           -> <START>che                          \n",
            "  was                            -> <START>sed                          \n",
            "  that                           -> <START>Ost                          \n",
            "  President                      -> <START>w1mseb n  s                  \n",
            "  Batch: 210 Loss: 12.736600176493328\n",
            "TF: False\n",
            "  want                           -> <START>clyer                        \n",
            "  containing                     -> <START>Pinidirrlsol                 \n",
            "  !                              -> <START>ie                           \n",
            "  lordship                       -> <START>vaitsrtrl                    \n",
            "  '                              -> <START>\"\"                           \n",
            "  Batch: 240 Loss: 12.135600344340007\n",
            "TF: True\n",
            "  Batch: 270 Loss: 12.459384059906006\n",
            "TF: True\n",
            "  Batch: 300 Loss: 12.494767506917318\n",
            "TF: True\n",
            "  Batch: 330 Loss: 12.821339670817057\n",
            "TF: True\n",
            "  Batch: 360 Loss: 12.067712116241456\n",
            "TF: True\n",
            "  Batch: 390 Loss: 13.043023935953777\n",
            "TF: True\n",
            "  Batch: 420 Loss: 12.149428590138752\n",
            "TF: False\n",
            "  result                         -> <START>fisied                       \n",
            "  The                            -> <START>fog                          \n",
            "  1960                           -> <START>on                           \n",
            "  in                             -> <START>'s                           \n",
            "  had                            -> <START>WhFn                         \n",
            "  Batch: 450 Loss: 12.41454496383667\n",
            "TF: False\n",
            "  it                             -> <START>in                           \n",
            "  that                           -> <START> frer                        \n",
            "  intellectual                   -> <START>aPhomlisi. i                 \n",
            "  to                             -> <START>ht                           \n",
            "  towards                        -> <START>telonnen                     \n",
            "  Batch: 480 Loss: 11.924868075052897\n",
            "TF: False\n",
            "  be                             -> <START>ss                           \n",
            "  law                            -> <START>ius                          \n",
            "  high                           -> <START>wersls                       \n",
            "  greater                        -> <START>Hcreene                      \n",
            "  for                            -> <START>The                          \n",
            "  Batch: 510 Loss: 12.510242621103922\n",
            "TF: True\n",
            "  Batch: 540 Loss: 11.855200163523357\n",
            "TF: False\n",
            "  the                            -> <START>tte                          \n",
            "  all                            -> <START>Al                           \n",
            "  room                           -> <START>nhie                         \n",
            "  a                              -> <START>en                           \n",
            "  \"                              -> <START>,                            \n",
            "  Batch: 570 Loss: 12.255085786183676\n",
            "TF: False\n",
            "  their                          -> <START>wiale                        \n",
            "  gross                          -> <START>gor                          \n",
            "  them                           -> <START>bme s                        \n",
            "  staff                          -> <START>radn                         \n",
            "  you                            -> <START>bi'                          \n",
            "  Batch: 600 Loss: 12.198812452952067\n",
            "TF: False\n",
            "  its                            -> <START>wue                          \n",
            "  month                          -> <START>waee                         \n",
            "  .                              -> <START>.                            \n",
            "  always                         -> <START>lnesde                       \n",
            "  I                              -> <START>,                            \n",
            "  Batch: 630 Loss: 12.292498970031739\n",
            "TF: False\n",
            "  new                            -> <START>oec                          \n",
            "  summer                         -> <START>lrsert-oe                    \n",
            "  said                           -> <START>gldl                         \n",
            "  be                             -> <START>tt                           \n",
            "  Bryan                          -> <START>tedoe                        \n",
            "  Batch: 660 Loss: 12.510583432515462\n",
            "TF: False\n",
            "  said                           -> <START>thckn                        \n",
            "  As                             -> <START>he                           \n",
            "  American                       -> <START>wAnbra ane s                 \n",
            "  blow                           -> <START>wuhe                         \n",
            "  in                             -> <START>a                            \n",
            "  Batch: 690 Loss: 12.213681411743163\n",
            "TF: True\n",
            "  Batch: 720 Loss: 12.547670364379883\n",
            "TF: False\n",
            "  slightly                       -> <START>bOwea                        \n",
            "  conference                     -> <START>epuint                       \n",
            "  a                              -> <START>I                            \n",
            "  guilds                         -> <START>mhh                          \n",
            "  !                              -> <START>t                            \n",
            "  Batch: 750 Loss: 12.175244839986165\n",
            "TF: True\n",
            "  Batch: 780 Loss: 12.136468664805095\n",
            "TF: False\n",
            "  minor                          -> <START>totace                       \n",
            "  cosmic                         -> <START>naitre                       \n",
            "  a                              -> <START>.n                           \n",
            "  soon                           -> <START>cnsD                         \n",
            "  co-operation                   -> <START>aonenrem                     \n",
            "  Batch: 810 Loss: 12.781308428446453\n",
            "TF: False\n",
            "  is                             -> <START>ist                          \n",
            "  the                            -> <START>Aan                          \n",
            "  a                              -> <START>.e                           \n",
            "  assured                        -> <START>renrve                       \n",
            "  ,                              -> <START>.                            \n",
            "  Batch: 840 Loss: 12.255286248524984\n",
            "TF: True\n",
            "  Batch: 870 Loss: 12.599811331431072\n",
            "TF: True\n",
            "  Batch: 900 Loss: 12.041024843851725\n",
            "TF: False\n",
            "  the                            -> <START>wnr                          \n",
            "  .                              -> <START>.                            \n",
            "  battle                         -> <START>Gathdt                       \n",
            "  The                            -> <START>tof                          \n",
            "  the                            -> <START>The                          \n",
            "  Batch: 930 Loss: 11.744389406840007\n",
            "TF: False\n",
            "  \"                              -> <START>,                            \n",
            "  to                             -> <START>o                            \n",
            "  over                           -> <START>oeoc                         \n",
            "  throw                          -> <START>qwowtt                       \n",
            "  what                           -> <START>hanr                         \n",
            "  Batch: 960 Loss: 12.606976000467936\n",
            "TF: True\n",
            "  Batch: 990 Loss: 12.334112103780111\n",
            "TF: False\n",
            "  Secretary                      -> <START>sihrih                       \n",
            "  hour                           -> <START>ho                           \n",
            "  there                          -> <START>hrerc                        \n",
            "  and                            -> <START>ha                           \n",
            "  \"                              -> <START>.3                           \n",
            "  Batch: 1020 Loss: 12.548505465189615\n",
            "TF: True\n",
            "  Batch: 1050 Loss: 12.449113353093464\n",
            "TF: False\n",
            "  expedients                     -> <START>aesiedig l                   \n",
            "  'd                             -> <START>y                            \n",
            "  \"                              -> <START>\"                            \n",
            "  not                            -> <START>lis                          \n",
            "  He                             -> <START>to                           \n",
            "  Batch: 1080 Loss: 12.315806420644124\n",
            "TF: True\n",
            "  Batch: 1110 Loss: 12.01081059773763\n",
            "TF: True\n",
            "  Batch: 1140 Loss: 12.046168581644695\n",
            "TF: False\n",
            "  formidable                     -> <START>deahisadooas                 \n",
            "  this                           -> <START>tan                          \n",
            "  can                            -> <START>enf                          \n",
            "  ,                              -> <START>,                            \n",
            "  eye                            -> <START>tog                          \n",
            "  Batch: 1170 Loss: 12.095474608739217\n",
            "TF: True\n",
            "  Batch: 1200 Loss: 12.901756095886231\n",
            "TF: False\n",
            "  Old                            -> <START>aar                          \n",
            "  with                           -> <START>bks                          \n",
            "  Welensky                       -> <START>chipul  g                    \n",
            "  time                           -> <START>hum                          \n",
            "  drained                        -> <START>raomwrle                     \n",
            "  Batch: 1230 Loss: 12.475178750356038\n",
            "TF: True\n",
            "  Batch: 1260 Loss: 11.916666221618652\n",
            "TF: False\n",
            "  of                             -> <START>of                           \n",
            "  again                          -> <START>eanbep                       \n",
            "  '                              -> <START>\"                            \n",
            "  of                             -> <START>of                           \n",
            "  something                      -> <START>paeNeonerse,erio             \n",
            "  Batch: 1290 Loss: 13.047491677602132\n",
            "TF: True\n",
            "  Batch: 1320 Loss: 11.798826694488525\n",
            "TF: False\n",
            "  had                            -> <START>tei                          \n",
            "  it                             -> <START>iol                          \n",
            "  .                              -> <START>.                            \n",
            "  it                             -> <START>of                           \n",
            "  of                             -> <START>ha                           \n",
            "  Batch: 1350 Loss: 11.613503551483154\n",
            "TF: False\n",
            "  ,                              -> <START>.y                           \n",
            "  Lord                           -> <START>foas                         \n",
            "  .                              -> <START>,                            \n",
            "  a                              -> <START>an                           \n",
            "  the                            -> <START>'e                           \n",
            "  Batch: 1380 Loss: 12.352824179331462\n",
            "TF: True\n",
            "  Batch: 1410 Loss: 12.079613939921062\n",
            "TF: False\n",
            "  (                              -> <START>,                            \n",
            "  day-time                       -> <START>edrncincsy   l               \n",
            "  is                             -> <START>,                            \n",
            "  negotiations                   -> <START>Anmpnrdeo  n                 \n",
            "  point                          -> <START>diyee                        \n",
            "  Batch: 1440 Loss: 12.192065111796062\n",
            "TF: True\n",
            "  Batch: 1470 Loss: 12.673437372843425\n",
            "TF: False\n",
            "  thread                         -> <START>wheie                        \n",
            "  \"                              -> <START>I                            \n",
            "  House                          -> <START>thros                        \n",
            "  school                         -> <START>doan                         \n",
            "  ,                              -> <START>I                            \n",
            "  Batch: 1500 Loss: 11.953729740778606\n",
            "TF: False\n",
            "  .                              -> <START>x.                           \n",
            "  cinema                         -> <START>fvaadsel                     \n",
            "  Anglesey                       -> <START>semonAoey                    \n",
            "  A                              -> <START>to                           \n",
            "  in                             -> <START>to                           \n",
            "  Batch: 1530 Loss: 12.417785612742106\n",
            "TF: False\n",
            "  of                             -> <START>ss                           \n",
            "  this                           -> <START>titt                         \n",
            "  Monckton                       -> <START>Herterutn                    \n",
            "  advised                        -> <START>dnoaas                       \n",
            "  There                          -> <START>wefsr                        \n",
            "  Batch: 1560 Loss: 11.753483994801838\n",
            "TF: True\n",
            "  Batch: 1590 Loss: 11.730654652913412\n",
            "TF: True\n",
            "  Batch: 1620 Loss: 11.354088481267294\n",
            "TF: False\n",
            "  that                           -> <START>nioh                         \n",
            "  sparked                        -> <START>counkd ct                    \n",
            "  shops                          -> <START>piyin                        \n",
            "  happens                        -> <START>Keelm3sh                     \n",
            "  and                            -> <START>Hhh                          \n",
            "  Batch: 1650 Loss: 11.643015797932943\n",
            "TF: False\n",
            "  presence                       -> <START>ouyfeese                     \n",
            "  an                             -> <START>if                           \n",
            "  this                           -> <START>th                           \n",
            "  Play                           -> <START>pnol                         \n",
            "  wasted                         -> <START>opeseds                      \n",
            "  Batch: 1680 Loss: 11.93432477315267\n",
            "TF: False\n",
            "  and                            -> <START>her                          \n",
            "  activities                     -> <START>onliftid                     \n",
            "  their                          -> <START>hwcsTe                       \n",
            "  which                          -> <START>tascldins                    \n",
            "  is                             -> <START>ab                           \n",
            "  Batch: 1710 Loss: 12.080802885691325\n",
            "TF: True\n",
            "  Batch: 1740 Loss: 12.12964235941569\n",
            "TF: True\n",
            "  Batch: 1770 Loss: 12.174293772379558\n",
            "TF: True\n",
            "  Batch: 1800 Loss: 11.721219380696615\n",
            "TF: False\n",
            "  deals                          -> <START>ualt                         \n",
            "  then                           -> <START>bipent                       \n",
            "  got                            -> <START>our                          \n",
            "  had                            -> <START>tme e                        \n",
            "  with                           -> <START>lud                          \n",
            "  Batch: 1830 Loss: 11.722308158874512\n",
            "TF: False\n",
            "  ,                              -> <START>,                            \n",
            "  former                         -> <START>corh-m                       \n",
            "  far                            -> <START>thst                         \n",
            "  defence                        -> <START>ooatead                      \n",
            "  of                             -> <START>an                           \n",
            "  Batch: 1860 Loss: 11.730123583475748\n",
            "TF: True\n",
            "  Batch: 1890 Loss: 11.983687575658163\n",
            "TF: False\n",
            "  uniform                        -> <START>tohtter                      \n",
            "  ,                              -> <START>,                            \n",
            "  but                            -> <START>wiss                         \n",
            "  ,                              -> <START>,                            \n",
            "  grandmother                    -> <START>vfnrore5aoPd                 \n",
            "  Batch: 1920 Loss: 11.652460066477458\n",
            "TF: False\n",
            "  lead                           -> <START>mhdt                         \n",
            "  Kinnaird                       -> <START>cefeuotos                    \n",
            "  .                              -> <START>..                           \n",
            "  ;                              -> <START>.                            \n",
            "  Arthur                         -> <START>Riree                        \n",
            "  Batch: 1950 Loss: 12.720792897542317\n",
            "TF: True\n",
            "  Batch: 1980 Loss: 11.737564023335775\n",
            "TF: False\n",
            "  32                             -> <START>sh                           \n",
            "  with                           -> <START>chtch                        \n",
            "  all                            -> <START>et0                          \n",
            "  and                            -> <START>aou                          \n",
            "  of                             -> <START>Df                           \n",
            "  Batch: 2010 Loss: 11.789006916681926\n",
            "TF: False\n",
            "  reason                         -> <START>meanyae                      \n",
            "  Mr.                            -> <START>as                           \n",
            "  he                             -> <START>toe                          \n",
            "  ,                              -> <START>.                            \n",
            "  set                            -> <START>ands                         \n",
            "  Batch: 2040 Loss: 11.718393802642822\n",
            "TF: True\n",
            "  Batch: 2070 Loss: 12.594153912862142\n",
            "TF: False\n",
            "  that                           -> <START>ixkt                         \n",
            "  has                            -> <START>hirt                         \n",
            "  shape                          -> <START>halgly                       \n",
            "  to                             -> <START>of                           \n",
            "  .                              -> <START>,                            \n",
            "  Batch: 2100 Loss: 11.972607453664144\n",
            "TF: False\n",
            "  chase                          -> <START>lorst                        \n",
            "  as                             -> <START>o                            \n",
            "  shaded                         -> <START>Sheecndey                    \n",
            "  a                              -> <START>i                            \n",
            "  ,                              -> <START>,                            \n",
            "  Batch: 2130 Loss: 11.329213857650757\n",
            "TF: False\n",
            "  I                              -> <START>,                            \n",
            "  sardonic                       -> <START>p\"wndtin                     \n",
            "  the                            -> <START>then                         \n",
            "  and                            -> <START>meer                         \n",
            "  you                            -> <START>bae                          \n",
            "  Batch: 2160 Loss: 12.06836493810018\n",
            "TF: True\n",
            "  Batch: 2190 Loss: 11.473066171010336\n",
            "TF: True\n",
            "  Batch: 2220 Loss: 12.447096920013427\n",
            "TF: True\n",
            "  Batch: 2250 Loss: 12.004007498423258\n",
            "TF: True\n",
            "  Batch: 2280 Loss: 12.077721405029298\n",
            "TF: False\n",
            "  bit                            -> <START>lnt                          \n",
            "  .                              -> <START>.                            \n",
            "  \"                              -> <START>\"                            \n",
            "  Ponsonby                       -> <START>peroitoe                     \n",
            "  that                           -> <START>wote                         \n",
            "  Batch: 2310 Loss: 11.403793430328369\n",
            "TF: True\n",
            "  Batch: 2340 Loss: 11.785258181889851\n",
            "TF: True\n",
            "  Batch: 2370 Loss: 11.728141371409098\n",
            "TF: False\n",
            "  In                             -> <START>th                           \n",
            "  theoretical                    -> <START>thaeeni                      \n",
            "  America                        -> <START>wibn '                       \n",
            "  to                             -> <START>tf                           \n",
            "  members                        -> <START>barabd                       \n",
            "  Batch: 2400 Loss: 11.039591360092164\n",
            "TF: True\n",
            "  Batch: 2430 Loss: 11.653602345784504\n",
            "TF: False\n",
            "  ,                              -> <START>,                            \n",
            "  it                             -> <START>bf                           \n",
            "  could                          -> <START>wains                        \n",
            "  mad                            -> <START>1erds                        \n",
            "  these                          -> <START>beeE                         \n",
            "  Batch: 2460 Loss: 12.438429800669352\n",
            "TF: True\n",
            "  Batch: 2490 Loss: 12.064681243896484\n",
            "TF: True\n",
            "  Batch: 2520 Loss: 12.033229573567708\n",
            "TF: True\n",
            "  Batch: 2550 Loss: 11.10094477335612\n",
            "Training epoch 2...\n",
            "TF: True\n",
            "  Batch: 30 Loss: 12.102587032318116\n",
            "TF: False\n",
            "  of                             -> <START>of                           \n",
            "  .                              -> <START>.                            \n",
            "  .                              -> <START>.                            \n",
            "  had                            -> <START>Aoy                          \n",
            "  come                           -> <START>dawg                         \n",
            "  Batch: 60 Loss: 11.787747971216838\n",
            "TF: True\n",
            "  Batch: 90 Loss: 11.071440267562867\n",
            "TF: False\n",
            "  and                            -> <START>avern                        \n",
            "  like                           -> <START>tew                          \n",
            "  If                             -> <START>he                           \n",
            "  to                             -> <START>m                            \n",
            "  only                           -> <START>ave                          \n",
            "  Batch: 120 Loss: 11.834279187520345\n",
            "TF: True\n",
            "  Batch: 150 Loss: 11.673929929733276\n",
            "TF: False\n",
            "  change                         -> <START>Mednrtem                     \n",
            "  '                              -> <START>a                            \n",
            "  ebriety                        -> <START>bomealgr                     \n",
            "  to                             -> <START>ot                           \n",
            "  direction                      -> <START>toLsinvs                     \n",
            "  Batch: 180 Loss: 11.388228925069173\n",
            "TF: False\n",
            "  engaged                        -> <START>eameth                       \n",
            "  that                           -> <START>oh                           \n",
            "  was                            -> <START>in         e                 \n",
            "  that                           -> <START>thh                          \n",
            "  President                      -> <START>iceannoe                     \n",
            "  Batch: 210 Loss: 11.808494170506796\n",
            "TF: False\n",
            "  want                           -> <START>wevk                         \n",
            "  containing                     -> <START>Bepkotawfng                  \n",
            "  !                              -> <START>6                            \n",
            "  lordship                       -> <START>asanlber    t                \n",
            "  '                              -> <START>\"                            \n",
            "  Batch: 240 Loss: 11.266556485493977\n",
            "TF: True\n",
            "  Batch: 270 Loss: 11.290351835886637\n",
            "TF: True\n",
            "  Batch: 300 Loss: 11.343422349294027\n",
            "TF: True\n",
            "  Batch: 330 Loss: 11.803104480107626\n",
            "TF: True\n",
            "  Batch: 360 Loss: 11.404637845357259\n",
            "TF: False\n",
            "  not                            -> <START>salo                         \n",
            "  important                      -> <START>oracmsfinve                  \n",
            "  the                            -> <START>oto                          \n",
            "  ,                              -> <START>,                            \n",
            "  it                             -> <START>is                           \n",
            "  Batch: 390 Loss: 11.920177110036214\n",
            "TF: False\n",
            "  .                              -> <START>.                            \n",
            "  -                              -> <START>in                           \n",
            "  room                           -> <START>rael                         \n",
            "  the                            -> <START>lhate                        \n",
            "  statement                      -> <START>reaengty                     \n",
            "  Batch: 420 Loss: 11.277168210347494\n",
            "TF: True\n",
            "  Batch: 450 Loss: 11.560199610392253\n",
            "TF: True\n",
            "  Batch: 480 Loss: 11.226922098795573\n",
            "TF: True\n",
            "  Batch: 510 Loss: 11.55120620727539\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-b7bf468b594c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-90-b7bf468b594c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mcurrent_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Handwritting/IAMWords.py\u001b[0m in \u001b[0;36mmake_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Yg8-hpPq2e2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}