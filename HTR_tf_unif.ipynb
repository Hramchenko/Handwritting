{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTR_tf_unif.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/HTR_tf_unif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "cf837e47-a4d4-4d31-cb3a-658d7032c5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "553da4b5-dd11-48a5-bfd9-5adf4f207e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SiZq2SoAI3yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_dataset(dataset):\n",
        "  l = len(dataset.codes)\n",
        "  s = \"<START>\"\n",
        "  dataset.codes[s] = l\n",
        "  dataset.inv_codes[l] = s\n",
        "  return dataset\n",
        "\n",
        "train_set = modify_dataset(train_set)\n",
        "test_set = modify_dataset(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch()\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, 128, 400).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=23, encoded_height=64, batchnorm=True, dropout=False, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        lstm_layers = 1\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, ntoken)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.concatenated = torch.FloatTensor(24, )\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x)\n",
        "        emb = self.embedding(prev)\n",
        "        emb = emb.permute([1, 0, 2])\n",
        "        x = torch.cat([x, emb], dim=2)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return h1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coyZNSEbv6CS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7896465b-5a3b-4058-88f1-6e0b61b55ae5"
      },
      "cell_type": "code",
      "source": [
        "decoder = HTRDecoder(len(train_set.codes)).to(device)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "outputId": "bf18b47c-a787-4f11-c826-e4d2b8313b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10197
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.codes['<START>']\n",
        "  current_symbol = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #s = enc.contiguous().view(1, batch_size, -1)\n",
        "   \n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "    \n",
        "    current_symbol[:, 0] = START\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "    for i in range(0, target.shape[1]):\n",
        "      symb = current_symbol[:, i].view(batch_size, 1).contiguous()\n",
        "      dec, hidden = decoder(s, symb, hidden)\n",
        "      if use_teacher_forcing:\n",
        "        current_symbol[:, i + 1] = target[:, i]\n",
        "      else:\n",
        "        sampled = torch.multinomial(dec.exp(), 1)\n",
        "        current_symbol[:, i+1] = sampled.squeeze()\n",
        "      o = dec.view(30, 1, 81).flatten(start_dim=0,end_dim=1)\n",
        "      t = target[:, i].flatten()\n",
        "      loss += criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 30\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if not use_teacher_forcing:\n",
        "        for k in range(0, 5):\n",
        "           print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(current_symbol[k,:]))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "      \n",
        "\n",
        "      \n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "TF: True\n",
            "  Batch: 30 Loss: 87.11978683471679\n",
            "TF: False\n",
            "  of                             -> <START>k                            \n",
            "  .                              -> <START>rhJ                          \n",
            "  .                              -> <START>qr                        Dn \n",
            "  had                            -> <START>Z  s+         Z     h        \n",
            "  come                           -> <START>xp   ? osq                   \n",
            "  Batch: 60 Loss: 27.65483678181966\n",
            "TF: True\n",
            "  Batch: 90 Loss: 19.13895454406738\n",
            "TF: True\n",
            "  Batch: 120 Loss: 17.177790355682372\n",
            "TF: True\n",
            "  Batch: 150 Loss: 16.284815947214764\n",
            "TF: False\n",
            "  change                         -> <START>+Sdlornans                   \n",
            "  '                              -> <START>lz                           \n",
            "  ebriety                        -> <START>'/6omn                       \n",
            "  to                             -> <START>h                            \n",
            "  direction                      -> <START>pgno                         \n",
            "  Batch: 180 Loss: 15.65736354192098\n",
            "TF: False\n",
            "  engaged                        -> <START>di ceh o      l              \n",
            "  that                           -> <START>-n                           \n",
            "  was                            -> <START>,p    h                      \n",
            "  that                           -> <START>ne                           \n",
            "  President                      -> <START>oZds                         \n",
            "  Batch: 210 Loss: 15.7090744972229\n",
            "TF: True\n",
            "  Batch: 240 Loss: 14.802171166737875\n",
            "TF: True\n",
            "  Batch: 270 Loss: 14.954624557495118\n",
            "TF: False\n",
            "  EICHMANN                       -> <START>nfdtmr/u                     \n",
            "  once                           -> <START>oynmed                       \n",
            "  under                          -> <START>anpds                        \n",
            "  common                         -> <START>uKo9e#h esn          e       \n",
            "  .                              -> <START>C                            \n",
            "  Batch: 300 Loss: 14.827142111460368\n",
            "TF: True\n",
            "  Batch: 330 Loss: 15.248360602060954\n",
            "TF: False\n",
            "  1960                           -> <START>toheslv                      \n",
            "  consoled                       -> <START>hhisahnet  o                 \n",
            "  a                              -> <START>ww                           \n",
            "  Conway                         -> <START>dctca    n                   \n",
            "  secret                         -> <START>acfdor                       \n",
            "  Batch: 360 Loss: 14.495178445180256\n",
            "TF: True\n",
            "  Batch: 390 Loss: 15.113532225290934\n",
            "TF: True\n",
            "  Batch: 420 Loss: 14.182599194844563\n",
            "TF: False\n",
            "  result                         -> <START>oroagod e                    \n",
            "  The                            -> <START>1cmc                         \n",
            "  1960                           -> <START>vin                          \n",
            "  in                             -> <START>vo                           \n",
            "  had                            -> <START>2hhns                        \n",
            "  Batch: 450 Loss: 14.480822722117106\n",
            "TF: False\n",
            "  it                             -> <START>                             \n",
            "  that                           -> <START>nrae                         \n",
            "  intellectual                   -> <START>-0escson    g                \n",
            "  to                             -> <START>en                           \n",
            "  towards                        -> <START>yi yu                        \n",
            "  Batch: 480 Loss: 14.004029369354248\n",
            "TF: True\n",
            "  Batch: 510 Loss: 14.146177387237548\n",
            "TF: True\n",
            "  Batch: 540 Loss: 13.627241357167561\n",
            "TF: False\n",
            "  the                            -> <START>uer                          \n",
            "  all                            -> <START>arr                          \n",
            "  room                           -> <START>bhprenu s                    \n",
            "  a                              -> <START>wh                           \n",
            "  \"                              -> <START>bW                           \n",
            "  Batch: 570 Loss: 14.034080314636231\n",
            "TF: True\n",
            "  Batch: 600 Loss: 13.825453027089436\n",
            "TF: True\n",
            "  Batch: 630 Loss: 13.761375904083252\n",
            "TF: False\n",
            "  new                            -> <START>3isr                         \n",
            "  summer                         -> <START>swg ed                       \n",
            "  said                           -> <START>lfida                        \n",
            "  be                             -> <START>n                            \n",
            "  Bryan                          -> <START>alve                         \n",
            "  Batch: 660 Loss: 14.152178223927816\n",
            "TF: False\n",
            "  said                           -> <START>he                           \n",
            "  As                             -> <START>hn                           \n",
            "  American                       -> <START>leirc au                     \n",
            "  blow                           -> <START>tmr r                        \n",
            "  in                             -> <START>o-                           \n",
            "  Batch: 690 Loss: 13.665421104431152\n",
            "TF: True\n",
            "  Batch: 720 Loss: 14.06586701075236\n",
            "TF: True\n",
            "  Batch: 750 Loss: 13.800369803110758\n",
            "TF: True\n",
            "  Batch: 780 Loss: 13.924535465240478\n",
            "TF: False\n",
            "  minor                          -> <START>Iouafu                       \n",
            "  cosmic                         -> <START>ooule                        \n",
            "  a                              -> <START>a                            \n",
            "  soon                           -> <START>hee                          \n",
            "  co-operation                   -> <START>veirrcorsd e                 \n",
            "  Batch: 810 Loss: 14.17243995666504\n",
            "TF: False\n",
            "  is                             -> <START>tsn                          \n",
            "  the                            -> <START>ysw                          \n",
            "  a                              -> <START>o                            \n",
            "  assured                        -> <START>ten seet                     \n",
            "  ,                              -> <START>w.                           \n",
            "  Batch: 840 Loss: 13.799581464131673\n",
            "TF: True\n",
            "  Batch: 870 Loss: 14.211153475443522\n",
            "TF: False\n",
            "  more                           -> <START>ailt                         \n",
            "  completed                      -> <START>ihecchs                      \n",
            "  a                              -> <START>,                            \n",
            "  and                            -> <START>fa                           \n",
            "  stay                           -> <START>epdd                         \n",
            "  Batch: 900 Loss: 13.589851411183675\n",
            "TF: False\n",
            "  the                            -> <START>hn                           \n",
            "  .                              -> <START>B                            \n",
            "  battle                         -> <START>Zobwt                        \n",
            "  The                            -> <START>ie                           \n",
            "  the                            -> <START>tme                          \n",
            "  Batch: 930 Loss: 13.389292017618816\n",
            "TF: False\n",
            "  \"                              -> <START>fd                           \n",
            "  to                             -> <START>T                            \n",
            "  over                           -> <START>inn                          \n",
            "  throw                          -> <START>hanys                        \n",
            "  what                           -> <START>hei                          \n",
            "  Batch: 960 Loss: 13.763927268981934\n",
            "TF: False\n",
            "  me                             -> <START>hot                          \n",
            "  there                          -> <START>oos                          \n",
            "  \"                              -> <START>'e                           \n",
            "  whom                           -> <START>ajerlp                       \n",
            "  shabby                         -> <START>baars  a                     \n",
            "  Batch: 990 Loss: 13.497869682312011\n",
            "TF: False\n",
            "  Secretary                      -> <START>neovrtor                     \n",
            "  hour                           -> <START>tc                           \n",
            "  there                          -> <START>aeHct                        \n",
            "  and                            -> <START>ams                          \n",
            "  \"                              -> <START>,n8                          \n",
            "  Batch: 1020 Loss: 14.219471804300944\n",
            "TF: False\n",
            "  obligation                     -> <START>oseudtun n                   \n",
            "  That                           -> <START>nreie                        \n",
            "  West                           -> <START>ron                          \n",
            "  the-                           -> <START>oho                          \n",
            "  so                             -> <START>c                            \n",
            "  Batch: 1050 Loss: 13.773617172241211\n",
            "TF: False\n",
            "  expedients                     -> <START>chruahnoso+n                 \n",
            "  'd                             -> <START>t                            \n",
            "  \"                              -> <START>,s                           \n",
            "  not                            -> <START>ynee                         \n",
            "  He                             -> <START>lt                           \n",
            "  Batch: 1080 Loss: 13.603595288594564\n",
            "TF: True\n",
            "  Batch: 1110 Loss: 13.594639650980632\n",
            "TF: True\n",
            "  Batch: 1140 Loss: 13.50649627049764\n",
            "TF: True\n",
            "  Batch: 1170 Loss: 13.303912480672201\n",
            "TF: True\n",
            "  Batch: 1200 Loss: 13.912101618448894\n",
            "TF: False\n",
            "  Old                            -> <START>the                          \n",
            "  with                           -> <START>oboye                        \n",
            "  Welensky                       -> <START>coiepnmeitogr                \n",
            "  time                           -> <START>iotes                        \n",
            "  drained                        -> <START>irmsuli                      \n",
            "  Batch: 1230 Loss: 13.782583872477213\n",
            "TF: True\n",
            "  Batch: 1260 Loss: 13.47535343170166\n",
            "TF: False\n",
            "  of                             -> <START>\"to                          \n",
            "  again                          -> <START>hie                          \n",
            "  '                              -> <START>xH                           \n",
            "  of                             -> <START>n                            \n",
            "  something                      -> <START>_pfcmfaie4nB.hmt lunn  et   s\n",
            "  Batch: 1290 Loss: 14.382807286580404\n",
            "TF: False\n",
            "  Ponsonby                       -> <START>ioue                         \n",
            "  families                       -> <START>srople                       \n",
            "  Labour                         -> <START>lai s                        \n",
            "  which                          -> <START>eoldrs                       \n",
            "  very                           -> <START>bth                          \n",
            "  Batch: 1320 Loss: 13.378572432200114\n",
            "TF: True\n",
            "  Batch: 1350 Loss: 13.122031879425048\n",
            "TF: True\n",
            "  Batch: 1380 Loss: 13.837269337972005\n",
            "TF: False\n",
            "  ,                              -> <START>.                            \n",
            "  ?                              -> <START>bq                           \n",
            "  great                          -> <START>wayr                         \n",
            "  nor                            -> <START>ny                           \n",
            "  rain                           -> <START>fa  y                        \n",
            "  Batch: 1410 Loss: 13.182048575083416\n",
            "TF: False\n",
            "  (                              -> <START>w                            \n",
            "  day-time                       -> <START>detiur                       \n",
            "  is                             -> <START>t                            \n",
            "  negotiations                   -> <START>5ooattmda ns                 \n",
            "  point                          -> <START>5ttme                        \n",
            "  Batch: 1440 Loss: 13.269682661692302\n",
            "TF: True\n",
            "  Batch: 1470 Loss: 13.686541811625164\n",
            "TF: True\n",
            "  Batch: 1500 Loss: 13.097267882029216\n",
            "TF: False\n",
            "  .                              -> <START>l,                           \n",
            "  cinema                         -> <START>2tmonta                      \n",
            "  Anglesey                       -> <START>sesheneta  n                 \n",
            "  A                              -> <START>\"st                          \n",
            "  in                             -> <START>p                            \n",
            "  Batch: 1530 Loss: 13.636205609639486\n",
            "TF: True\n",
            "  Batch: 1560 Loss: 12.749807612101238\n",
            "TF: False\n",
            "  going                          -> <START>the                          \n",
            "  reopened                       -> <START>mbert -                      \n",
            "  sun                            -> <START>ri                           \n",
            "  to                             -> <START>to                           \n",
            "  \"                              -> <START>.                            \n",
            "  Batch: 1590 Loss: 13.017298730214437\n",
            "TF: True\n",
            "  Batch: 1620 Loss: 12.951274649302166\n",
            "TF: True\n",
            "  Batch: 1650 Loss: 13.052249908447266\n",
            "TF: False\n",
            "  presence                       -> <START>Isizoe                       \n",
            "  an                             -> <START>ft                           \n",
            "  this                           -> <START>oGtd                         \n",
            "  Play                           -> <START>hae                          \n",
            "  wasted                         -> <START>rodrke                       \n",
            "  Batch: 1680 Loss: 12.970602575937907\n",
            "TF: False\n",
            "  and                            -> <START>wigd                         \n",
            "  activities                     -> <START>taorod                       \n",
            "  their                          -> <START>Copgert                      \n",
            "  which                          -> <START>pmiiemid                     \n",
            "  is                             -> <START>tt                           \n",
            "  Batch: 1710 Loss: 13.432259114583333\n",
            "TF: False\n",
            "  the                            -> <START>e5                           \n",
            "  ,                              -> <START>w                            \n",
            "  by                             -> <START>se                           \n",
            "  her                            -> <START>tg                           \n",
            "  more                           -> <START>aing                         \n",
            "  Batch: 1740 Loss: 13.10018835067749\n",
            "TF: True\n",
            "  Batch: 1770 Loss: 13.210921255747477\n",
            "TF: False\n",
            "  For                            -> <START>teu                          \n",
            "  monoxide                       -> <START>p1xnpremsnua md              \n",
            "  on                             -> <START>fopte                        \n",
            "  such                           -> <START>tmie                         \n",
            "  the                            -> <START>thh                          \n",
            "  Batch: 1800 Loss: 13.038701756795247\n",
            "TF: True\n",
            "  Batch: 1830 Loss: 12.968681971232096\n",
            "TF: True\n",
            "  Batch: 1860 Loss: 12.874611632029216\n",
            "TF: False\n",
            "  south                          -> <START>ocure                        \n",
            "  against                        -> <START>Ctlour                       \n",
            "  \"                              -> <START>.                            \n",
            "  was                            -> <START>aioe                         \n",
            "  agitation                      -> <START>eemelalura                   \n",
            "  Batch: 1890 Loss: 13.180863189697266\n",
            "TF: True\n",
            "  Batch: 1920 Loss: 12.723029549916586\n",
            "TF: False\n",
            "  lead                           -> <START>rFen                         \n",
            "  Kinnaird                       -> <START>foa0aei g                    \n",
            "  .                              -> <START>.                            \n",
            "  ;                              -> <START>,                            \n",
            "  Arthur                         -> <START>silhda                       \n",
            "  Batch: 1950 Loss: 13.948807207743327\n",
            "TF: False\n",
            "  because                        -> <START>atabobds                     \n",
            "  rollers                        -> <START>dri                          \n",
            "  here                           -> <START>iba                          \n",
            "  apply                          -> <START>NTtte                        \n",
            "  all                            -> <START>the                          \n",
            "  Batch: 1980 Loss: 12.75368226369222\n",
            "TF: False\n",
            "  32                             -> <START>ban                          \n",
            "  with                           -> <START>aal                          \n",
            "  all                            -> <START>iea                          \n",
            "  and                            -> <START>aaadR                        \n",
            "  of                             -> <START>mt                           \n",
            "  Batch: 2010 Loss: 13.039904244740804\n",
            "TF: True\n",
            "  Batch: 2040 Loss: 12.964721330006917\n",
            "TF: False\n",
            "  house                          -> <START>wce-c                        \n",
            "  boys                           -> <START>itha                         \n",
            "  its                            -> <START>Uhn                          \n",
            "  is                             -> <START>-a                           \n",
            "  might                          -> <START>rineti                       \n",
            "  Batch: 2070 Loss: 13.36134459177653\n",
            "TF: False\n",
            "  that                           -> <START>rbs                          \n",
            "  has                            -> <START>eoc                          \n",
            "  shape                          -> <START>ceyuy                        \n",
            "  to                             -> <START>th                           \n",
            "  .                              -> <START>.                            \n",
            "  Batch: 2100 Loss: 13.057107861836752\n",
            "TF: True\n",
            "  Batch: 2130 Loss: 12.593471590677897\n",
            "TF: False\n",
            "  I                              -> <START>.                            \n",
            "  sardonic                       -> <START>treimp                       \n",
            "  the                            -> <START>vhi                          \n",
            "  and                            -> <START>Mrte                         \n",
            "  you                            -> <START>opsr                         \n",
            "  Batch: 2160 Loss: 12.913851197560628\n",
            "TF: True\n",
            "  Batch: 2190 Loss: 12.714178085327148\n",
            "TF: True\n",
            "  Batch: 2220 Loss: 13.0985413869222\n",
            "TF: False\n",
            "  which                          -> <START>pwwrete                      \n",
            "  leading                        -> <START>bit ylr                      \n",
            "  Oi                             -> <START>aee                          \n",
            "  came                           -> <START>uoyrcy                       \n",
            "  water                          -> <START>hmrgt                        \n",
            "  Batch: 2250 Loss: 12.849110889434815\n",
            "TF: False\n",
            "  .                              -> <START>,                            \n",
            "  a                              -> <START>-                            \n",
            "  ,                              -> <START>.                            \n",
            "  fact                           -> <START>tif                          \n",
            "  a                              -> <START>a                            \n",
            "  Batch: 2280 Loss: 12.82869618733724\n",
            "TF: False\n",
            "  bit                            -> <START>sna                          \n",
            "  .                              -> <START>t                            \n",
            "  \"                              -> <START>\"                            \n",
            "  Ponsonby                       -> <START>lrsppiies                    \n",
            "  that                           -> <START>are                          \n",
            "  Batch: 2310 Loss: 12.515231577555339\n",
            "TF: False\n",
            "  him                            -> <START>Was                          \n",
            "  very                           -> <START>he                           \n",
            "  rumbled                        -> <START>mhcmPoele                    \n",
            "  to                             -> <START>be                           \n",
            "  outstanding                    -> <START>xrrcinitsnge                 \n",
            "  Batch: 2340 Loss: 12.937541548411051\n",
            "TF: True\n",
            "  Batch: 2370 Loss: 12.634805679321289\n",
            "TF: True\n",
            "  Batch: 2400 Loss: 12.43637620608012\n",
            "TF: False\n",
            "  for                            -> <START>fee                          \n",
            "  first                          -> <START>Svcne                        \n",
            "  \"                              -> <START>n                            \n",
            "  negotiators                    -> <START>ldmaponnnkderke              \n",
            "  forms                          -> <START>fwerr                        \n",
            "  Batch: 2430 Loss: 12.771931807200113\n",
            "TF: True\n",
            "  Batch: 2460 Loss: 13.296969191233318\n",
            "TF: True\n",
            "  Batch: 2490 Loss: 13.207826391855876\n",
            "TF: False\n",
            "  cries                          -> <START>whvn                         \n",
            "  us                             -> <START>eak                          \n",
            "  a                              -> <START>t                            \n",
            "  don't                          -> <START>eamuev                       \n",
            "  the                            -> <START>hecl                         \n",
            "  Batch: 2520 Loss: 13.32680508295695\n",
            "TF: True\n",
            "  Batch: 2550 Loss: 12.223252550760906\n",
            "Training epoch 1...\n",
            "TF: False\n",
            "  or                             -> <START>ih                           \n",
            "  had                            -> <START>we                           \n",
            "  the                            -> <START>ahedd                        \n",
            "  ,                              -> <START>.                            \n",
            "  .                              -> <START>,                            \n",
            "  Batch: 30 Loss: 13.02295773824056\n",
            "TF: True\n",
            "  Batch: 60 Loss: 12.866643810272217\n",
            "TF: True\n",
            "  Batch: 90 Loss: 12.11960875193278\n",
            "TF: True\n",
            "  Batch: 120 Loss: 12.777269109090168\n",
            "TF: True\n",
            "  Batch: 150 Loss: 12.803348827362061\n",
            "TF: False\n",
            "  change                         -> <START>phueelst                     \n",
            "  '                              -> <START>'                            \n",
            "  ebriety                        -> <START>Mforains                     \n",
            "  to                             -> <START>jf                           \n",
            "  direction                      -> <START>eoiosene                     \n",
            "  Batch: 180 Loss: 12.4664813041687\n",
            "TF: True\n",
            "  Batch: 210 Loss: 12.827177429199219\n",
            "TF: False\n",
            "  want                           -> <START>:iomm                        \n",
            "  containing                     -> <START>CWexapgi                     \n",
            "  !                              -> <START>t                            \n",
            "  lordship                       -> <START>tiglny n                     \n",
            "  '                              -> <START>I                            \n",
            "  Batch: 240 Loss: 12.274530665079753\n",
            "TF: True\n",
            "  Batch: 270 Loss: 12.335462029774984\n",
            "TF: False\n",
            "  EICHMANN                       -> <START>Toneer isyt                  \n",
            "  once                           -> <START>mgre                         \n",
            "  under                          -> <START>tils                         \n",
            "  common                         -> <START>mkoueyki                     \n",
            "  .                              -> <START>.                            \n",
            "  Batch: 300 Loss: 12.358172829945882\n",
            "TF: True\n",
            "  Batch: 330 Loss: 12.62262315750122\n",
            "TF: True\n",
            "  Batch: 360 Loss: 12.415175278981527\n",
            "TF: False\n",
            "  not                            -> <START>wed                          \n",
            "  important                      -> <START>cmenerisnls                  \n",
            "  the                            -> <START>tes                          \n",
            "  ,                              -> <START>,                            \n",
            "  it                             -> <START>th                           \n",
            "  Batch: 390 Loss: 12.843076674143473\n",
            "TF: True\n",
            "  Batch: 420 Loss: 12.108991781870523\n",
            "TF: True\n",
            "  Batch: 450 Loss: 12.590227095286052\n",
            "TF: False\n",
            "  it                             -> <START>te                           \n",
            "  that                           -> <START>+hes                         \n",
            "  intellectual                   -> <START>iiimmterst                   \n",
            "  to                             -> <START>the                          \n",
            "  towards                        -> <START>iorlotin                     \n",
            "  Batch: 480 Loss: 12.174670505523682\n",
            "TF: False\n",
            "  be                             -> <START>je                           \n",
            "  law                            -> <START>owen                         \n",
            "  high                           -> <START>\"Bords                       \n",
            "  greater                        -> <START>outaet                       \n",
            "  for                            -> <START>Oon                          \n",
            "  Batch: 510 Loss: 12.57153975168864\n",
            "TF: True\n",
            "  Batch: 540 Loss: 11.950857480367025\n",
            "TF: True\n",
            "  Batch: 570 Loss: 12.511285463968912\n",
            "TF: False\n",
            "  their                          -> <START>honly                        \n",
            "  gross                          -> <START>mea                          \n",
            "  them                           -> <START>wwne                         \n",
            "  staff                          -> <START>mruel                        \n",
            "  you                            -> <START>thea                         \n",
            "  Batch: 600 Loss: 12.296781762440999\n",
            "TF: True\n",
            "  Batch: 630 Loss: 11.98783572514852\n",
            "TF: True\n",
            "  Batch: 660 Loss: 12.581046295166015\n",
            "TF: False\n",
            "  said                           -> <START>orshw                        \n",
            "  As                             -> <START>ot                e          \n",
            "  American                       -> <START>suethremsee                  \n",
            "  blow                           -> <START>wid s                        \n",
            "  in                             -> <START>if                           \n",
            "  Batch: 690 Loss: 11.927055231730144\n",
            "TF: False\n",
            "  little                         -> <START>oind                         \n",
            "  wire                           -> <START>baarl                        \n",
            "  size                           -> <START>lacre s                      \n",
            "  Mr.                            -> <START>wh                           \n",
            "  east                           -> <START>hheme                        \n",
            "  Batch: 720 Loss: 12.435029665629068\n",
            "TF: True\n",
            "  Batch: 750 Loss: 12.275553862253824\n",
            "TF: True\n",
            "  Batch: 780 Loss: 12.389267444610596\n",
            "TF: True\n",
            "  Batch: 810 Loss: 12.577690855662029\n",
            "TF: True\n",
            "  Batch: 840 Loss: 12.227963781356811\n",
            "TF: True\n",
            "  Batch: 870 Loss: 13.011410522460938\n",
            "TF: False\n",
            "  more                           -> <START>yhee                         \n",
            "  completed                      -> <START>nMirnt                       \n",
            "  a                              -> <START>a                            \n",
            "  and                            -> <START>ffy                          \n",
            "  stay                           -> <START>ooi                          \n",
            "  Batch: 900 Loss: 12.193400033315022\n",
            "TF: True\n",
            "  Batch: 930 Loss: 11.752361869812011\n",
            "TF: False\n",
            "  \"                              -> <START>f                            \n",
            "  to                             -> <START>t                            \n",
            "  over                           -> <START>the                          \n",
            "  throw                          -> <START>hinerse                      \n",
            "  what                           -> <START>hoen                         \n",
            "  Batch: 960 Loss: 12.544784228006998\n",
            "TF: False\n",
            "  me                             -> <START>uon                          \n",
            "  there                          -> <START>Tass                         \n",
            "  \"                              -> <START>a                            \n",
            "  whom                           -> <START>bmutt                        \n",
            "  shabby                         -> <START>errnlr                       \n",
            "  Batch: 990 Loss: 12.329388618469238\n",
            "TF: False\n",
            "  Secretary                      -> <START>Sfronn                       \n",
            "  hour                           -> <START>xhen                         \n",
            "  there                          -> <START>ines                         \n",
            "  and                            -> <START>teast                        \n",
            "  \"                              -> <START>,V                           \n",
            "  Batch: 1020 Loss: 12.73208573659261\n",
            "TF: False\n",
            "  obligation                     -> <START>liyie2einnp ut               \n",
            "  That                           -> <START>ois                          \n",
            "  West                           -> <START>thHer                        \n",
            "  the-                           -> <START>thae                         \n",
            "  so                             -> <START>o                            \n",
            "  Batch: 1050 Loss: 12.677338218688964\n",
            "TF: True\n",
            "  Batch: 1080 Loss: 12.32542371749878\n",
            "TF: True\n",
            "  Batch: 1110 Loss: 12.156336943308512\n",
            "TF: True\n",
            "  Batch: 1140 Loss: 11.864098993937175\n",
            "TF: False\n",
            "  formidable                     -> <START>rsmicss glf                  \n",
            "  this                           -> <START>siid                         \n",
            "  can                            -> <START>hhe                          \n",
            "  ,                              -> <START>b                            \n",
            "  eye                            -> <START>aHe                          \n",
            "  Batch: 1170 Loss: 12.1277383963267\n",
            "TF: False\n",
            "  in                             -> <START>es                           \n",
            "  2a-calling                     -> <START>hcenisg                      \n",
            "  cannot                         -> <START>arnnded                      \n",
            "  freedom                        -> <START>coichga  e   s               \n",
            "  there                          -> <START>weth                         \n",
            "  Batch: 1200 Loss: 12.51279985109965\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-b7bf468b594c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-b7bf468b594c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0msymb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_symbol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0muse_teacher_forcing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcurrent_symbol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-ab15e69c5d87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, prev, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Yg8-hpPq2e2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}