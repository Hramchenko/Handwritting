{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V9 of EQ_no_prof simpl new_proffs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/V9_of_EQ_no_prof_simpl_new_proffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "6852b03c-900c-49d3-ae61-9ada2133fc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "image_width = 1500\n",
        "image_height = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqHNfBMaLYmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from xml.dom import minidom\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from random import random\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "class ArtificialHandwrittingObject:\n",
        "    \n",
        "    def __init__(self, name, img):\n",
        "        self.name = name\n",
        "        self.image = img\n",
        "\n",
        "class ArtificialHandwritting:\n",
        "    \n",
        "    def __init__(self, datasetDir, scale, image_width, image_height, encode_word):\n",
        "        self.scale = scale\n",
        "        self.height = image_height\n",
        "        self.width = image_width\n",
        "        self.datasetDirectory = datasetDir\n",
        "        self.data = {}\n",
        "        self.initOffsets()\n",
        "        self.encode_word = encode_word\n",
        "        \n",
        "        for f_name in os.listdir(datasetDir):\n",
        "            if not f_name.endswith(\".xml\"):\n",
        "                continue\n",
        "            full_path = datasetDir + f_name\n",
        "            print(full_path)\n",
        "            self.process_xml(full_path)\n",
        "            \n",
        "    def initOffsets(self):\n",
        "        self.offsets = {\n",
        "            'a': 0,\n",
        "            'b': -0.1,\n",
        "            'c': 0,\n",
        "            'd': 0,\n",
        "            'e': 0,\n",
        "            'f': -0.2,\n",
        "            'g': +0.3,\n",
        "            'h': 0,\n",
        "            'i': -0.15,\n",
        "            'j': +0.2,\n",
        "            'k': 0,\n",
        "            'l': -0.15,\n",
        "            'm': 0,\n",
        "            'n': 0,\n",
        "            'o': 0,\n",
        "            'p': 0,\n",
        "            'q': +0.2,\n",
        "            'r': 0,\n",
        "            's': 0,\n",
        "            't': -0.2,\n",
        "            'u': 0,\n",
        "            'v': 0,\n",
        "            'w': 0,\n",
        "            'x': 0,\n",
        "            'y': +0.2,\n",
        "            'z': 0,\n",
        "        }\n",
        "        \n",
        "        \n",
        "        \n",
        "    def getValue(self, node, name):\n",
        "        v  = node.getElementsByTagName(name) \n",
        "        if len(v) != 1:\n",
        "            raise Exception()\n",
        "        v = v[0].firstChild.data\n",
        "        return v\n",
        "    \n",
        "    def make_random_batch(self, word_len, rand_x):\n",
        "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "        texts = []\n",
        "        datas = []\n",
        "        #data = torch.FloatTensor(batch_size, self.image_height, self.image_width)\n",
        "        for batch_idx in range(0, batch_size):\n",
        "          w = \"\"\n",
        "          for i in range(0, word_len):\n",
        "            s_idx = floor(random()*len(alphabet))\n",
        "            s = alphabet[s_idx]\n",
        "            w += s\n",
        "          x = floor(random()*rand_x)\n",
        "          d, _ = self.make_word(w, x)\n",
        "          d = d = torch.as_tensor(d)#.unsqueeze(0)\n",
        "          \n",
        "          datas.append(d)\n",
        "          w = self.encode_word(w)\n",
        "          texts.append(w)\n",
        "        t = torch.stack(texts)\n",
        "        data = torch.stack(datas, dim=0)\n",
        "        return data, t\n",
        "          \n",
        "       \n",
        "    def make_word(self, word, x_start):\n",
        "        center = self.height*0.5\n",
        "        result = sp.ones([self.height, self.width], dtype=\"float32\")\n",
        "        candidates = []\n",
        "        for k in self.data.keys():\n",
        "            d = self.data[k]\n",
        "            flag = True\n",
        "            for s in word:\n",
        "                if s not in d:\n",
        "                    flag = False\n",
        "                    break\n",
        "            if flag:\n",
        "                candidates.append(k)\n",
        "        idx = floor(random()*len(candidates))\n",
        "        if (len(candidates) == 0):\n",
        "          print(\"Bad word: \" + word)\n",
        "          raise Exception()\n",
        "        \n",
        "        img_file = candidates[idx]\n",
        "        page = self.data[img_file]\n",
        "        x = x_start\n",
        "        for s in word:\n",
        "            s_idx = floor(len(page[s])*random())\n",
        "            obj_im = page[s][s_idx].image\n",
        "            c = 0.5*obj_im.shape[0]\n",
        "            dy = 0 + floor(center - c + obj_im.shape[0]*(self.offsets[s]+random()*0.05))#+random()*0.05))\n",
        "            #print(dy)\n",
        "            #print(obj_im.dtype)\n",
        "            result[dy: dy + obj_im.shape[0], x: x+obj_im.shape[1]] = obj_im\n",
        "            x += obj_im.shape[1]\n",
        "        return result, img_file  \n",
        "      \n",
        "            \n",
        "    def process_xml(self, f_path):\n",
        "        \n",
        "        xml_file = minidom.parse(f_path)\n",
        "        #xml_file.normalize()\n",
        "        annotation = xml_file.getElementsByTagName('annotation')\n",
        "        if len(annotation) != 1:\n",
        "            raise Exception()\n",
        "        annotation = annotation[0]\n",
        "        img_name = self.getValue(annotation, 'filename')\n",
        "        image_file = self.datasetDirectory + img_name\n",
        "        img = cv2.imread(image_file, 0)\n",
        "        print(image_file)\n",
        "        objects = annotation.getElementsByTagName('object')\n",
        "        #print(len(objects))\n",
        "        content = {}\n",
        "        for obj in objects:\n",
        "            name  = self.getValue(obj, 'name')\n",
        "            if name not in content:\n",
        "                content[name] = []\n",
        "            x_min = int(self.getValue(obj, 'xmin'))\n",
        "            y_min = int(self.getValue(obj, 'ymin'))\n",
        "            x_max = int(self.getValue(obj, 'xmax'))\n",
        "            y_max = int(self.getValue(obj, 'ymax'))\n",
        "            obj_im = img[y_min: y_max, x_min: x_max]\n",
        "            s = self.scale\n",
        "            sh = obj_im.shape\n",
        "            obj_im = cv2.resize(obj_im, (floor(sh[1]*s), floor(sh[0]*s)), cv2.INTER_LANCZOS4)\n",
        "            if obj_im.shape[0] > self.height:\n",
        "                continue\n",
        "            obj_im = sp.float32(obj_im)    \n",
        "            obj_im = (obj_im - obj_im.min())/(obj_im.max() - obj_im.min())\n",
        "            handwr_obj = ArtificialHandwrittingObject(name, obj_im)\n",
        "            content[name].append(handwr_obj)\n",
        "        if img_name in self.data:\n",
        "            raise Exception()\n",
        "        self.data[img_name] = content\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "e9e11010-cf21-4f3e-8606-a3f158efe02f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "\n",
        "pad_length=-1\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPROjYtaLvLS",
        "colab_type": "code",
        "outputId": "d890f8ec-e287-4e7f-83f9-b108c1a40f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "artificial_data = ArtificialHandwritting(\"./MyLetters/\", 1.0, image_width, image_height, train_set.encode_word)\n",
        "data, target = artificial_data.make_random_batch(3, 400)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./MyLetters/a01-107u.xml\n",
            "./MyLetters/a01-107u.png\n",
            "./MyLetters/my.xml\n",
            "./MyLetters/my.jpg\n",
            "./MyLetters/a01-053u.xml\n",
            "./MyLetters/a01-053u.png\n",
            "./MyLetters/a01-128.xml\n",
            "./MyLetters/a01-128.png\n",
            "./MyLetters/a01-107.xml\n",
            "./MyLetters/a01-107.png\n",
            "./MyLetters/a01-026u.xml\n",
            "./MyLetters/a01-026u.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yx2SxZgGT0Z-",
        "colab_type": "code",
        "outputId": "e965a342-6619-4b0f-c68b-2af32b5ce262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "artificial_data.data[\"my.jpg\"].keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 'u', 't', 'v', 'w', 'x', 'y', 'z', 'j'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wceUSjfAWM1v",
        "colab_type": "code",
        "outputId": "6a1ac8e8-eb09-4a88-f58c-7e4d6b99a645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(data[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAABeCAYAAADyiGdeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEzIREZEG/A+izzLjWI3aYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 200, 1500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOlJkOzNgngX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Layers import *\n",
        "from HTREncoder import *\n",
        "from HTRDecoder import *\n",
        "from HTRDiscriminator import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "outputId": "1e5159d8-ecb8-4d0a-c639-bf1f99fdce43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 9, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "cXja4G8p7KKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_onehot(x, n):\n",
        "    one_hot = torch.zeros((x.shape[0], n)).to(device)\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoderResult:\n",
        "  \n",
        "  def __init__(self):\n",
        "    None\n",
        "\n",
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, batch_size, ntoken, encoded_width=92, encoded_height=64, batchnorm=False, dropout=True, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = features_size\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "            hidden_m = hidden.permute(1, 0, 2)\n",
        "            hidden_m = hidden_m.flatten(start_dim=1)\n",
        "            #print(x.shape)\n",
        "            #print(hidden_m.shape)\n",
        "            x_m = x.flatten(start_dim=1)\n",
        "            attention_inp = torch.cat([x_m, hidden_m], dim=1).detach()\n",
        "            self.attention_weights = self.attention(attention_inp)\n",
        "            self.attention_weights = F.softmax(self.attention_weights, dim=1).unsqueeze(1)\n",
        "            #print(\"iiiif\")\n",
        "            #print(self.attention_weights.shape)\n",
        "            \n",
        "            self.attention_weights = self.attention_weights.repeat([1, self.encoded_height, 1])\n",
        "            #print(\"fffff\")\n",
        "            #print(x.shape)\n",
        "            #print(self.attention_weights.shape)\n",
        "            x = x * self.attention_weights\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        x = torch.cat([x.flatten(start_dim=1), emb], dim=1)\n",
        "        x = x.unsqueeze(0)\n",
        "        result = HTRDecoderResult()\n",
        "        result.rnn_input = x\n",
        "        result.input_hidden = hidden\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        result.x = x\n",
        "        result.hidden = hidden\n",
        "        return result\n",
        "      \n",
        "    def makeHidden(self):\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            h2 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return (h1, h2)\n",
        "        else:\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return h1\n",
        "\n",
        "\n",
        "decoder = HTRDecoder(batch_size, len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHbnIOOP03r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(max_size):\n",
        "  print(\"Testing...\")\n",
        "  \n",
        "  freq = 20\n",
        "  \n",
        "  test_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  \n",
        "  test_loss = 0\n",
        "  \n",
        "  with torch.no_grad():  \n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "\n",
        "      if True:\n",
        "        l = 1 + floor((max_size - 1)*random())\n",
        "        data, target = artificial_data.make_random_batch(l, 200)\n",
        "        orig_data = data*255;\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)  \n",
        "      else:  \n",
        "        orig_data, target = batch\n",
        "        data = orig_data/255.0\n",
        "        #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)\n",
        "      hidden = decoder.makeHidden().to(device)    \n",
        "\n",
        "      loss = 0\n",
        "      enc = encoder(data)\n",
        "      #print(enc.shape)\n",
        "      s = enc\n",
        "      #print(s.shape)\n",
        "      #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "\n",
        "      old_symbol[:, 0] = START\n",
        "\n",
        "      for i in range(0, target.shape[1]):\n",
        "\n",
        "        decoder_result = decoder(s, old_symbol, hidden)\n",
        "        dec = decoder_result.x\n",
        "        hidden = decoder_result.hidden\n",
        "\n",
        "        recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "\n",
        "        loss += criterion(dec, target[:, i])\n",
        "      c_loss += loss.item()/(target.shape[1] + 0)\n",
        "      test_loss += loss.item()/(target.shape[1] + 0)\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        if True:#not use_teacher_forcing:\n",
        "          for k in range(0, min(3, target.shape[0])):\n",
        "              decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "              plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "              plt.show()\n",
        "              print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "        c_loss /= freq \n",
        "        print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "        c_loss = 0\n",
        "      batch_idx += 1  \n",
        "  print(\"Test loss: %f\" % (test_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dvEYjtLYlcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator = HTRDiscriminator(batch_size, 256*2, 512, 10, len(train_set.codes)).to(device)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zel5bfutWKKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_discriminator(s, target, use_teacher_forcing, train_mode, discriminator_target):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  train_mask = torch.ByteTensor(batch_size).to(device)\n",
        "  train_mask[:] = 1\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)    \n",
        "  discriminator_loss = 0\n",
        "  discriminator_hidden = discriminator.makeHidden().to(device)\n",
        "\n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    decoder_outputs = dec.topk(1, dim=1)[1].flatten()\n",
        "\n",
        "    \n",
        "    if train_mode:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden.detach(), decoder_result.input_hidden.detach(),decoder_result.rnn_input.detach(), decoder_outputs.detach(), discriminator_target)\n",
        "    else:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden, decoder_result.input_hidden, decoder_result.rnn_input, decoder_outputs, discriminator_target)\n",
        "      \n",
        "    if i != 0:\n",
        "      discriminator_loss += dl\n",
        "\n",
        "    recognition_result[:, i] = decoder_outputs.detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]    \n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        " \n",
        "  discriminator_loss /= target.shape[1]\n",
        "  return (recognition_result, loss, discriminator_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yrtvy2ikWNwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_decoder(s, target, use_teacher_forcing):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)  \n",
        "  \n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]\n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        "  return (recognition_result, loss)\n",
        "\n",
        "\n",
        "batch_zeros = torch.zeros(batch_size, 1).to(device)\n",
        "batch_ones = torch.ones(batch_size, 1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DxYdkXeTnNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 30\n",
        "  \n",
        "  train_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "\n",
        "  \n",
        "  train_loss = 0\n",
        "  discr_applied = 0\n",
        "  \n",
        "  while True:\n",
        "    if batch_idx > 400:\n",
        "      break\n",
        "    if True:\n",
        "      l = 1 + floor((max_size - 1)*random())\n",
        "      data, target = artificial_data.make_random_batch(l, 200)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "    else:\n",
        "      batch = train_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "      orig_data, target = batch\n",
        "      data = orig_data/255.0\n",
        "      #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    \n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc\n",
        "    #s = enc.permute(1, 0, 2)\n",
        "    #s = s.squeeze(0)\n",
        "    #print(s.shape)\n",
        "    #print(s.shape)\n",
        "    #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    #s = s.flatten(start_dim=1).squeeze(0)\n",
        "    s = s.unsqueeze(0)\n",
        "    #print(s.shape)\n",
        "    \n",
        "    discr_loss = 1\n",
        "    if True:#target.shape[1] > 1:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      _, _, tf_loss = apply_discriminator(s, target, True, True, batch_ones)\n",
        "      _, _, fr_loss = apply_discriminator(s, target, False, True, batch_zeros)\n",
        "      dl = tf_loss + fr_loss\n",
        "      discr_loss = dl.item()\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        print(\"Discr loss: %f\" %(dl.item()))\n",
        "      #print(dl)\n",
        "      dl.backward()\n",
        "      discriminator_optimizer.step()\n",
        "   \n",
        "\n",
        "      #     use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "      # #    recognition_result, loss, discriminator_loss = apply_discriminator(s, target, use_teacher_forcing, True, batch_zeros)\n",
        "      #     recognition_result, loss = apply_decoder(s, target, use_teacher_forcing)\n",
        "    batch_loss = None  \n",
        "    if discr_loss < 0.3:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      recognition_result, loss, discriminator_loss = apply_discriminator(s, target, True, False, batch_zeros)\n",
        "      batch_loss = loss.item()\n",
        "      loss = loss + discriminator_loss\n",
        "      discr_applied += 1\n",
        "      #print(\"Apply descr...\")\n",
        "    else:\n",
        "      recognition_result, loss = apply_decoder(s, target, True)\n",
        "      batch_loss = loss.item()\n",
        "\n",
        "      \n",
        "      \n",
        "    c_loss += batch_loss/(target.shape[1] + 0)\n",
        "    train_loss += batch_loss/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      \n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "  print(\"Discr applied %d times.\"%discr_applied)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xrGl2QIsP13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = \"/gdrive/My Drive/v9.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3iWRuawx-Dx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state = torch.load(file_name)\n",
        "# encoder.load_state_dict(state[\"encoder\"])\n",
        "# encoder_optimizer.load_state_dict(state[\"encoder_optimizer\"])\n",
        "# decoder.load_state_dict(state[\"decoder\"])\n",
        "# decoder_optimizer.load_state_dict(state[\"decoder_optimizer\"])\n",
        "# discriminator.load_state_dict(state[\"discriminator\"])\n",
        "# discriminator_optimizer.load_state_dict(state[\"discriminator_optimizer\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJBi4bwwywG6",
        "colab_type": "code",
        "outputId": "1acb8b74-2dd2-4a02-f532-0e26646e5167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 100):\n",
        "  max_size = 2\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "Discr loss: 0.117913\n",
            "  Batch: 30 Loss: 3.613363178571065\n",
            "Discr loss: 0.160202\n",
            "  Batch: 60 Loss: 3.4773786783218386\n",
            "Discr loss: 0.288539\n",
            "  Batch: 90 Loss: 3.4538140376408895\n",
            "Discr loss: 0.235093\n",
            "  Batch: 120 Loss: 3.463936535517375\n",
            "Discr loss: 0.223893\n",
            "  Batch: 150 Loss: 3.4476118882497153\n",
            "Discr loss: 0.193378\n",
            "  Batch: 180 Loss: 3.4440653721491494\n",
            "Discr loss: 0.183503\n",
            "  Batch: 210 Loss: 3.430910611152649\n",
            "Discr loss: 0.191703\n",
            "  Batch: 240 Loss: 3.431853771209717\n",
            "Discr loss: 0.121503\n",
            "  Batch: 270 Loss: 3.4345165967941282\n",
            "Discr loss: 0.153652\n",
            "  Batch: 300 Loss: 3.41439205010732\n",
            "Discr loss: 0.316357\n",
            "  Batch: 330 Loss: 3.3812597751617433\n",
            "Discr loss: 0.182390\n",
            "  Batch: 360 Loss: 3.363641873995463\n",
            "Discr loss: 0.101862\n",
            "  Batch: 390 Loss: 3.365760310490926\n",
            "Train loss: 3.429950\n",
            "Discr applied 368 times.\n",
            "Testing...\n",
            "Test loss: 3.350569\n",
            "Training epoch 1...\n",
            "Discr loss: 0.178120\n",
            "  Batch: 30 Loss: 3.4871156851450604\n",
            "Discr loss: 0.083078\n",
            "  Batch: 60 Loss: 3.3841721614201865\n",
            "Discr loss: 0.280914\n",
            "  Batch: 90 Loss: 3.384622049331665\n",
            "Discr loss: 0.266363\n",
            "  Batch: 120 Loss: 3.366774384180705\n",
            "Discr loss: 0.180922\n",
            "  Batch: 150 Loss: 3.350455625851949\n",
            "Discr loss: 0.125525\n",
            "  Batch: 180 Loss: 3.350015370051066\n",
            "Discr loss: 0.185369\n",
            "  Batch: 210 Loss: 3.3319718996683756\n",
            "Discr loss: 0.408873\n",
            "  Batch: 240 Loss: 3.3531299591064454\n",
            "Discr loss: 0.263762\n",
            "  Batch: 270 Loss: 3.3340552965799968\n",
            "Discr loss: 0.173089\n",
            "  Batch: 300 Loss: 3.343538681666056\n",
            "Discr loss: 0.278544\n",
            "  Batch: 330 Loss: 3.3505672375361124\n",
            "Discr loss: 0.144585\n",
            "  Batch: 360 Loss: 3.339695358276367\n",
            "Discr loss: 0.181324\n",
            "  Batch: 390 Loss: 3.3526994466781614\n",
            "Train loss: 3.355032\n",
            "Discr applied 388 times.\n",
            "Testing...\n",
            "Test loss: 3.329305\n",
            "Training epoch 2...\n",
            "Discr loss: 0.171028\n",
            "  Batch: 30 Loss: 3.437469736735026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b121bc4025e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0c4c1fcac14f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, max_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_zeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfr_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0mdiscr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Discr loss: %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HsfEczGzx_ph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state_dict = {\n",
        "#         \"encoder\": encoder.state_dict(),\n",
        "#         \"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
        "#         \"decoder\": decoder.state_dict(),\n",
        "#         \"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
        "#         \"discriminator\": discriminator.state_dict(),\n",
        "#         \"discriminator_optimizer\": discriminator_optimizer.state_dict(),\n",
        "#       }\n",
        "\n",
        "# torch.save(state_dict, file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8IIhQBGsOoL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d2c17f51-74a4-4e19-92d1-c3e69e6663a8"
      },
      "cell_type": "code",
      "source": [
        "test(3)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  'bo' -> 'o<STOP><START>'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  'hu' -> 'f<STOP><START>'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAABPCAYAAAAUa1W3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  'ip' -> 'w<STOP><START>'\n",
            "  Batch: 20 Loss: 6.226113510131836\n",
            "Test loss: 5.916594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mrN5kEctKjQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data, target = artificial_data.make_random_batch(3, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBYhXSxDKjtf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCBCuthGKmW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 30):\n",
        "  plt.imshow(data[i])\n",
        "  plt.show()\n",
        "  print(test_set.decode_word(target[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obmiumeGK5E9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}