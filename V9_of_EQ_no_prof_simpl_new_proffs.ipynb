{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V9 of EQ_no_prof simpl new_proffs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/V9_of_EQ_no_prof_simpl_new_proffs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "05db02cc-d9f5-483f-b2e7-9d5c38e6ba66"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "image_width = 1500\n",
        "image_height = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqHNfBMaLYmd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from xml.dom import minidom\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from random import random\n",
        "import scipy as sp\n",
        "\n",
        "\n",
        "class ArtificialHandwrittingObject:\n",
        "    \n",
        "    def __init__(self, name, img):\n",
        "        self.name = name\n",
        "        self.image = img\n",
        "\n",
        "class ArtificialHandwritting:\n",
        "    \n",
        "    def __init__(self, datasetDir, scale, image_width, image_height, encode_word):\n",
        "        self.scale = scale\n",
        "        self.height = image_height\n",
        "        self.width = image_width\n",
        "        self.datasetDirectory = datasetDir\n",
        "        self.data = {}\n",
        "        self.initOffsets()\n",
        "        self.encode_word = encode_word\n",
        "        \n",
        "        for f_name in os.listdir(datasetDir):\n",
        "            if not f_name.endswith(\".xml\"):\n",
        "                continue\n",
        "            full_path = datasetDir + f_name\n",
        "            print(full_path)\n",
        "            self.process_xml(full_path)\n",
        "            \n",
        "    def initOffsets(self):\n",
        "        self.offsets = {\n",
        "            'a': 0,\n",
        "            'b': -0.1,\n",
        "            'c': 0,\n",
        "            'd': 0,\n",
        "            'e': 0,\n",
        "            'f': -0.2,\n",
        "            'g': +0.3,\n",
        "            'h': 0,\n",
        "            'i': -0.15,\n",
        "            'j': +0.2,\n",
        "            'k': 0,\n",
        "            'l': -0.15,\n",
        "            'm': 0,\n",
        "            'n': 0,\n",
        "            'o': 0,\n",
        "            'p': 0,\n",
        "            'q': +0.2,\n",
        "            'r': 0,\n",
        "            's': 0,\n",
        "            't': -0.2,\n",
        "            'u': 0,\n",
        "            'v': 0,\n",
        "            'w': 0,\n",
        "            'x': 0,\n",
        "            'y': +0.2,\n",
        "            'z': 0,\n",
        "        }\n",
        "        \n",
        "        \n",
        "        \n",
        "    def getValue(self, node, name):\n",
        "        v  = node.getElementsByTagName(name) \n",
        "        if len(v) != 1:\n",
        "            raise Exception()\n",
        "        v = v[0].firstChild.data\n",
        "        return v\n",
        "    \n",
        "    def make_random_batch(self, word_len, rand_x):\n",
        "        alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "        texts = []\n",
        "        datas = []\n",
        "        #data = torch.FloatTensor(batch_size, self.image_height, self.image_width)\n",
        "        for batch_idx in range(0, batch_size):\n",
        "          w = \"\"\n",
        "          for i in range(0, word_len):\n",
        "            s_idx = floor(random()*len(alphabet))\n",
        "            s = alphabet[s_idx]\n",
        "            w += s\n",
        "          x = floor(random()*rand_x)\n",
        "          d, _ = self.make_word(w, x)\n",
        "          d = d = torch.as_tensor(d)#.unsqueeze(0)\n",
        "          \n",
        "          datas.append(d)\n",
        "          w = self.encode_word(w)\n",
        "          texts.append(w)\n",
        "        t = torch.stack(texts)\n",
        "        data = torch.stack(datas, dim=0)\n",
        "        return data, t\n",
        "          \n",
        "       \n",
        "    def make_word(self, word, x_start):\n",
        "        center = self.height*0.5\n",
        "        result = sp.ones([self.height, self.width], dtype=\"float32\")\n",
        "        candidates = []\n",
        "        for k in self.data.keys():\n",
        "            d = self.data[k]\n",
        "            flag = True\n",
        "            for s in word:\n",
        "                if s not in d:\n",
        "                    flag = False\n",
        "                    break\n",
        "            if flag:\n",
        "                candidates.append(k)\n",
        "        idx = floor(random()*len(candidates))\n",
        "        if (len(candidates) == 0):\n",
        "          print(\"Bad word: \" + word)\n",
        "          raise Exception()\n",
        "        \n",
        "        img_file = candidates[idx]\n",
        "        page = self.data[img_file]\n",
        "        x = x_start\n",
        "        for s in word:\n",
        "            s_idx = floor(len(page[s])*random())\n",
        "            obj_im = page[s][s_idx].image\n",
        "            c = 0.5*obj_im.shape[0]\n",
        "            dy = 0 + floor(center - c + obj_im.shape[0]*(self.offsets[s]+random()*0.05))#+random()*0.05))\n",
        "            #print(dy)\n",
        "            #print(obj_im.dtype)\n",
        "            result[dy: dy + obj_im.shape[0], x: x+obj_im.shape[1]] = obj_im\n",
        "            x += obj_im.shape[1]\n",
        "        return result, img_file  \n",
        "      \n",
        "            \n",
        "    def process_xml(self, f_path):\n",
        "        \n",
        "        xml_file = minidom.parse(f_path)\n",
        "        #xml_file.normalize()\n",
        "        annotation = xml_file.getElementsByTagName('annotation')\n",
        "        if len(annotation) != 1:\n",
        "            raise Exception()\n",
        "        annotation = annotation[0]\n",
        "        img_name = self.getValue(annotation, 'filename')\n",
        "        image_file = self.datasetDirectory + img_name\n",
        "        img = cv2.imread(image_file, 0)\n",
        "        print(image_file)\n",
        "        objects = annotation.getElementsByTagName('object')\n",
        "        #print(len(objects))\n",
        "        content = {}\n",
        "        for obj in objects:\n",
        "            name  = self.getValue(obj, 'name')\n",
        "            if name not in content:\n",
        "                content[name] = []\n",
        "            x_min = int(self.getValue(obj, 'xmin'))\n",
        "            y_min = int(self.getValue(obj, 'ymin'))\n",
        "            x_max = int(self.getValue(obj, 'xmax'))\n",
        "            y_max = int(self.getValue(obj, 'ymax'))\n",
        "            obj_im = img[y_min: y_max, x_min: x_max]\n",
        "            s = self.scale\n",
        "            sh = obj_im.shape\n",
        "            obj_im = cv2.resize(obj_im, (floor(sh[1]*s), floor(sh[0]*s)), cv2.INTER_LANCZOS4)\n",
        "            if obj_im.shape[0] > self.height:\n",
        "                continue\n",
        "            obj_im = sp.float32(obj_im)    \n",
        "            obj_im = (obj_im - obj_im.min())/(obj_im.max() - obj_im.min())\n",
        "            handwr_obj = ArtificialHandwrittingObject(name, obj_im)\n",
        "            content[name].append(handwr_obj)\n",
        "        if img_name in self.data:\n",
        "            raise Exception()\n",
        "        self.data[img_name] = content\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "7a089540-6248-40e6-aba7-d878b1fe0e92"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "\n",
        "pad_length=-1\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size, line_height=image_height, line_width=image_width, scale=1, pad_length=pad_length, rand_x=400)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aPROjYtaLvLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "076fa8b7-523c-4b86-d06d-36591acb9364"
      },
      "cell_type": "code",
      "source": [
        "artificial_data = ArtificialHandwritting(\"./MyLetters/\", 1.0, image_width, image_height, train_set.encode_word)\n",
        "data, target = artificial_data.make_random_batch(3, 400)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./MyLetters/a01-107u.xml\n",
            "./MyLetters/a01-107u.png\n",
            "./MyLetters/my.xml\n",
            "./MyLetters/my.jpg\n",
            "./MyLetters/a01-053u.xml\n",
            "./MyLetters/a01-053u.png\n",
            "./MyLetters/a01-128.xml\n",
            "./MyLetters/a01-128.png\n",
            "./MyLetters/a01-107.xml\n",
            "./MyLetters/a01-107.png\n",
            "./MyLetters/a01-026u.xml\n",
            "./MyLetters/a01-026u.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yx2SxZgGT0Z-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2d40647-b0cd-4bcd-da3a-d5aa8d7bab8b"
      },
      "cell_type": "code",
      "source": [
        "artificial_data.data[\"my.jpg\"].keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 'u', 't', 'v', 'w', 'x', 'y', 'z', 'j'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "wceUSjfAWM1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "8c39635e-9716-4ee1-8691-d131d45962b9"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(data[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAABeCAYAAADyiGdeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 200, 1500])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FOlJkOzNgngX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from Layers import *\n",
        "from HTREncoder import *\n",
        "from HTRDecoder import *\n",
        "from HTRDiscriminator import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch(use_binarization=False)\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, image_width, image_height).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fvJufA-1d9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f3049b3-b7a6-4f79-ddcf-b99ca6be7643"
      },
      "cell_type": "code",
      "source": [
        "c = encoder(data)\n",
        "c.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64, 9, 11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "cXja4G8p7KKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_onehot(x, n):\n",
        "    one_hot = torch.zeros((x.shape[0], n)).to(device)\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoderResult:\n",
        "  \n",
        "  def __init__(self):\n",
        "    None\n",
        "\n",
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, batch_size, ntoken, encoded_width=92, encoded_height=64, batchnorm=False, dropout=True, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        self.lstm_layers = 2\n",
        "        self.rnn_type = rnn_type\n",
        "        self.emb_size = 128\n",
        "        features_size = self.encoded_height*encoded_width + self.emb_size\n",
        "        from math import floor\n",
        "        lstm_inp_size = features_size\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "            self.rnn = nn.LSTM(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "            self.rnn = nn.GRU(lstm_inp_size, self.lstm_size, self.lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, self.emb_size)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "\n",
        "        self.attention = FullyConnectedX([self.lstm_size*2 + self.encoded_height*encoded_width, self.encoded_height*encoded_width*2,  self.encoded_width], activation_fn=nn.LeakyReLU(0.2), last_fn=nn.Tanh())\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x).squeeze()\n",
        "        if hidden is not None:\n",
        "            hidden_m = hidden.permute(1, 0, 2)\n",
        "            hidden_m = hidden_m.flatten(start_dim=1)\n",
        "            #print(x.shape)\n",
        "            #print(hidden_m.shape)\n",
        "            x_m = x.flatten(start_dim=1)\n",
        "            attention_inp = torch.cat([x_m, hidden_m], dim=1).detach()\n",
        "            self.attention_weights = self.attention(attention_inp)\n",
        "            self.attention_weights = F.softmax(self.attention_weights, dim=1).unsqueeze(1)\n",
        "            #print(\"iiiif\")\n",
        "            #print(self.attention_weights.shape)\n",
        "            \n",
        "            self.attention_weights = self.attention_weights.repeat([1, self.encoded_height, 1])\n",
        "            #print(\"fffff\")\n",
        "            #print(x.shape)\n",
        "            #print(self.attention_weights.shape)\n",
        "            x = x * self.attention_weights\n",
        "        emb = self.embedding(prev).squeeze().detach()\n",
        "        x = torch.cat([x.flatten(start_dim=1), emb], dim=1)\n",
        "        x = x.unsqueeze(0)\n",
        "        result = HTRDecoderResult()\n",
        "        result.rnn_input = x\n",
        "        result.input_hidden = hidden\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.squeeze(dim=0)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        result.x = x\n",
        "        result.hidden = hidden\n",
        "        return result\n",
        "      \n",
        "    def makeHidden(self):\n",
        "        if self.rnn_type == \"LSTM\":\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            h2 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return (h1, h2)\n",
        "        else:\n",
        "            h1 = torch.zeros(self.lstm_layers, self.batch_size, self.lstm_size)\n",
        "            return h1\n",
        "\n",
        "\n",
        "decoder = HTRDecoder(batch_size, len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MHbnIOOP03r-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(max_size):\n",
        "  print(\"Testing...\")\n",
        "  \n",
        "  freq = 20\n",
        "  \n",
        "  test_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  \n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  \n",
        "  test_loss = 0\n",
        "  \n",
        "  with torch.no_grad():  \n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "\n",
        "      if True:\n",
        "        l = 1 + floor((max_size - 1)*random())\n",
        "        data, target = artificial_data.make_random_batch(l, 200)\n",
        "        orig_data = data*255;\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)  \n",
        "      else:  \n",
        "        orig_data, target = batch\n",
        "        data = orig_data/255.0\n",
        "        #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "        data = data.unsqueeze(1).to(device)\n",
        "        target = target.to(device)\n",
        "      hidden = decoder.makeHidden().to(device)    \n",
        "\n",
        "      loss = 0\n",
        "      enc = encoder(data)\n",
        "      #print(enc.shape)\n",
        "      s = enc\n",
        "      #print(s.shape)\n",
        "      #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "\n",
        "      old_symbol[:, 0] = START\n",
        "\n",
        "      for i in range(0, target.shape[1]):\n",
        "\n",
        "        decoder_result = decoder(s, old_symbol, hidden)\n",
        "        dec = decoder_result.x\n",
        "        hidden = decoder_result.hidden\n",
        "\n",
        "        recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "        old_symbol[:, 0] = target[:, i]\n",
        "\n",
        "        loss += criterion(dec, target[:, i])\n",
        "      c_loss += loss.item()/(target.shape[1] + 0)\n",
        "      test_loss += loss.item()/(target.shape[1] + 0)\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        if True:#not use_teacher_forcing:\n",
        "          for k in range(0, min(3, target.shape[0])):\n",
        "              decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "              plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "              plt.show()\n",
        "              print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "        c_loss /= freq \n",
        "        print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "        c_loss = 0\n",
        "      batch_idx += 1  \n",
        "  print(\"Test loss: %f\" % (test_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dvEYjtLYlcN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator = HTRDiscriminator(batch_size, 256*2, 512, 10, len(train_set.codes)).to(device)      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zel5bfutWKKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_discriminator(s, target, use_teacher_forcing, train_mode, discriminator_target):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  train_mask = torch.ByteTensor(batch_size).to(device)\n",
        "  train_mask[:] = 1\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)    \n",
        "  discriminator_loss = 0\n",
        "  discriminator_hidden = discriminator.makeHidden().to(device)\n",
        "\n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    decoder_outputs = dec.topk(1, dim=1)[1].flatten()\n",
        "\n",
        "    \n",
        "    if train_mode:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden.detach(), decoder_result.input_hidden.detach(),decoder_result.rnn_input.detach(), decoder_outputs.detach(), discriminator_target)\n",
        "    else:\n",
        "      dl, discriminator_hidden = discriminator.apply(discriminator_hidden, decoder_result.input_hidden, decoder_result.rnn_input, decoder_outputs, discriminator_target)\n",
        "      \n",
        "    if i != 0:\n",
        "      discriminator_loss += dl\n",
        "\n",
        "    recognition_result[:, i] = decoder_outputs.detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]    \n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        " \n",
        "  discriminator_loss /= target.shape[1]\n",
        "  return (recognition_result, loss, discriminator_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yrtvy2ikWNwI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def apply_decoder(s, target, use_teacher_forcing):\n",
        "  loss = 0\n",
        "  START = train_set.start_code\n",
        "  STOP = train_set.stop_code\n",
        "  recognition_result = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  recognition_result.fill_(START)\n",
        "  old_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "  stop_symbol.fill_(STOP)\n",
        "  old_symbol[:, 0] = START\n",
        "\n",
        "  hidden = decoder.makeHidden().to(device)  \n",
        "  \n",
        "  for i in range(0, target.shape[1] + 1):\n",
        "\n",
        "    decoder_result = decoder(s, old_symbol, hidden)\n",
        "    dec = decoder_result.x\n",
        "    hidden = decoder_result.hidden\n",
        "\n",
        "    recognition_result[:, i] = dec.topk(1, dim=1)[1].flatten().detach()\n",
        "    \n",
        "    if i == target.shape[1]:\n",
        "      target_symbol = stop_symbol[:, 0]\n",
        "    else:\n",
        "      target_symbol = target[:, i]\n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "      old_symbol[:, 0] = target_symbol\n",
        "    else:\n",
        "      old_symbol[:, 0] = recognition_result[:, i]\n",
        "    #import pdb; pdb.set_trace()\n",
        "\n",
        "    loss += criterion(dec, target_symbol)\n",
        "  return (recognition_result, loss)\n",
        "\n",
        "\n",
        "batch_zeros = torch.zeros(batch_size, 1).to(device)\n",
        "batch_ones = torch.ones(batch_size, 1).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DxYdkXeTnNB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "teacher_forcing_ratio = 1#0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "def train(epoch, max_size):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 30\n",
        "  \n",
        "  train_set.to_start(max_size, equalize_freq=True)\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "\n",
        "  \n",
        "  train_loss = 0\n",
        "  discr_applied = 0\n",
        "  \n",
        "  while True:\n",
        "    if batch_idx > 400:\n",
        "      break\n",
        "    if True:\n",
        "      l = 1 + floor((max_size - 1)*random())\n",
        "      data, target = artificial_data.make_random_batch(l, 200)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "    else:\n",
        "      batch = train_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "      orig_data, target = batch\n",
        "      data = orig_data/255.0\n",
        "      #data = data.view(batch_size, 1, image_width, image_height).to(device)\n",
        "      data = data.unsqueeze(1).to(device)\n",
        "      target = target.to(device)\n",
        "\n",
        "\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "\n",
        "    \n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    s = enc\n",
        "    #s = enc.permute(1, 0, 2)\n",
        "    #s = s.squeeze(0)\n",
        "    #print(s.shape)\n",
        "    #print(s.shape)\n",
        "    #s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    #s = s.flatten(start_dim=1).squeeze(0)\n",
        "    s = s.unsqueeze(0)\n",
        "    #print(s.shape)\n",
        "    \n",
        "    discr_loss = 1\n",
        "    if True:#target.shape[1] > 1:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      _, _, tf_loss = apply_discriminator(s, target, True, True, batch_ones)\n",
        "      _, _, fr_loss = apply_discriminator(s, target, False, True, batch_zeros)\n",
        "      dl = tf_loss + fr_loss\n",
        "      discr_loss = dl.item()\n",
        "      if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        print(\"Discr loss: %f\" %(dl.item()))\n",
        "      #print(dl)\n",
        "      dl.backward()\n",
        "      discriminator_optimizer.step()\n",
        "   \n",
        "\n",
        "      #     use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "      # #    recognition_result, loss, discriminator_loss = apply_discriminator(s, target, use_teacher_forcing, True, batch_zeros)\n",
        "      #     recognition_result, loss = apply_decoder(s, target, use_teacher_forcing)\n",
        "    batch_loss = None  \n",
        "    if discr_loss < 0.3:\n",
        "      discriminator_optimizer.zero_grad()\n",
        "      recognition_result, loss, discriminator_loss = apply_discriminator(s, target, True, False, batch_zeros)\n",
        "      batch_loss = loss.item()\n",
        "      loss = loss + discriminator_loss\n",
        "      discr_applied += 1\n",
        "      #print(\"Apply descr...\")\n",
        "    else:\n",
        "      recognition_result, loss = apply_decoder(s, target, True)\n",
        "      batch_loss = loss.item()\n",
        "\n",
        "      \n",
        "      \n",
        "    c_loss += batch_loss/(target.shape[1] + 0)\n",
        "    train_loss += batch_loss/(target.shape[1] + 0)\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      \n",
        "      if False:#True:#not use_teacher_forcing:\n",
        "        for k in range(0, min(3, target.shape[0])):\n",
        "            decoded = recognition_result[k,0:target.shape[1] + 1]\n",
        "            plt.imshow(orig_data[k].cpu(), cmap=\"gray\")\n",
        "            plt.show()\n",
        "            print(\"  '\" + train_set.decode_word(target[k,:]) + \"' -> '\" + train_set.decode_word(decoded) + \"'\")\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    #grad_clip = 0.1\n",
        "    #torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    #torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "  print(\"Train loss: %f\"%(train_loss/batch_idx))\n",
        "  print(\"Discr applied %d times.\"%discr_applied)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1xrGl2QIsP13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = \"/gdrive/My Drive/v9.tar\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3iWRuawx-Dx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state = torch.load(file_name)\n",
        "# encoder.load_state_dict(state[\"encoder\"])\n",
        "# encoder_optimizer.load_state_dict(state[\"encoder_optimizer\"])\n",
        "# decoder.load_state_dict(state[\"decoder\"])\n",
        "# decoder_optimizer.load_state_dict(state[\"decoder_optimizer\"])\n",
        "# discriminator.load_state_dict(state[\"discriminator\"])\n",
        "# discriminator_optimizer.load_state_dict(state[\"discriminator_optimizer\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJBi4bwwywG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2810
        },
        "outputId": "791b8788-255c-475e-eef4-6c5f477fb2ec"
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 100):\n",
        "  max_size = 2\n",
        "  train(i, max_size)\n",
        "  test(max_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "Discr loss: 0.871778\n",
            "  Batch: 30 Loss: 8.393046347300212\n",
            "Discr loss: 0.009911\n",
            "  Batch: 60 Loss: 5.394910558064779\n",
            "Discr loss: 0.001518\n",
            "  Batch: 90 Loss: 4.259238751729329\n",
            "Discr loss: 0.000983\n",
            "  Batch: 120 Loss: 4.023917865753174\n",
            "Discr loss: 0.000743\n",
            "  Batch: 150 Loss: 3.86503488222758\n",
            "Discr loss: 0.439839\n",
            "  Batch: 180 Loss: 3.73380233446757\n",
            "Discr loss: 0.512579\n",
            "  Batch: 210 Loss: 3.4929624716440837\n",
            "Discr loss: 1.216908\n",
            "  Batch: 240 Loss: 3.410261352856954\n",
            "Discr loss: 1.262413\n",
            "  Batch: 270 Loss: 3.366743000348409\n",
            "Discr loss: 1.214164\n",
            "  Batch: 300 Loss: 3.3583986282348635\n",
            "Discr loss: 0.929881\n",
            "  Batch: 330 Loss: 3.333993148803711\n",
            "Discr loss: 0.611858\n",
            "  Batch: 360 Loss: 3.334122363726298\n",
            "Discr loss: 0.248468\n",
            "  Batch: 390 Loss: 3.323134676615397\n",
            "Train loss: 4.070000\n",
            "Discr applied 132 times.\n",
            "Testing...\n",
            "Test loss: 3.337211\n",
            "Training epoch 1...\n",
            "Discr loss: 0.234111\n",
            "  Batch: 30 Loss: 3.750331711769104\n",
            "Discr loss: 0.241736\n",
            "  Batch: 60 Loss: 3.6247297128041587\n",
            "Discr loss: 0.170239\n",
            "  Batch: 90 Loss: 3.5472845951716105\n",
            "Discr loss: 0.154465\n",
            "  Batch: 120 Loss: 3.642402180035909\n",
            "Discr loss: 0.157927\n",
            "  Batch: 150 Loss: 3.7261908292770385\n",
            "Discr loss: 0.122873\n",
            "  Batch: 180 Loss: 3.687404759724935\n",
            "Discr loss: 0.121894\n",
            "  Batch: 210 Loss: 3.612400460243225\n",
            "Discr loss: 0.135068\n",
            "  Batch: 240 Loss: 3.5446218729019163\n",
            "Discr loss: 0.148144\n",
            "  Batch: 270 Loss: 3.5565651655197144\n",
            "Discr loss: 0.187001\n",
            "  Batch: 300 Loss: 3.464833203951518\n",
            "Discr loss: 0.248077\n",
            "  Batch: 330 Loss: 3.4332971572875977\n",
            "Discr loss: 0.136580\n",
            "  Batch: 360 Loss: 3.4243061621983846\n",
            "Discr loss: 0.174678\n",
            "  Batch: 390 Loss: 3.408330949147542\n",
            "Train loss: 3.558294\n",
            "Discr applied 353 times.\n",
            "Testing...\n",
            "Test loss: 3.396413\n",
            "Training epoch 2...\n",
            "Discr loss: 0.175313\n",
            "  Batch: 30 Loss: 3.5095022042592365\n",
            "Discr loss: 0.109084\n",
            "  Batch: 60 Loss: 3.390571610132853\n",
            "Discr loss: 0.110731\n",
            "  Batch: 90 Loss: 3.3578505992889403\n",
            "Discr loss: 0.105569\n",
            "  Batch: 120 Loss: 3.350925882657369\n",
            "Discr loss: 0.237085\n",
            "  Batch: 150 Loss: 3.3492280721664427\n",
            "Discr loss: 0.101803\n",
            "  Batch: 180 Loss: 3.350372401873271\n",
            "Discr loss: 1.212687\n",
            "  Batch: 210 Loss: 3.3471536159515383\n",
            "Discr loss: 0.977423\n",
            "  Batch: 240 Loss: 3.3389345328013103\n",
            "Discr loss: 0.269788\n",
            "  Batch: 270 Loss: 3.3334486881891885\n",
            "Discr loss: 0.227230\n",
            "  Batch: 300 Loss: 3.332613809903463\n",
            "Discr loss: 0.084086\n",
            "  Batch: 330 Loss: 3.3314090172449746\n",
            "Discr loss: 0.111970\n",
            "  Batch: 360 Loss: 3.3247961680094402\n",
            "Discr loss: 0.148234\n",
            "  Batch: 390 Loss: 3.3243275324503583\n",
            "Train loss: 3.347782\n",
            "Discr applied 323 times.\n",
            "Testing...\n",
            "Test loss: 3.334565\n",
            "Training epoch 3...\n",
            "Discr loss: 0.210053\n",
            "  Batch: 30 Loss: 3.441065231959025\n",
            "Discr loss: 0.178922\n",
            "  Batch: 60 Loss: 3.335732356707255\n",
            "Discr loss: 0.139533\n",
            "  Batch: 90 Loss: 3.343480602900187\n",
            "Discr loss: 0.167933\n",
            "  Batch: 120 Loss: 3.3358266353607178\n",
            "Discr loss: 0.175042\n",
            "  Batch: 150 Loss: 3.3371882915496824\n",
            "Discr loss: 0.137260\n",
            "  Batch: 180 Loss: 3.326260813077291\n",
            "Discr loss: 0.141075\n",
            "  Batch: 210 Loss: 3.3445189793904624\n",
            "Discr loss: 0.162932\n",
            "  Batch: 240 Loss: 3.328166953722636\n",
            "Discr loss: 0.247836\n",
            "  Batch: 270 Loss: 3.3306362628936768\n",
            "Discr loss: 0.246144\n",
            "  Batch: 300 Loss: 3.327634557088216\n",
            "Discr loss: 0.231483\n",
            "  Batch: 330 Loss: 3.3351107120513914\n",
            "Discr loss: 0.068320\n",
            "  Batch: 360 Loss: 3.339362112681071\n",
            "Discr loss: 0.170657\n",
            "  Batch: 390 Loss: 3.3322974681854247\n",
            "Train loss: 3.334385\n",
            "Discr applied 387 times.\n",
            "Testing...\n",
            "Test loss: 3.332767\n",
            "Training epoch 4...\n",
            "Discr loss: 0.279196\n",
            "  Batch: 30 Loss: 3.443120074272156\n",
            "Discr loss: 0.194577\n",
            "  Batch: 60 Loss: 3.3130440870920816\n",
            "Discr loss: 0.202047\n",
            "  Batch: 90 Loss: 3.3226584513982136\n",
            "Discr loss: 0.165305\n",
            "  Batch: 120 Loss: 3.3213390906651816\n",
            "Discr loss: 0.209950\n",
            "  Batch: 150 Loss: 3.3194628318150836\n",
            "Discr loss: 0.135868\n",
            "  Batch: 180 Loss: 3.3328236738840737\n",
            "Discr loss: 0.236326\n",
            "  Batch: 210 Loss: 3.3308539311091105\n",
            "Discr loss: 0.148674\n",
            "  Batch: 240 Loss: 3.3207488457361856\n",
            "Discr loss: 0.173526\n",
            "  Batch: 270 Loss: 3.3102256854375205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b121bc4025e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0c4c1fcac14f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, max_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mdiscriminator_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0mrecognition_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_zeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m       \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdiscr_applied\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HsfEczGzx_ph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# state_dict = {\n",
        "#         \"encoder\": encoder.state_dict(),\n",
        "#         \"encoder_optimizer\": encoder_optimizer.state_dict(),\n",
        "#         \"decoder\": decoder.state_dict(),\n",
        "#         \"decoder_optimizer\": decoder_optimizer.state_dict(),\n",
        "#         \"discriminator\": discriminator.state_dict(),\n",
        "#         \"discriminator_optimizer\": discriminator_optimizer.state_dict(),\n",
        "#       }\n",
        "\n",
        "# torch.save(state_dict, file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8IIhQBGsOoL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrN5kEctKjQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data, target = artificial_data.make_random_batch(3, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBYhXSxDKjtf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dCBCuthGKmW-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0, 30):\n",
        "  plt.imshow(data[i])\n",
        "  plt.show()\n",
        "  print(test_set.decode_word(target[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "obmiumeGK5E9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}