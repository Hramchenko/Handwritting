{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTR_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/HTR_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "60a29ba7-b596-4db9-b2a1-d56148b88cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "#device = torch.device(\"cuda:0\")\n",
        "device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "60d6e3eb-d0e9-4eb2-b32e-acef58007443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SiZq2SoAI3yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_dataset(dataset):\n",
        "  l = len(dataset.codes)\n",
        "  s = \"<START>\"\n",
        "  dataset.codes[s] = l\n",
        "  dataset.inv_codes[l] = s\n",
        "  return dataset\n",
        "\n",
        "train_set = modify_dataset(train_set)\n",
        "test_set = modify_dataset(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch()\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, 128, 400).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        #print(h.shape)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        #print(\"MP \" + str(h.shape))\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([0, 3, 1, 2])\n",
        "        #print(\"P \" + str(h.shape))\n",
        "        #h = h.flatten(start_dim=2)\n",
        "        #print(\"P[0] \" + str(h.shape))\n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=23, encoded_height=64, batchnorm=True, dropout=False):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 128\n",
        "        lstm_layers = 2\n",
        "        self.rnn = nn.LSTM(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=True)\n",
        "        self.embedding = nn.Embedding(ntoken, ntoken)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*2, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.concatenated = torch.FloatTensor(24, )\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x)\n",
        "        emb = self.embedding(prev)\n",
        "        emb = emb.permute([1, 0, 2])\n",
        "        #print(x.shape)\n",
        "        #print(emb.shape)\n",
        "        x = torch.cat([x, emb], dim=2)\n",
        "        #print(x.shape)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        #print(\"rnn\")\n",
        "        #print(x.shape)\n",
        "        #print(\"rnn \" + str(x.shape))\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        #print(\"x\" + str(x.shape))\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      h1 = torch.zeros(4, batch_size, self.lstm_size).to(device)\n",
        "      h2 = torch.zeros(4, batch_size, self.lstm_size).to(device)\n",
        "      return (h1, h2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coyZNSEbv6CS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = HTRDecoder(len(train_set.codes)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBGJwuf1NSpS",
        "colab_type": "code",
        "outputId": "070be0ac-50b7-4b09-f9b4-2daa1862977e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_set.codes)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "h7qaWKG_Nn6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#decoder.embedding(current_symbol).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2oPukk2K2HF",
        "colab_type": "code",
        "outputId": "c718d0ad-0a73-4e8d-8c88-05ae77bef144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "enc = encoder(data)\n",
        "print(\"enc\")\n",
        "print(enc.shape)\n",
        "#s = enc.contiguous().view(1, batch_size, -1)#[1, :, :].view(1, batch_size, -1)\n",
        "s = enc.permute(1, 0, 2)\n",
        "s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "print(\"xx\")\n",
        "print(s.shape)\n",
        "print(current_symbol.shape)\n",
        "hidden = decoder.makeHidden()\n",
        "dec, h = decoder(s, current_symbol)\n",
        "#print(dec.shape)\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enc\n",
            "torch.Size([23, 30, 64])\n",
            "xx\n",
            "torch.Size([1, 30, 1472])\n",
            "torch.Size([30, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MRcrPoEJgEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0805820-dcba-4af9-faaf-e0cde287f879"
      },
      "cell_type": "code",
      "source": [
        "print(dec.shape)\n",
        "ss = torch.multinomial(dec.exp(), 1)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([30, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvgFEbZkhbRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "dc5ee8c2-7f2b-4443-bd19-67305feb4385"
      },
      "cell_type": "code",
      "source": [
        "ss"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14],\n",
              "        [65],\n",
              "        [80],\n",
              "        [58],\n",
              "        [79],\n",
              "        [67],\n",
              "        [23],\n",
              "        [59],\n",
              "        [30],\n",
              "        [ 7],\n",
              "        [74],\n",
              "        [64],\n",
              "        [73],\n",
              "        [ 2],\n",
              "        [39],\n",
              "        [36],\n",
              "        [ 6],\n",
              "        [69],\n",
              "        [54],\n",
              "        [79],\n",
              "        [66],\n",
              "        [13],\n",
              "        [28],\n",
              "        [ 3],\n",
              "        [50],\n",
              "        [38],\n",
              "        [13],\n",
              "        [ 9],\n",
              "        [ 6],\n",
              "        [ 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "O0Vw2v96yhqM",
        "colab_type": "code",
        "outputId": "44a76d2c-41e0-40e9-f462-3c29550f4a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "hidden.shape\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-a83816c89964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "StyCrcug1ya9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "usrU0Lduz3FY",
        "colab_type": "code",
        "outputId": "18506f57-04ba-43f7-cea0-d552d8cd22c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h[0].shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 30, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "_oqTyldn3a6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "outputId": "006d7f55-4da9-4add-e06d-ef11abfd4ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2904
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.codes['<START>']\n",
        "  current_symbol = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #s = enc.contiguous().view(1, batch_size, -1)\n",
        "    \n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "    \n",
        "    #print(\"---------\")\n",
        "    #print(s.shape)\n",
        "    current_symbol[:, 0] = START\n",
        "    #print(current_symbol[:, 0].shape)\n",
        "    for i in range(0, target.shape[1]):\n",
        "      #enc_i = enc[i, :, :].view(1, batch_size, -1)\n",
        "      symb = current_symbol[:, i].view(batch_size, 1).contiguous()\n",
        "      dec, hidden = decoder(s, symb, hidden)\n",
        "      #print(\"dec \" + str(dec.shape))\n",
        "      sampled = torch.multinomial(dec.exp(), 1)\n",
        "      current_symbol[:, i+1] = sampled.squeeze()\n",
        "      o = dec.view(30, 1, 81).flatten(start_dim=0,end_dim=1)\n",
        "#      o = dec.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "      t = target[:, i].flatten()\n",
        "      \n",
        "      \n",
        "      #print(o.shape)\n",
        "      #print(t.shape)\n",
        "      loss += criterion(o, t)\n",
        "    \n",
        "    #output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    #o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    #t = target.flatten()\n",
        "\n",
        "    #loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 1\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "  1 239.6279754638672\n",
            "  2 117.57829284667969\n",
            "  3 116.05209350585938\n",
            "  4 114.99407958984375\n",
            "  5 112.21305084228516\n",
            "  6 110.7999267578125\n",
            "  7 109.36397552490234\n",
            "  8 108.42613983154297\n",
            "  9 107.76636505126953\n",
            "  10 103.55252838134766\n",
            "  11 102.61219787597656\n",
            "  12 99.52122497558594\n",
            "  13 98.07105255126953\n",
            "  14 95.46385955810547\n",
            "  15 92.95638275146484\n",
            "  16 90.49456024169922\n",
            "  17 90.5299072265625\n",
            "  18 86.03430938720703\n",
            "  19 83.0023193359375\n",
            "  20 81.03985595703125\n",
            "  21 77.54261779785156\n",
            "  22 75.4160385131836\n",
            "  23 74.32049560546875\n",
            "  24 70.83709716796875\n",
            "  25 66.58506774902344\n",
            "  26 62.502960205078125\n",
            "  27 62.600425720214844\n",
            "  28 61.00706100463867\n",
            "  29 56.83729934692383\n",
            "  30 52.731632232666016\n",
            "  31 50.34827423095703\n",
            "  32 48.70368194580078\n",
            "  33 46.856773376464844\n",
            "  34 44.66219711303711\n",
            "  35 43.779457092285156\n",
            "  36 40.0951042175293\n",
            "  37 38.531044006347656\n",
            "  38 38.55298614501953\n",
            "  39 35.166114807128906\n",
            "  40 36.9328498840332\n",
            "  41 31.358367919921875\n",
            "  42 31.54657745361328\n",
            "  43 27.664350509643555\n",
            "  44 32.40379333496094\n",
            "  45 27.989837646484375\n",
            "  46 24.84962272644043\n",
            "  47 31.733320236206055\n",
            "  48 26.123552322387695\n",
            "  49 24.39487648010254\n",
            "  50 26.58205223083496\n",
            "  51 28.526260375976562\n",
            "  52 25.123720169067383\n",
            "  53 23.255971908569336\n",
            "  54 24.83268165588379\n",
            "  55 25.02787971496582\n",
            "  56 24.189817428588867\n",
            "  57 25.63820457458496\n",
            "  58 26.693511962890625\n",
            "  59 28.099079132080078\n",
            "  60 20.532976150512695\n",
            "  61 25.893281936645508\n",
            "  62 21.040245056152344\n",
            "  63 24.90978240966797\n",
            "  64 21.79682731628418\n",
            "  65 24.737171173095703\n",
            "  66 17.938232421875\n",
            "  67 21.554624557495117\n",
            "  68 22.0637264251709\n",
            "  69 20.884540557861328\n",
            "  70 22.150421142578125\n",
            "  71 18.817501068115234\n",
            "  72 21.974111557006836\n",
            "  73 20.7016544342041\n",
            "  74 20.714458465576172\n",
            "  75 18.30423927307129\n",
            "  76 23.628625869750977\n",
            "  77 21.66744041442871\n",
            "  78 15.073492050170898\n",
            "  79 20.730836868286133\n",
            "  80 17.459348678588867\n",
            "  81 17.79746437072754\n",
            "  82 18.52531623840332\n",
            "  83 17.77739906311035\n",
            "  84 20.789382934570312\n",
            "  85 20.983417510986328\n",
            "  86 19.226728439331055\n",
            "  87 16.850982666015625\n",
            "  88 22.582149505615234\n",
            "  89 18.14610481262207\n",
            "  90 17.352943420410156\n",
            "  91 17.08843994140625\n",
            "  92 15.98934555053711\n",
            "  93 17.30864906311035\n",
            "  94 20.278413772583008\n",
            "  95 18.717092514038086\n",
            "  96 16.20870018005371\n",
            "  97 17.107933044433594\n",
            "  98 17.18909454345703\n",
            "  99 17.705036163330078\n",
            "  100 15.940326690673828\n",
            "  101 15.478474617004395\n",
            "  102 16.226789474487305\n",
            "  103 18.72345542907715\n",
            "  104 20.68709945678711\n",
            "  105 20.712648391723633\n",
            "  106 17.051780700683594\n",
            "  107 20.91002655029297\n",
            "  108 18.479406356811523\n",
            "  109 19.557979583740234\n",
            "  110 17.039813995361328\n",
            "  111 17.838516235351562\n",
            "  112 23.061840057373047\n",
            "  113 16.634872436523438\n",
            "  114 21.345043182373047\n",
            "  115 18.681861877441406\n",
            "  116 15.594903945922852\n",
            "  117 16.092580795288086\n",
            "  118 16.70404624938965\n",
            "  119 21.72117042541504\n",
            "  120 12.959458351135254\n",
            "  121 19.68544578552246\n",
            "  122 16.38364601135254\n",
            "  123 20.642580032348633\n",
            "  124 16.794645309448242\n",
            "  125 16.879928588867188\n",
            "  126 17.78995132446289\n",
            "  127 16.614225387573242\n",
            "  128 22.145599365234375\n",
            "  129 15.188419342041016\n",
            "  130 14.557662963867188\n",
            "  131 15.254518508911133\n",
            "  132 16.03809928894043\n",
            "  133 15.341874122619629\n",
            "  134 17.138994216918945\n",
            "  135 19.453210830688477\n",
            "  136 16.193464279174805\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-35cb806368c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-35cb806368c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"  \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mgrad_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "yDcJcBeTRx8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ntoken, latent_size=64, batchnorm=True, dropout=False):\n",
        "        super(Net, self).__init__()\n",
        "    \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "        \n",
        "        \n",
        "\n",
        "        lstm_size = 128\n",
        "        lstm_layers = 2\n",
        "        # (batch_size, timesteps, input_dim)\n",
        "        self.rnn = nn.LSTM(latent_size, lstm_size, lstm_layers, dropout=0.3, bidirectional=True)\n",
        "        self.decoder = nn.Linear(23*lstm_size*2, ntoken*30)#nn.Linear(lstm_size, ntoken)\n",
        "#        self.fcd = FullyConnected([latent_size, 32*2*2])\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "#         nlayers = 2\n",
        "        \n",
        "#         self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=0.3)\n",
        "#         self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "#         self.init_weights()\n",
        "\n",
        "#         self.nhid = nhid\n",
        "#         self.nlayers = nlayers\n",
        "\n",
        "        \n",
        "#     def forward(self, x, hidden=None):\n",
        "#         emb = self.drop(self.encoder(x))\n",
        "#         output, hidden = self.rnn(emb, hidden)\n",
        "#         output = self.drop(output)\n",
        "#         decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "#         return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "                \n",
        "\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        #print(h.shape)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        #print(\"MP \" + str(h.shape))\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([0, 3, 1, 2])\n",
        "        #print(\"P \" + str(h.shape))\n",
        "        #h = h.flatten(start_dim=2)\n",
        "        #print(\"P[0] \" + str(h.shape))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "      \n",
        "    def decode(self, h):\n",
        "        x = self.drop(h)\n",
        "        x, hidden = self.rnn(x)\n",
        "        #print(\"rnn \" + str(x.shape))\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        #print(\"x\" + str(x.shape))\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "#     def decode(self, h):\n",
        "#         flatten = self.fcd(h)\n",
        "        \n",
        "#         output, hidden = self.rnn(flatten, hidden)\n",
        "#         output = self.drop(output)\n",
        "#         decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "#         x = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
        "#         return x, hidden\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encode(x)\n",
        "        #print(h.shape)\n",
        "        r = self.decode(h)\n",
        "        return r\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBFrsrjANC7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = Net(len(train_set.alphabet)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsOu6ZgQNYj-",
        "colab_type": "code",
        "outputId": "c1d550c6-f784-4d68-d6f6-48a029aa45b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1340
        }
      },
      "cell_type": "code",
      "source": [
        "output=net.forward(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50644f010039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e033a1a897b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m#print(h.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e033a1a897b9>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m#print(h.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f98529e74821>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Or9Tpl0fynpp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBfzTcnIz009",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1P9QaB3z6Ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "o.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dYoiQiRn0LyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OFliCq40X2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = target.flatten()\n",
        "t.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pk0LIx2r2Q6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "2400/900\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oh3-QRAh0kQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def to_onehot(x, n, device=None):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = torch.Tensor(x).to(torch.long)\n",
        "    one_hot = torch.zeros((x.shape[0], n))\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cn5_QB71iL3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "  with torch.no_grad():\n",
        "    test_set.to_start()\n",
        "    batch_idx = 0\n",
        "    c_loss = 0\n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "      data, target = batch\n",
        "      data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = net.forward(data)\n",
        "      o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "      t = target.flatten()\n",
        "      loss = criterion(o, t)\n",
        "      c_loss += loss.item()\n",
        "      batch_idx += 1\n",
        "    print(\"  Test loss: \" + str(c_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6vrxSFnHohp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7e9ZVSt7Knz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def forcing_train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    net.zero_grad()\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "    output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    t = target.flatten()\n",
        "\n",
        "    loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 200\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n",
        "  test(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7-0G8mgyLpp",
        "colab_type": "code",
        "outputId": "896fba65-0340-4dc9-e74b-641f6fc4b54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    net.zero_grad()\n",
        "    output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    t = target.flatten()\n",
        "\n",
        "    loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 200\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n",
        "  test(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "  200 0.40906761199235914\n",
            "  400 0.39776449725031854\n",
            "  600 0.3906879594922066\n",
            "  800 0.3917499200254679\n",
            "  1000 0.38672335997223856\n",
            "  1200 0.38541785448789595\n",
            "  1400 0.3825364875793457\n",
            "  1600 0.37158095367252825\n",
            "  1800 0.3702904235571623\n",
            "  2000 0.36803763136267664\n",
            "  2200 0.36549833960831163\n",
            "  2400 0.36047813162207604\n",
            "  Test loss: 0.36128279687459597\n",
            "Training epoch 1...\n",
            "  200 0.35956719525158404\n",
            "  400 0.3510160192847252\n",
            "  600 0.34734406001865864\n",
            "  800 0.35050836570560934\n",
            "  1000 0.34704456470906736\n",
            "  1200 0.34642483055591583\n",
            "  1400 0.34863717637956143\n",
            "  1600 0.3386073859035969\n",
            "  1800 0.3388120226562023\n",
            "  2000 0.33854374147951605\n",
            "  2200 0.33705330215394497\n",
            "  2400 0.3338170325756073\n",
            "  Test loss: 0.3369936909631034\n",
            "Training epoch 2...\n",
            "  200 0.3353282096982002\n",
            "  400 0.3285587485134602\n",
            "  600 0.32515278153121474\n",
            "  800 0.3304219239205122\n",
            "  1000 0.3270692164450884\n",
            "  1200 0.3259670868515968\n",
            "  1400 0.3295812138915062\n",
            "  1600 0.3206078252196312\n",
            "  1800 0.32301347233355043\n",
            "  2000 0.32078570030629633\n",
            "  2200 0.32101370878517627\n",
            "  2400 0.31618963681161405\n",
            "  Test loss: 0.324501984123129\n",
            "Training epoch 3...\n",
            "  200 0.31979030393064023\n",
            "  400 0.313747259452939\n",
            "  600 0.31113329611718654\n",
            "  800 0.3151177726686001\n",
            "  1000 0.3131934582442045\n",
            "  1200 0.31273031860589984\n",
            "  1400 0.31652368485927584\n",
            "  1600 0.3067044451087713\n",
            "  1800 0.30865790508687496\n",
            "  2000 0.3078420922905207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iZRxn3i2Mv7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(HTRModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        \n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2djeG_URgCl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XNiu6d5RReX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    for batch, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9o4Fc8PRWyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    for i, (data, targets) in enumerate(data_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        output, hidden = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkTmZg8TQ7EW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}