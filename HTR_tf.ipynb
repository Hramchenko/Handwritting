{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTR_tf.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/HTR_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "60a29ba7-b596-4db9-b2a1-d56148b88cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "#device = torch.device(\"cuda:0\")\n",
        "device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "60d6e3eb-d0e9-4eb2-b32e-acef58007443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SiZq2SoAI3yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_dataset(dataset):\n",
        "  l = len(dataset.codes)\n",
        "  s = \"<START>\"\n",
        "  dataset.codes[s] = l\n",
        "  dataset.inv_codes[l] = s\n",
        "  return dataset\n",
        "\n",
        "train_set = modify_dataset(train_set)\n",
        "test_set = modify_dataset(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch()\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, 128, 400).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        #print(h.shape)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        #print(\"MP \" + str(h.shape))\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([0, 3, 1, 2])\n",
        "        #print(\"P \" + str(h.shape))\n",
        "        #h = h.flatten(start_dim=2)\n",
        "        #print(\"P[0] \" + str(h.shape))\n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=23, encoded_height=64, batchnorm=True, dropout=False):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 128\n",
        "        lstm_layers = 2\n",
        "        self.rnn = nn.LSTM(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=True)\n",
        "        self.embedding = nn.Embedding(ntoken, ntoken)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*2, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        self.concatenated = torch.FloatTensor(24, )\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        x = self.drop(x)\n",
        "        emb = self.embedding(prev)\n",
        "        emb = emb.permute([1, 0, 2])\n",
        "        #print(x.shape)\n",
        "        #print(emb.shape)\n",
        "        x = torch.cat([x, emb], dim=2)\n",
        "        #print(x.shape)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        #print(\"rnn\")\n",
        "        #print(x.shape)\n",
        "        #print(\"rnn \" + str(x.shape))\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        #print(\"x\" + str(x.shape))\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      h1 = torch.zeros(4, batch_size, self.lstm_size).to(device)\n",
        "      h2 = torch.zeros(4, batch_size, self.lstm_size).to(device)\n",
        "      return (h1, h2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coyZNSEbv6CS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = HTRDecoder(len(train_set.codes)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBGJwuf1NSpS",
        "colab_type": "code",
        "outputId": "f34ade75-7587-4f12-a184-d77a83b37aa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train_set.codes)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "h7qaWKG_Nn6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#decoder.embedding(current_symbol).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N2oPukk2K2HF",
        "colab_type": "code",
        "outputId": "0d3ee89c-a488-472b-99b6-7d26dad5b166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "enc = encoder(data)\n",
        "print(\"enc\")\n",
        "print(enc.shape)\n",
        "#s = enc.contiguous().view(1, batch_size, -1)#[1, :, :].view(1, batch_size, -1)\n",
        "s = enc.permute(1, 0, 2)\n",
        "s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "print(\"xx\")\n",
        "print(s.shape)\n",
        "print(current_symbol.shape)\n",
        "hidden = decoder.makeHidden()\n",
        "dec, h = decoder(s, current_symbol)\n",
        "#print(dec.shape)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enc\n",
            "torch.Size([23, 30, 64])\n",
            "xx\n",
            "torch.Size([1, 30, 1472])\n",
            "torch.Size([30, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MRcrPoEJgEH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d98e4e3d-6800-4944-b65e-38a485c421e4"
      },
      "cell_type": "code",
      "source": [
        "print(dec.shape)\n",
        "ss = torch.multinomial(dec.exp(), 1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([30, 81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvgFEbZkhbRC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a09286a2-4a74-48d0-9cf4-2b9f695c4a8b"
      },
      "cell_type": "code",
      "source": [
        "ss"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[27],\n",
              "        [15],\n",
              "        [65],\n",
              "        [16],\n",
              "        [66],\n",
              "        [26],\n",
              "        [12],\n",
              "        [49],\n",
              "        [15],\n",
              "        [20],\n",
              "        [31],\n",
              "        [52],\n",
              "        [69],\n",
              "        [13],\n",
              "        [73],\n",
              "        [72],\n",
              "        [10],\n",
              "        [ 6],\n",
              "        [46],\n",
              "        [24],\n",
              "        [66],\n",
              "        [80],\n",
              "        [33],\n",
              "        [14],\n",
              "        [ 1],\n",
              "        [33],\n",
              "        [79],\n",
              "        [52],\n",
              "        [71],\n",
              "        [22]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "metadata": {
        "id": "O0Vw2v96yhqM",
        "colab_type": "code",
        "outputId": "fe27c96b-3987-4981-c17d-d5844b8a0337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "hidden.shape\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-a83816c89964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "StyCrcug1ya9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "usrU0Lduz3FY",
        "colab_type": "code",
        "outputId": "a83f2718-6132-48af-e14e-50d6ac03d563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "h[0].shape"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 30, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "metadata": {
        "id": "_oqTyldn3a6W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "outputId": "2da3fbd0-0389-4440-cc37-559835d32033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2890
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.codes['<START>']\n",
        "  current_symbol = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #s = enc.contiguous().view(1, batch_size, -1)\n",
        "    \n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, 30, 1472)\n",
        "    \n",
        "    #print(\"---------\")\n",
        "    #print(s.shape)\n",
        "    current_symbol[:, 0] = START\n",
        "    #print(current_symbol[:, 0].shape)\n",
        "    for i in range(0, target.shape[1]):\n",
        "      use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "      #enc_i = enc[i, :, :].view(1, batch_size, -1)\n",
        "      symb = current_symbol[:, i].view(batch_size, 1).contiguous()\n",
        "      dec, hidden = decoder(s, symb, hidden)\n",
        "      #print(\"dec \" + str(dec.shape))\n",
        "      if use_teacher_forcing:\n",
        "        current_symbol[:, i + 1] = target[:, i]\n",
        "      else:\n",
        "        sampled = torch.multinomial(dec.exp(), 1)\n",
        "        current_symbol[:, i+1] = sampled.squeeze()\n",
        "      o = dec.view(30, 1, 81).flatten(start_dim=0,end_dim=1)\n",
        "#      o = dec.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "      t = target[:, i].flatten()\n",
        "      \n",
        "      \n",
        "      #print(o.shape)\n",
        "      #print(t.shape)\n",
        "      loss += criterion(o, t)\n",
        "    \n",
        "    #output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    #o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    #t = target.flatten()\n",
        "\n",
        "    #loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 1\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "  1 261.6237030029297\n",
            "  2 128.81109619140625\n",
            "  3 127.64968872070312\n",
            "  4 126.10823822021484\n",
            "  5 124.3232421875\n",
            "  6 123.29328155517578\n",
            "  7 121.56023406982422\n",
            "  8 119.98106384277344\n",
            "  9 119.13552856445312\n",
            "  10 116.22201538085938\n",
            "  11 115.78242492675781\n",
            "  12 114.02812194824219\n",
            "  13 112.35768127441406\n",
            "  14 109.95079040527344\n",
            "  15 107.57024383544922\n",
            "  16 106.1613540649414\n",
            "  17 105.139404296875\n",
            "  18 102.02766418457031\n",
            "  19 99.41085052490234\n",
            "  20 97.68424987792969\n",
            "  21 94.51673889160156\n",
            "  22 92.45153045654297\n",
            "  23 91.80305480957031\n",
            "  24 88.26911926269531\n",
            "  25 84.4974136352539\n",
            "  26 81.21769714355469\n",
            "  27 79.9267578125\n",
            "  28 78.05807495117188\n",
            "  29 75.10962677001953\n",
            "  30 70.5427017211914\n",
            "  31 68.47817993164062\n",
            "  32 65.32772827148438\n",
            "  33 62.71171569824219\n",
            "  34 60.50139236450195\n",
            "  35 59.11337661743164\n",
            "  36 54.28748321533203\n",
            "  37 52.80339431762695\n",
            "  38 51.23579025268555\n",
            "  39 47.7235221862793\n",
            "  40 46.4582633972168\n",
            "  41 41.618896484375\n",
            "  42 40.610252380371094\n",
            "  43 36.73063659667969\n",
            "  44 39.158164978027344\n",
            "  45 34.6715202331543\n",
            "  46 31.46328353881836\n",
            "  47 35.85594940185547\n",
            "  48 30.658246994018555\n",
            "  49 28.534969329833984\n",
            "  50 29.417722702026367\n",
            "  51 30.895931243896484\n",
            "  52 27.361276626586914\n",
            "  53 25.129859924316406\n",
            "  54 26.405418395996094\n",
            "  55 26.202016830444336\n",
            "  56 25.447357177734375\n",
            "  57 26.66588020324707\n",
            "  58 27.61517333984375\n",
            "  59 28.662260055541992\n",
            "  60 21.860260009765625\n",
            "  61 26.79310417175293\n",
            "  62 21.44070053100586\n",
            "  63 25.834373474121094\n",
            "  64 22.556352615356445\n",
            "  65 25.49022102355957\n",
            "  66 18.319438934326172\n",
            "  67 22.261451721191406\n",
            "  68 22.790285110473633\n",
            "  69 21.681137084960938\n",
            "  70 23.101736068725586\n",
            "  71 19.352354049682617\n",
            "  72 22.977670669555664\n",
            "  73 21.575666427612305\n",
            "  74 21.51380157470703\n",
            "  75 18.345537185668945\n",
            "  76 24.221445083618164\n",
            "  77 22.547103881835938\n",
            "  78 15.312511444091797\n",
            "  79 21.38792610168457\n",
            "  80 17.50975227355957\n",
            "  81 17.814693450927734\n",
            "  82 19.108871459960938\n",
            "  83 18.065765380859375\n",
            "  84 21.32324981689453\n",
            "  85 21.748804092407227\n",
            "  86 19.507917404174805\n",
            "  87 17.396940231323242\n",
            "  88 23.089962005615234\n",
            "  89 18.464374542236328\n",
            "  90 17.779918670654297\n",
            "  91 17.297313690185547\n",
            "  92 16.3027400970459\n",
            "  93 17.73013687133789\n",
            "  94 20.793249130249023\n",
            "  95 19.106338500976562\n",
            "  96 16.45879554748535\n",
            "  97 17.341585159301758\n",
            "  98 17.236955642700195\n",
            "  99 17.90210723876953\n",
            "  100 16.166635513305664\n",
            "  101 15.35585880279541\n",
            "  102 16.49528694152832\n",
            "  103 18.799257278442383\n",
            "  104 21.18580436706543\n",
            "  105 20.876996994018555\n",
            "  106 17.156005859375\n",
            "  107 21.23668098449707\n",
            "  108 18.498172760009766\n",
            "  109 19.561763763427734\n",
            "  110 17.20088005065918\n",
            "  111 17.67949867248535\n",
            "  112 23.454811096191406\n",
            "  113 17.04425811767578\n",
            "  114 21.744966506958008\n",
            "  115 18.574626922607422\n",
            "  116 16.022432327270508\n",
            "  117 16.11636734008789\n",
            "  118 16.903154373168945\n",
            "  119 21.33243751525879\n",
            "  120 12.809309959411621\n",
            "  121 19.559322357177734\n",
            "  122 16.45993995666504\n",
            "  123 20.205595016479492\n",
            "  124 16.66334342956543\n",
            "  125 16.824440002441406\n",
            "  126 17.877872467041016\n",
            "  127 16.44837760925293\n",
            "  128 21.816225051879883\n",
            "  129 15.01076889038086\n",
            "  130 14.429069519042969\n",
            "  131 15.439972877502441\n",
            "  132 16.10341453552246\n",
            "  133 15.12634563446045\n",
            "  134 16.687236785888672\n",
            "  135 19.716129302978516\n",
            "  136 16.565505981445312\n",
            "  137 13.79288387298584\n",
            "  138 15.775495529174805\n",
            "  139 20.133869171142578\n",
            "  140 15.028437614440918\n",
            "  141 18.01799964904785\n",
            "  142 14.293819427490234\n",
            "  143 14.791083335876465\n",
            "  144 13.863728523254395\n",
            "  145 17.53429412841797\n",
            "  146 20.078964233398438\n",
            "  147 17.464187622070312\n",
            "  148 13.354862213134766\n",
            "  149 15.256434440612793\n",
            "  150 15.658294677734375\n",
            "  151 17.21567153930664\n",
            "  152 17.563671112060547\n",
            "  153 17.350671768188477\n",
            "  154 14.184469223022461\n",
            "  155 17.215335845947266\n",
            "  156 15.214688301086426\n",
            "  157 16.95738410949707\n",
            "  158 14.803159713745117\n",
            "  159 16.40519142150879\n",
            "  160 14.084015846252441\n",
            "  161 15.548315048217773\n",
            "  162 18.620849609375\n",
            "  163 18.3547306060791\n",
            "  164 16.042652130126953\n",
            "  165 15.507144927978516\n",
            "  166 16.535499572753906\n",
            "  167 13.93606185913086\n",
            "  168 16.222375869750977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDcJcBeTRx8u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, ntoken, latent_size=64, batchnorm=True, dropout=False):\n",
        "        super(Net, self).__init__()\n",
        "    \n",
        "        self.latent_size = latent_size\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([50, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "        \n",
        "        \n",
        "\n",
        "        lstm_size = 128\n",
        "        lstm_layers = 2\n",
        "        # (batch_size, timesteps, input_dim)\n",
        "        self.rnn = nn.LSTM(latent_size, lstm_size, lstm_layers, dropout=0.3, bidirectional=True)\n",
        "        self.decoder = nn.Linear(23*lstm_size*2, ntoken*30)#nn.Linear(lstm_size, ntoken)\n",
        "#        self.fcd = FullyConnected([latent_size, 32*2*2])\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "#         nlayers = 2\n",
        "        \n",
        "#         self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=0.3)\n",
        "#         self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "#         self.init_weights()\n",
        "\n",
        "#         self.nhid = nhid\n",
        "#         self.nlayers = nlayers\n",
        "\n",
        "        \n",
        "#     def forward(self, x, hidden=None):\n",
        "#         emb = self.drop(self.encoder(x))\n",
        "#         output, hidden = self.rnn(emb, hidden)\n",
        "#         output = self.drop(output)\n",
        "#         decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "#         return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                weight.new(self.nlayers, bsz, self.nhid).zero_())\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "                \n",
        "\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        #print(h.shape)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        #print(\"MP \" + str(h.shape))\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        #h = h.permute([0, 3, 1, 2])\n",
        "        #print(\"P \" + str(h.shape))\n",
        "        #h = h.flatten(start_dim=2)\n",
        "        #print(\"P[0] \" + str(h.shape))\n",
        "        \n",
        "        \n",
        "        return h\n",
        "      \n",
        "    def decode(self, h):\n",
        "        x = self.drop(h)\n",
        "        x, hidden = self.rnn(x)\n",
        "        #print(\"rnn \" + str(x.shape))\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        #print(\"x\" + str(x.shape))\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "    \n",
        "#     def decode(self, h):\n",
        "#         flatten = self.fcd(h)\n",
        "        \n",
        "#         output, hidden = self.rnn(flatten, hidden)\n",
        "#         output = self.drop(output)\n",
        "#         decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "#         x = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
        "#         return x, hidden\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.encode(x)\n",
        "        #print(h.shape)\n",
        "        r = self.decode(h)\n",
        "        return r\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBFrsrjANC7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net = Net(len(train_set.alphabet)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsOu6ZgQNYj-",
        "colab_type": "code",
        "outputId": "c1d550c6-f784-4d68-d6f6-48a029aa45b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1340
        }
      },
      "cell_type": "code",
      "source": [
        "output=net.forward(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50644f010039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e033a1a897b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;31m#print(h.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e033a1a897b9>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;31m#print(h.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-f98529e74821>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Or9Tpl0fynpp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SBfzTcnIz009",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1P9QaB3z6Ss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "o.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dYoiQiRn0LyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OFliCq40X2W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "t = target.flatten()\n",
        "t.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pk0LIx2r2Q6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "2400/900\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oh3-QRAh0kQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def to_onehot(x, n, device=None):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = torch.Tensor(x).to(torch.long)\n",
        "    one_hot = torch.zeros((x.shape[0], n))\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cn5_QB71iL3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "  with torch.no_grad():\n",
        "    test_set.to_start()\n",
        "    batch_idx = 0\n",
        "    c_loss = 0\n",
        "    while True:\n",
        "      batch = test_set.make_batch()\n",
        "      if batch is None:\n",
        "        break\n",
        "      data, target = batch\n",
        "      data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = net.forward(data)\n",
        "      o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "      t = target.flatten()\n",
        "      loss = criterion(o, t)\n",
        "      c_loss += loss.item()\n",
        "      batch_idx += 1\n",
        "    print(\"  Test loss: \" + str(c_loss/batch_idx))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6vrxSFnHohp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7e9ZVSt7Knz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def forcing_train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    net.zero_grad()\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "    output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    t = target.flatten()\n",
        "\n",
        "    loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 200\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n",
        "  test(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H7-0G8mgyLpp",
        "colab_type": "code",
        "outputId": "896fba65-0340-4dc9-e74b-641f6fc4b54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    net.zero_grad()\n",
        "    output = net.forward(data)\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    o = output.view(30, -1, 80).flatten(start_dim=0,end_dim=1)\n",
        "    t = target.flatten()\n",
        "\n",
        "    loss = criterion(o, t)\n",
        "    c_loss += loss.item()\n",
        "    freq = 200\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      c_loss /= freq \n",
        "      print(\"  \" + str(batch_idx) + \" \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(net.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n",
        "  test(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "  200 0.40906761199235914\n",
            "  400 0.39776449725031854\n",
            "  600 0.3906879594922066\n",
            "  800 0.3917499200254679\n",
            "  1000 0.38672335997223856\n",
            "  1200 0.38541785448789595\n",
            "  1400 0.3825364875793457\n",
            "  1600 0.37158095367252825\n",
            "  1800 0.3702904235571623\n",
            "  2000 0.36803763136267664\n",
            "  2200 0.36549833960831163\n",
            "  2400 0.36047813162207604\n",
            "  Test loss: 0.36128279687459597\n",
            "Training epoch 1...\n",
            "  200 0.35956719525158404\n",
            "  400 0.3510160192847252\n",
            "  600 0.34734406001865864\n",
            "  800 0.35050836570560934\n",
            "  1000 0.34704456470906736\n",
            "  1200 0.34642483055591583\n",
            "  1400 0.34863717637956143\n",
            "  1600 0.3386073859035969\n",
            "  1800 0.3388120226562023\n",
            "  2000 0.33854374147951605\n",
            "  2200 0.33705330215394497\n",
            "  2400 0.3338170325756073\n",
            "  Test loss: 0.3369936909631034\n",
            "Training epoch 2...\n",
            "  200 0.3353282096982002\n",
            "  400 0.3285587485134602\n",
            "  600 0.32515278153121474\n",
            "  800 0.3304219239205122\n",
            "  1000 0.3270692164450884\n",
            "  1200 0.3259670868515968\n",
            "  1400 0.3295812138915062\n",
            "  1600 0.3206078252196312\n",
            "  1800 0.32301347233355043\n",
            "  2000 0.32078570030629633\n",
            "  2200 0.32101370878517627\n",
            "  2400 0.31618963681161405\n",
            "  Test loss: 0.324501984123129\n",
            "Training epoch 3...\n",
            "  200 0.31979030393064023\n",
            "  400 0.313747259452939\n",
            "  600 0.31113329611718654\n",
            "  800 0.3151177726686001\n",
            "  1000 0.3131934582442045\n",
            "  1200 0.31273031860589984\n",
            "  1400 0.31652368485927584\n",
            "  1600 0.3067044451087713\n",
            "  1800 0.30865790508687496\n",
            "  2000 0.3078420922905207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iZRxn3i2Mv7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(HTRModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        \n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        emb = self.drop(self.encoder(x))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.nlayers, bsz, self.nhid).zero_(),\n",
        "                weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2djeG_URgCl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XNiu6d5RReX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    for batch, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_loader) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9o4Fc8PRWyf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    for i, (data, targets) in enumerate(data_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        output, hidden = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / len(data_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkTmZg8TQ7EW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}