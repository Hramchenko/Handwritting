{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HTR_tf_unif_att_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hramchenko/Handwritting/blob/master/HTR_tf_unif_att_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uL5QRz_WMkMF",
        "colab_type": "code",
        "outputId": "a58ff575-fdc0-4062-bd3d-b426e494acdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"Device \" + torch.cuda.get_device_name(0))\n",
        "device = torch.device(\"cuda:0\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Tesla K80\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5M_rV-VMqso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVeBVZEgMtb2",
        "colab_type": "code",
        "outputId": "151ab851-9c9a-4cb7-adaf-804182572218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./Handwritting/\")\n",
        "from IAMWords import IAMWords\n",
        "train_set = IAMWords(\"train\", \"./IAM/\", batch_size=batch_size)\n",
        "test_set = IAMWords(\"test\", \"./IAM/\", batch_size=batch_size)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading ./IAM/words.train.pkl...\n",
            "Reading finished\n",
            "Reading ./IAM/words.test.pkl...\n",
            "Reading finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SiZq2SoAI3yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def modify_dataset(dataset):\n",
        "  l = len(dataset.codes)\n",
        "  s = \"<START>\"\n",
        "  dataset.codes[s] = l\n",
        "  dataset.inv_codes[l] = s\n",
        "  return dataset\n",
        "\n",
        "train_set = modify_dataset(train_set)\n",
        "test_set = modify_dataset(test_set)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2fWx6tuK-4m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.core.debugger import set_trace\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jLNlm1yURr4W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
        "        if pool_layer is not None:\n",
        "            layers.append(pool_layer)\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XmHNGG3oRshP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DeconvLayer(nn.Module):\n",
        "    def __init__(self, size, padding=1, stride=1, \n",
        "                 bn=False, dropout=False, activation_fn=nn.ReLU(), output_padding=0):\n",
        "        super(DeconvLayer, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.ConvTranspose2d(size[0], size[1], size[2], padding=padding, \n",
        "                                         stride=stride, output_padding=output_padding))\n",
        "        if bn:\n",
        "            layers.append(nn.BatchNorm2d(size[1]))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout2d())\n",
        "        layers.append(activation_fn)\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hO1gydZeRvE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
        "        super(FullyConnected, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout())\n",
        "            layers.append(activation_fn())\n",
        "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        \n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QuAkNIOOQkar",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConnectedX(nn.Module):\n",
        "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, last_fn=None):\n",
        "        super(FullyConnectedX, self).__init__()\n",
        "        layers = []\n",
        "        self.flatten = flatten\n",
        "        for i in range(len(sizes) - 2):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
        "            if dropout:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
        "        else: \n",
        "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
        "        if last_fn is not None:\n",
        "            layers.append(last_fn)\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if self.flatten:\n",
        "            x = x.view(x.shape[0], -1)\n",
        "        return self.model(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tNCy7E6BNI1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = train_set.make_batch()\n",
        "data, target = batch\n",
        "target = target.to(device)\n",
        "data = data/255.0\n",
        "data = data.view(batch_size, 1, 128, 400).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9OQHd-uOoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTREncoder(nn.Module):\n",
        "    def __init__(self, batchnorm=True, dropout=False):\n",
        "        super(HTREncoder, self).__init__()\n",
        "        \n",
        "        self.convolutions = nn.Sequential(\n",
        "        ConvLayer([1, 16, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([16, 32, 3], padding=0, bn=batchnorm),\n",
        "        #ConvLayer([32, 50, 3], padding=0, bn=batchnorm),\n",
        "        ConvLayer([32, 64, 3], padding=0, stride=2, bn=batchnorm, pool_layer=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = self.convolutions(x)\n",
        "        h = F.max_pool2d(h, [h.size(2), 1], padding=[0, 0])\n",
        "        h = h.permute([2, 3, 0, 1])[0]\n",
        "        return h\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ihilbywpul9A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = HTREncoder().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXja4G8p7KKz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_onehot(x, n):\n",
        "    one_hot = torch.zeros((x.shape[0], n)).to(device)\n",
        "    one_hot.scatter_(1, x[:, None], 1.)\n",
        "    if device is not None:\n",
        "        one_hot = one_hot.to(device)\n",
        "    return one_hot  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fIiy-eFLvC5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HTRDecoder(nn.Module):\n",
        "    def __init__(self, ntoken, encoded_width=48, encoded_height=64, batchnorm=True, dropout=False, rnn_type=\"LSTM\"):\n",
        "        super(HTRDecoder, self).__init__()\n",
        "        self.ntoken = ntoken\n",
        "        self.encoded_width = encoded_width\n",
        "        self.encoded_height = encoded_height\n",
        "        self.lstm_size = 256\n",
        "        lstm_layers = 1\n",
        "        self.rnn_type = rnn_type\n",
        "        \n",
        "        if rnn_type == \"LSTM\":\n",
        "          self.rnn = nn.LSTM(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        else:\n",
        "          self.rnn = nn.GRU(self.encoded_height*encoded_width + ntoken, self.lstm_size, lstm_layers, dropout=0.3, bidirectional=False)\n",
        "        self.embedding = nn.Embedding(ntoken, ntoken)\n",
        "        self.decoder = nn.Linear(1*self.lstm_size*1, ntoken)#*batch_size)\n",
        "        self.drop = nn.Dropout(0.3)\n",
        "        \n",
        "        print(\"-----\")\n",
        "        print(self.lstm_size + self.ntoken)\n",
        "        print(self.encoded_height*self.encoded_width)\n",
        "        \n",
        "        self.attention = FullyConnectedX([self.lstm_size + self.ntoken, 1024, 2048, self.encoded_height*self.encoded_width], activation_fn=nn.ReLU(), last_fn=nn.Tanh())\n",
        "        #self.concatenated = torch.FloatTensor(24, )\n",
        "        self.attention_weights = None\n",
        "    \n",
        "    def forward(self, x, prev, hidden=None):\n",
        "        #set_trace()\n",
        "        x = self.drop(x)\n",
        "        emb = to_onehot(prev.squeeze(), self.ntoken)\n",
        "        emb = emb.unsqueeze(1)\n",
        "        emb = emb.permute([1, 0, 2])\n",
        "        if hidden is not None:\n",
        "          #print(x.shape)\n",
        "          #print(hidden.shape)\n",
        "          #print(emb.shape)\n",
        "          #print(hidden.shape)\n",
        "          attention_inp = torch.cat([emb, hidden], dim=2)\n",
        "          #print(attention_inp.shape)\n",
        "          #self.attention_weights = self.attention(attention_inp)\n",
        "          #print(\"iiiiiiiii\")\n",
        "          #print(self.attention_weights.shape)\n",
        "          #print(x.shape)\n",
        "          self.attention_weights = F.softmax(self.attention(attention_inp), dim=1)\n",
        "          #print(attention_w.shape)\n",
        "          #print(x.shape)\n",
        "          x = x * self.attention_weights\n",
        "          #print(\"********************\")\n",
        "          #print(x)\n",
        "          #print(attention_w)\n",
        "          #print(X)\n",
        "          #print(\"---------------\")\n",
        "        #emb = self.embedding(prev)\n",
        "\n",
        "        \n",
        "        #print(emb.shape)\n",
        "        \n",
        "        #print(x.shape)\n",
        "        #print(emb.shape)\n",
        "        x = torch.cat([x, emb], dim=2)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.drop(x)\n",
        "        x = self.decoder(x)\n",
        "        return x, hidden  \n",
        "      \n",
        "    def makeHidden(self):\n",
        "      if self.rnn_type == \"LSTM\":\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        h2 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return (h1, h2)\n",
        "      else:\n",
        "        h1 = torch.zeros(1, batch_size, self.lstm_size).to(device)\n",
        "        return h1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coyZNSEbv6CS",
        "colab_type": "code",
        "outputId": "a343faa5-c836-42d8-9c12-f6fd413534ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "decoder = HTRDecoder(len(train_set.codes), rnn_type=\"GRU\").to(device)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----\n",
            "337\n",
            "3072\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ziLheucQKlpE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "START = train_set.codes['<START>']\n",
        "current_symbol = torch.LongTensor(batch_size, 1).to(device)\n",
        "current_symbol[:, :] = START"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0GHhQNaOtyJA",
        "colab_type": "code",
        "outputId": "f7cba19f-3761-4295-b15e-984cf63811d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "23*64\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "JUtlRV5GxNpu",
        "colab_type": "code",
        "outputId": "000d8017-434e-4430-ac49-a1c8c653d1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6375
        }
      },
      "cell_type": "code",
      "source": [
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=0.00005)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "from random import random\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "  print(\"Training epoch \" + str(epoch) + \"...\")\n",
        "  \n",
        "  freq = 30\n",
        "  \n",
        "  train_set.to_start()\n",
        "  batch_idx = 0\n",
        "  c_loss = 0\n",
        "  START = train_set.codes['<START>']\n",
        "  current_symbol = torch.LongTensor(batch_size, 30+1).to(device)\n",
        "  while True:\n",
        "    batch = train_set.make_batch()\n",
        "    if batch is None:\n",
        "      break\n",
        "    encoder.zero_grad()\n",
        "    decoder.zero_grad()\n",
        "    \n",
        "    data, target = batch\n",
        "    data = data.view(batch_size, 1, 128, 400)/255.0\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    hidden = decoder.makeHidden()    \n",
        "\n",
        "    loss = 0\n",
        "    enc = encoder(data)\n",
        "    #print(enc.shape)\n",
        "    #s = enc.contiguous().view(1, batch_size, -1)\n",
        "   \n",
        "    s = enc.permute(1, 0, 2)\n",
        "    s = s.flatten(start_dim=1).view(1, batch_size, -1)\n",
        "    \n",
        "    current_symbol[:, 0] = START\n",
        "    use_teacher_forcing = True if random() < teacher_forcing_ratio else False\n",
        "    #print(\"*************\")\n",
        "    for i in range(0, target.shape[1]):\n",
        "      symb = current_symbol[:, i].view(batch_size, 1).contiguous()\n",
        "      dec, hidden = decoder(s, symb, hidden)\n",
        "      \n",
        "      if False:#(batch_idx % freq == 0) and (batch_idx != 0):\n",
        "        if i == 0:\n",
        "          print(\"------------\")\n",
        "        if i in [0, 3, 7]:\n",
        "          im = decoder.attention_weights[0,0,:].cpu().detach().numpy()\n",
        "          print(im.shape)\n",
        "          plt.plot(im)\n",
        "          plt.show()\n",
        "        \n",
        "      if use_teacher_forcing:\n",
        "        current_symbol[:, i + 1] = target[:, i]\n",
        "      else:\n",
        "        sampled = torch.multinomial(dec.exp(), 1)\n",
        "        current_symbol[:, i+1] = sampled.squeeze()\n",
        "      #print(dec.shape)\n",
        "      o = dec#dec.view(30, 1, 81).flatten(start_dim=0,end_dim=1)\n",
        "      #print(o.shape)\n",
        "      t = target[:, i].flatten()\n",
        "      loss += criterion(o, t)\n",
        "      #print(t[0].shape)\n",
        "#      print(\"->\" + train_set.inv_codes[t[0].item()])\n",
        "#      print(train_set.inv_codes[o[0]] + \"->\" + train_set.inv_codes[t[0]])\n",
        "    c_loss += loss.item()\n",
        "\n",
        "    if (batch_idx % freq == 0) and (batch_idx != 0):\n",
        "      print(\"TF: \" + str(use_teacher_forcing))\n",
        "      if not use_teacher_forcing:\n",
        "        for k in range(0, 5):\n",
        "           print(\"  \" + train_set.decode_word(target[k,:]) + \" -> \" + train_set.decode_word(current_symbol[k,:]))\n",
        "      c_loss /= freq \n",
        "      print(\"  Batch: \" + str(batch_idx) + \" Loss: \" + str(c_loss))\n",
        "      c_loss = 0\n",
        "      \n",
        "\n",
        "      \n",
        "    loss.backward()\n",
        "    grad_clip = 0.1\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), grad_clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), grad_clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    batch_idx += 1\n",
        "\n",
        "for i in range(0, 100):\n",
        "  train(i)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training epoch 0...\n",
            "TF: False\n",
            "  was                            -> <START>zeK0_cB(q0Soch;vvNR--?V.!yKPx\n",
            "  is                             -> <START>J-3B#5aMA4/Vb0'C38q.;U/\" /sk:\n",
            "  '                              -> <START>XTlz3af:zonB#0zL!D(r)4sz6zT&u\n",
            "  Malachias                      -> <START>g2YZm+M79i5eGCofByr8ygC0E)fjk\n",
            "  waste                          -> <START>HL*l9iEeFkpN#4mZw\"rnctr9rF\" t\n",
            "  Batch: 30 Loss: 125.41790110270182\n",
            "TF: False\n",
            "  John's                         -> <START>k3d s  a#M-4  h              \n",
            "  .                              -> <START>Ac y5H!.1)q#'TZc'-.4VW\";Ju /N\n",
            "  just                           -> <START>Ru)' T    X   d     )     G  \n",
            "  to                             -> <START>DrdlATsUW_o3\"3*k t n?4 6  s  \n",
            "  Munyard                        -> <START>mW  o                      c \n",
            "  Batch: 60 Loss: 80.57046076456706\n",
            "TF: False\n",
            "  EICHMANN                       -> <START>/fV,                   i     \n",
            "  once                           -> <START>oe  u                        \n",
            "  under                          -> <START>(<START>e4i  e                     \n",
            "  common                         -> <START>_M-       i                  \n",
            "  .                              -> <START>h/!j)t                       \n",
            "  Batch: 90 Loss: 27.08995812733968\n",
            "TF: False\n",
            "  Dr.                            -> <START>BJ -                         \n",
            "  ,                              -> <START>LEyN                         \n",
            "  thoughtful                     -> <START>PEpe.                        \n",
            "  the                            -> <START>n&nn                         \n",
            "  in                             -> <START>nX t                         \n",
            "  Batch: 120 Loss: 21.492502403259277\n",
            "TF: False\n",
            "  a                              -> <START>tfch                         \n",
            "  '                              -> <START>5hYJ&oo                      \n",
            "  action                         -> <START>Yrk_                         \n",
            "  tremble                        -> <START>nGoi                         \n",
            "  London                         -> <START>sJ ak                        \n",
            "  Batch: 150 Loss: 18.81577377319336\n",
            "TF: False\n",
            "  their                          -> <START>ps rQ                        \n",
            "  gross                          -> <START>xH*ne                        \n",
            "  them                           -> <START>.1                           \n",
            "  staff                          -> <START>-j                           \n",
            "  you                            -> <START>Icie        r                \n",
            "  Batch: 180 Loss: 17.083221658070883\n",
            "TF: True\n",
            "  Batch: 210 Loss: 16.457300853729247\n",
            "TF: False\n",
            "  sea                            -> <START>Qn                           \n",
            "  mind                           -> <START>upd                          \n",
            "  and                            -> <START>Lry                          \n",
            "  .                              -> <START>Y      r                     \n",
            "  said                           -> <START>)hs                          \n",
            "  Batch: 240 Loss: 16.095221328735352\n",
            "TF: True\n",
            "  Batch: 270 Loss: 15.859705098470052\n",
            "TF: True\n",
            "  Batch: 300 Loss: 15.22565860748291\n",
            "TF: True\n",
            "  Batch: 330 Loss: 15.35179131825765\n",
            "TF: True\n",
            "  Batch: 360 Loss: 15.134615262349447\n",
            "TF: False\n",
            "  the                            -> <START>pso                          \n",
            "  beaches                        -> <START>posr' i                      \n",
            "  ,                              -> <START>He                           \n",
            "  to                             -> <START>fas                          \n",
            "  and                            -> <START>uli e                        \n",
            "  Batch: 390 Loss: 15.182570298512777\n",
            "TF: True\n",
            "  Batch: 420 Loss: 14.735412724812825\n",
            "TF: False\n",
            "  thread                         -> <START>aot                          \n",
            "  \"                              -> <START>C                            \n",
            "  House                          -> <START>glwn                         \n",
            "  school                         -> <START>litna                        \n",
            "  ,                              -> <START>4s                           \n",
            "  Batch: 450 Loss: 14.7438445409139\n",
            "TF: True\n",
            "  Batch: 480 Loss: 14.366652234395344\n",
            "TF: False\n",
            "  ,                              -> <START>cg                           \n",
            "  progression                    -> <START>I'eoelmae                    \n",
            "  suppose                        -> <START>ntilaec                      \n",
            "  ,                              -> <START>do                           \n",
            "  were                           -> <START>zeeo                         \n",
            "  Batch: 510 Loss: 14.533890883127848\n",
            "TF: True\n",
            "  Batch: 540 Loss: 14.458664830525716\n",
            "TF: False\n",
            "  told                           -> <START>Ot                           \n",
            "  and                            -> <START>,r l                         \n",
            "  ,                              -> <START>Y                            \n",
            "  expert                         -> <START>ateg                         \n",
            "  face                           -> <START>owee                         \n",
            "  Batch: 570 Loss: 14.285210005442302\n",
            "TF: True\n",
            "  Batch: 600 Loss: 14.53982671101888\n",
            "TF: False\n",
            "  that                           -> <START>nOn                          \n",
            "  has                            -> <START>sctrlo                       \n",
            "  shape                          -> <START>suod                         \n",
            "  to                             -> <START>ed                           \n",
            "  .                              -> <START> n                  e        \n",
            "  Batch: 630 Loss: 14.33584451675415\n",
            "TF: True\n",
            "  Batch: 660 Loss: 14.199498303731282\n",
            "TF: True\n",
            "  Batch: 690 Loss: 14.248380025227865\n",
            "TF: True\n",
            "  Batch: 720 Loss: 14.013991196950277\n",
            "TF: False\n",
            "  Passion                        -> <START>Uon                          \n",
            "  4makrodeb                      -> <START>mturmtteti t                 \n",
            "  Gaitskell                      -> <START>diuaietds                    \n",
            "  that                           -> <START>Yeon                         \n",
            "  does                           -> <START>Nva                          \n",
            "  Batch: 750 Loss: 14.532213815053304\n",
            "Training epoch 1...\n",
            "TF: False\n",
            "  was                            -> <START>mer                          \n",
            "  is                             -> <START> '                           \n",
            "  '                              -> <START>l                            \n",
            "  Malachias                      -> <START>lf-rrnsinlg                  \n",
            "  waste                          -> <START>Etmhl                        \n",
            "  Batch: 30 Loss: 14.273148282368977\n",
            "TF: True\n",
            "  Batch: 60 Loss: 14.26296723683675\n",
            "TF: False\n",
            "  EICHMANN                       -> <START>aalarslaesr                  \n",
            "  once                           -> <START>,atpd                        \n",
            "  under                          -> <START>ttoeh     a                  \n",
            "  common                         -> <START>uareoet                      \n",
            "  .                              -> <START>*n                           \n",
            "  Batch: 90 Loss: 13.79981263478597\n",
            "TF: True\n",
            "  Batch: 120 Loss: 13.903298791249593\n",
            "TF: True\n",
            "  Batch: 150 Loss: 13.957150395711263\n",
            "TF: False\n",
            "  their                          -> <START>fotse                        \n",
            "  gross                          -> <START>yrw                          \n",
            "  them                           -> <START>tcto                         \n",
            "  staff                          -> <START>enn                          \n",
            "  you                            -> <START>eo                           \n",
            "  Batch: 180 Loss: 13.574970912933349\n",
            "TF: False\n",
            "  the                            -> <START>.g  n                        \n",
            "  and                            -> <START>Tien                         \n",
            "  in                             -> <START>t                            \n",
            "  rose-brocade                   -> <START>a-ncottanials                \n",
            "  pitch                          -> <START>eeiee                        \n",
            "  Batch: 210 Loss: 13.727864710489909\n",
            "TF: True\n",
            "  Batch: 240 Loss: 13.999841849009195\n",
            "TF: True\n",
            "  Batch: 270 Loss: 13.948617299397787\n",
            "TF: True\n",
            "  Batch: 300 Loss: 13.611159420013427\n",
            "TF: False\n",
            "  here                           -> <START>art      s                   \n",
            "  .                              -> <START>h                            \n",
            "  want                           -> <START>weois                        \n",
            "  had                            -> <START>rho                          \n",
            "  lace                           -> <START>a6-                          \n",
            "  Batch: 330 Loss: 13.934950796763102\n",
            "TF: False\n",
            "  in                             -> <START>i.                           \n",
            "  2a-calling                     -> <START>ohornynes                    \n",
            "  cannot                         -> <START>iwnlum                       \n",
            "  freedom                        -> <START>ehaou i                      \n",
            "  there                          -> <START>ypl y                        \n",
            "  Batch: 360 Loss: 13.820675309499105\n",
            "TF: False\n",
            "  the                            -> <START>me                           \n",
            "  beaches                        -> <START>wiireo                       \n",
            "  ,                              -> <START>h                            \n",
            "  to                             -> <START>ao                           \n",
            "  and                            -> <START>ftthbe                       \n",
            "  Batch: 390 Loss: 13.928584130605062\n",
            "TF: False\n",
            "  .                              -> <START>y                            \n",
            "  the                            -> <START>Ioe                          \n",
            "  I                              -> <START>,e                           \n",
            "  February                       -> <START>tereroth                     \n",
            "  bloodstained                   -> <START>fenetbairmi n                \n",
            "  Batch: 420 Loss: 13.606258233388266\n",
            "TF: False\n",
            "  thread                         -> <START>tihcee                       \n",
            "  \"                              -> <START>I                            \n",
            "  House                          -> <START>wehr                         \n",
            "  school                         -> <START>ioe                          \n",
            "  ,                              -> <START>q                            \n",
            "  Batch: 450 Loss: 13.67381731669108\n",
            "TF: True\n",
            "  Batch: 480 Loss: 13.354664421081543\n",
            "TF: True\n",
            "  Batch: 510 Loss: 13.650854682922363\n",
            "TF: False\n",
            "  For                            -> <START>Iss                          \n",
            "  monoxide                       -> <START>mcmkutptdds                  \n",
            "  on                             -> <START>hme                          \n",
            "  such                           -> <START>thtr                         \n",
            "  the                            -> <START>fnTl                         \n",
            "  Batch: 540 Loss: 13.572016112009685\n",
            "TF: False\n",
            "  told                           -> <START>1nv                          \n",
            "  and                            -> <START>lhte                         \n",
            "  ,                              -> <START>.,                           \n",
            "  expert                         -> <START>'hd                          \n",
            "  face                           -> <START>mnrs                         \n",
            "  Batch: 570 Loss: 13.456892236073811\n",
            "TF: False\n",
            "  wrote                          -> <START>bads  g                      \n",
            "  'd                             -> <START>yt                           \n",
            "  a                              -> <START>P                            \n",
            "  of                             -> <START>bhee                         \n",
            "  sufficiently                   -> <START>Iasiopntuvl                  \n",
            "  Batch: 600 Loss: 13.771322186787923\n",
            "TF: False\n",
            "  that                           -> <START>haet                         \n",
            "  has                            -> <START>aot                          \n",
            "  shape                          -> <START>mmdy s                       \n",
            "  to                             -> <START>tv                           \n",
            "  .                              -> <START>J                            \n",
            "  Batch: 630 Loss: 13.61133893330892\n",
            "TF: False\n",
            "  the                            -> <START>ho y                         \n",
            "  Bitter-Herbs                   -> <START>qoehprgndedr   e             \n",
            "  from                           -> <START>aehr                         \n",
            "  the                            -> <START>tng                          \n",
            "  than                           -> <START>Mhkli                        \n",
            "  Batch: 660 Loss: 13.471314303080241\n",
            "TF: False\n",
            "  an                             -> <START>;ao                          \n",
            "  .                              -> <START>                             \n",
            "  Valet                          -> <START>Heine                        \n",
            "  aspect                         -> <START>boohn                        \n",
            "  the                            -> <START>vw e                         \n",
            "  Batch: 690 Loss: 13.546850299835205\n",
            "TF: True\n",
            "  Batch: 720 Loss: 13.326812934875488\n",
            "TF: True\n",
            "  Batch: 750 Loss: 13.83358392715454\n",
            "Training epoch 2...\n",
            "TF: False\n",
            "  was                            -> <START>am                           \n",
            "  is                             -> <START>of                           \n",
            "  '                              -> <START>L                            \n",
            "  Malachias                      -> <START>ufmotoiss                    \n",
            "  waste                          -> <START>_cene                        \n",
            "  Batch: 30 Loss: 13.671409225463867\n",
            "TF: False\n",
            "  John's                         -> <START>nvhos                        \n",
            "  .                              -> <START>o                            \n",
            "  just                           -> <START>eewgele                      \n",
            "  to                             -> <START>ct                           \n",
            "  Munyard                        -> <START>suiosuey                     \n",
            "  Batch: 60 Loss: 13.6887588818868\n",
            "TF: True\n",
            "  Batch: 90 Loss: 13.201624457041422\n",
            "TF: True\n",
            "  Batch: 120 Loss: 13.436355908711752\n",
            "TF: False\n",
            "  a                              -> <START>w                            \n",
            "  '                              -> <START>f8d                          \n",
            "  action                         -> <START>Muoe                         \n",
            "  tremble                        -> <START>Onid                         \n",
            "  London                         -> <START>nnoimg e                     \n",
            "  Batch: 150 Loss: 13.43425817489624\n",
            "TF: True\n",
            "  Batch: 180 Loss: 12.910738341013591\n",
            "TF: True\n",
            "  Batch: 210 Loss: 13.140249983469646\n",
            "TF: False\n",
            "  sea                            -> <START>Fhr                          \n",
            "  mind                           -> <START>tals                         \n",
            "  and                            -> <START>hia                          \n",
            "  .                              -> <START>i,                           \n",
            "  said                           -> <START>0rai e                       \n",
            "  Batch: 240 Loss: 13.440630690256755\n",
            "TF: True\n",
            "  Batch: 270 Loss: 13.338804244995117\n",
            "TF: False\n",
            "  stepped                        -> <START>lelltmsy                     \n",
            "  startled                       -> <START>rurnaiLe i                   \n",
            "  to                             -> <START>*a                           \n",
            "  he                             -> <START>feis                         \n",
            "  children                       -> <START>piirien s                    \n",
            "  Batch: 300 Loss: 13.088913059234619\n",
            "TF: False\n",
            "  here                           -> <START>Tyt                          \n",
            "  .                              -> <START>ei                           \n",
            "  want                           -> <START>hogy                         \n",
            "  had                            -> <START>ley                          \n",
            "  lace                           -> <START>ihe                          \n",
            "  Batch: 330 Loss: 13.425275198618571\n",
            "TF: True\n",
            "  Batch: 360 Loss: 13.304064623514812\n",
            "TF: False\n",
            "  the                            -> <START>M                            \n",
            "  beaches                        -> <START>4olutel                      \n",
            "  ,                              -> <START>2                            \n",
            "  to                             -> <START>Te                           \n",
            "  and                            -> <START>aioen                        \n",
            "  Batch: 390 Loss: 13.536920420328777\n",
            "TF: True\n",
            "  Batch: 420 Loss: 13.06491142908732\n",
            "TF: False\n",
            "  thread                         -> <START>wrmarty                      \n",
            "  \"                              -> <START>,                            \n",
            "  House                          -> <START>bhtth                        \n",
            "  school                         -> <START>aalt                         \n",
            "  ,                              -> <START>P                            \n",
            "  Batch: 450 Loss: 13.16607608795166\n",
            "TF: True\n",
            "  Batch: 480 Loss: 12.89232292175293\n",
            "TF: False\n",
            "  ,                              -> <START>G                            \n",
            "  progression                    -> <START>fosdnswr                     \n",
            "  suppose                        -> <START>woioelon                     \n",
            "  ,                              -> <START>t                            \n",
            "  were                           -> <START>hos g                        \n",
            "  Batch: 510 Loss: 13.127410538991292\n",
            "TF: False\n",
            "  For                            -> <START>ua                           \n",
            "  monoxide                       -> <START>irraanrgeeirs                \n",
            "  on                             -> <START>aa                           \n",
            "  such                           -> <START>toe                          \n",
            "  the                            -> <START> wo                          \n",
            "  Batch: 540 Loss: 13.129182942708333\n",
            "TF: False\n",
            "  told                           -> <START>mae                          \n",
            "  and                            -> <START>iot                          \n",
            "  ,                              -> <START>K                            \n",
            "  expert                         -> <START>ttly                         \n",
            "  face                           -> <START>iuhy                         \n",
            "  Batch: 570 Loss: 12.934216658274332\n",
            "TF: False\n",
            "  wrote                          -> <START>ahoyd                        \n",
            "  'd                             -> <START>tf                           \n",
            "  a                              -> <START>2e                           \n",
            "  of                             -> <START>vy                           \n",
            "  sufficiently                   -> <START>caunnelodgg                  \n",
            "  Batch: 600 Loss: 13.268212445576985\n",
            "TF: False\n",
            "  that                           -> <START>Hod                          \n",
            "  has                            -> <START>,he                          \n",
            "  shape                          -> <START>orgaet                       \n",
            "  to                             -> <START>ie                           \n",
            "  .                              -> <START>.                            \n",
            "  Batch: 630 Loss: 13.102273527781168\n",
            "TF: False\n",
            "  the                            -> <START>.n                           \n",
            "  Bitter-Herbs                   -> <START>heprecgoonnnd                \n",
            "  from                           -> <START>phdis                        \n",
            "  the                            -> <START>yay                          \n",
            "  than                           -> <START>foarl                        \n",
            "  Batch: 660 Loss: 13.038908545176188\n",
            "TF: True\n",
            "  Batch: 690 Loss: 13.014485422770182\n",
            "TF: False\n",
            "  In                             -> <START>,s                           \n",
            "  theoretical                    -> <START>)mcernd                      \n",
            "  America                        -> <START>asfvm                        \n",
            "  to                             -> <START>b                            \n",
            "  members                        -> <START>fcotesgm                     \n",
            "  Batch: 720 Loss: 12.934386857350667\n",
            "TF: False\n",
            "  Passion                        -> <START>wGeie                        \n",
            "  4makrodeb                      -> <START>ahpbtaemft                   \n",
            "  Gaitskell                      -> <START>nnehotadd                    \n",
            "  that                           -> <START>heh                          \n",
            "  does                           -> <START>iod                          \n",
            "  Batch: 750 Loss: 13.435948371887207\n",
            "Training epoch 3...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yg8-hpPq2e2p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}